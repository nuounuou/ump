#!/usr/bin/env python3
"""
SAM2 Video Segmentation â†’ YOLO Dataset â€” Gradio Web UI

è¿è¡Œï¼š
cd /home/nuounuou/sam2/notebooks && python app_gradio.py

ç„¶åä½ çš„ç”µè„‘):
SSH ç«¯å£è½¬å‘:ssh -L 7860:localhost:7860 nuounuou@172.26.211.82

å¦‚æœå‡ºç°ç«¯å£å ç”¨,æ€æ­»è¿›ç¨‹: kill $(lsof -t -i:7860) 2>/dev/null
é‡æ–°è¿è¡Œ:cd /home/nuounuou/sam2/notebooks && python app_gradio.py
"""

import os
import sys
import shutil
import subprocess
import numpy as np
import torch
import cv2
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from PIL import Image, ImageDraw
import gradio as gr
import traceback
import csv
import glob
import json
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è·¯å¾„è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from sam2.build_sam import build_sam2_video_predictor
import app_template_align as ta
import maze_processing as mp

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.join(SCRIPT_DIR, 'video_to_img')
VIDEOS_DIR = os.path.join(SCRIPT_DIR, 'videos')
YOLO_DIR = os.path.join(SCRIPT_DIR, 'yolo')
YOLO_DATASET_DIR = os.path.join(SCRIPT_DIR, 'yolo_dataset')
SAM2_CHECKPOINT = os.path.join(PROJECT_ROOT, 'checkpoints', 'sam2.1_hiera_tiny.pt')
MODEL_CFG = 'configs/sam2.1/sam2.1_hiera_t.yaml'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAM2 æ¨¡å‹å…¨å±€ç¼“å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
_predictor = None
_device = None


def get_device():
    global _device
    if _device is None:
        if torch.cuda.is_available():
            _device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            _device = torch.device('mps')
        else:
            _device = torch.device('cpu')
    return _device


def get_predictor():
    global _predictor
    if _predictor is None:
        device = get_device()
        print(f'[SAM2] Loading model on {device} ...')
        _predictor = build_sam2_video_predictor(MODEL_CFG, SAM2_CHECKPOINT, device=device)
        print('[SAM2] Model loaded.')
    return _predictor


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mask_to_yolo_seg(mask, img_w, img_h, simplify_tolerance=2.0):
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return None
    mask_uint8 = (mask_2d > 0).astype(np.uint8) * 255
    mask_h, mask_w = mask_2d.shape
    if mask_h != img_h or mask_w != img_w:
        mask_uint8 = cv2.resize(mask_uint8, (img_w, img_h), interpolation=cv2.INTER_NEAREST)
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) == 0:
        return None
    largest = max(contours, key=cv2.contourArea)
    if simplify_tolerance > 0:
        epsilon = simplify_tolerance * cv2.arcLength(largest, True) / 100.0
        largest = cv2.approxPolyDP(largest, epsilon, True)
    if len(largest) < 3:
        return None
    polygon = largest.reshape(-1, 2).astype(np.float32)
    polygon[:, 0] /= img_w
    polygon[:, 1] /= img_h
    return polygon.flatten().tolist()


def mask_to_yolo_bbox(mask, img_w, img_h):
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return None
    mask_h, mask_w = mask_2d.shape
    coords = np.column_stack(np.where(mask_2d > 0))
    if len(coords) == 0:
        return None
    y_coords, x_coords = coords[:, 0], coords[:, 1]
    x_min_mask, x_max_mask = float(x_coords.min()), float(x_coords.max())
    y_min_mask, y_max_mask = float(y_coords.min()), float(y_coords.max())
    if mask_h != img_h or mask_w != img_w:
        scale_x = img_w / mask_w
        scale_y = img_h / mask_h
        x_min = x_min_mask * scale_x
        x_max = x_max_mask * scale_x
        y_min = y_min_mask * scale_y
        y_max = y_max_mask * scale_y
    else:
        x_min, x_max = x_min_mask, x_max_mask
        y_min, y_max = y_min_mask, y_max_mask
    return int(x_min), int(y_min), int(x_max), int(y_max)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è§†é¢‘åˆ‡å¸§ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_video_files():
    """åˆ—å‡º videos/ ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
    if not os.path.isdir(VIDEOS_DIR):
        return []
    exts = ('.mp4', '.avi', '.mov', '.mkv')
    return sorted([f for f in os.listdir(VIDEOS_DIR) if f.lower().endswith(exts)])


def on_extract_frames(video_file, frame_interval):
    """ä»è§†é¢‘ä¸­æŠ½å¸§ï¼Œä¿å­˜åˆ° video_to_img/"""
    if not video_file:
        return "è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶", gr.update()
    video_path = os.path.join(VIDEOS_DIR, video_file)
    output_name = Path(video_file).stem
    output_path = os.path.join(BASE_DIR, output_name)
    os.makedirs(output_path, exist_ok=True)
    frame_interval = max(1, int(frame_interval))
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vf", f"select='not(mod(n,{frame_interval}))'",
        "-vsync", "0", "-q:v", "2", "-start_number", "0",
        os.path.join(output_path, "%05d.jpg"),
    ]
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True,
                       encoding='utf-8', errors='ignore')
        n = len([f for f in os.listdir(output_path) if f.lower().endswith('.jpg')])
        new_dirs = list_video_dirs()
        return (f"åˆ‡å¸§å®Œæˆ: {output_name}, å…± {n} å¸§ (æ¯ {frame_interval} å¸§å– 1 å¸§)\n"
                f"ä¿å­˜è‡³: {output_path}"),\
               gr.update(choices=new_dirs, value=output_name)
    except Exception as e:
        return f"åˆ‡å¸§å¤±è´¥: {e}", gr.update()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¸§/ç›®å½•ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_video_dirs():
    """åˆ—å‡º video_to_img ä¸‹æ‰€æœ‰åŒ…å« jpg çš„å­æ–‡ä»¶å¤¹"""
    dirs = []
    if not os.path.isdir(BASE_DIR):
        return dirs
    for d in sorted(os.listdir(BASE_DIR)):
        full = os.path.join(BASE_DIR, d)
        if os.path.isdir(full):
            jpgs = [f for f in os.listdir(full) if f.lower().endswith(('.jpg', '.jpeg'))]
            if jpgs:
                dirs.append(d)
    return dirs


def get_sorted_frame_names(video_dir_name):
    """è·å–æŸä¸ªè§†é¢‘ç›®å½•ä¸‹æ‰€æœ‰æ’åºåçš„å¸§æ–‡ä»¶å"""
    d = os.path.join(BASE_DIR, video_dir_name)
    names = [f for f in os.listdir(d) if f.lower().endswith(('.jpg', '.jpeg'))]
    names.sort(key=lambda p: int(os.path.splitext(p)[0]))
    return names


def load_frame_image(video_dir_name, frame_idx):
    """åŠ è½½æŒ‡å®šå¸§å›¾ç‰‡ï¼Œè¿”å› PIL Image"""
    names = get_sorted_frame_names(video_dir_name)
    if not names:
        return None
    frame_idx = max(0, min(frame_idx, len(names) - 1))
    path = os.path.join(BASE_DIR, video_dir_name, names[frame_idx])
    return Image.open(path).convert('RGB')


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å›¾ç‰‡ä¸Šç»˜åˆ¶æ ‡è®°ç‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def draw_points_on_image(pil_img, points, labels):
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶é€‰ä¸­çš„ç‚¹ï¼Œæ­£æ ·æœ¬=ç»¿è‰²ï¼Œè´Ÿæ ·æœ¬=çº¢è‰²"""
    img_draw = pil_img.copy()
    draw = ImageDraw.Draw(img_draw)
    r = 6
    for i, (pt, lbl) in enumerate(zip(points, labels)):
        x, y = int(pt[0]), int(pt[1])
        color = (0, 255, 0) if lbl == 1 else (255, 0, 0)
        outline = (255, 255, 255)
        draw.ellipse([x - r, y - r, x + r, y + r], fill=color, outline=outline, width=2)
        # æ˜Ÿå·æ ‡è®°
        draw.line([(x - r - 2, y), (x + r + 2, y)], fill=outline, width=1)
        draw.line([(x, y - r - 2), (x, y + r + 2)], fill=outline, width=1)
        # ç¼–å·
        draw.text((x + r + 4, y - r), str(i + 1), fill='white')
    return img_draw


def draw_mask_overlay(pil_img, mask, alpha=0.45):
    """åœ¨å›¾ç‰‡ä¸Šå åŠ  maskï¼ˆçº¢è‰²åŠé€æ˜ï¼‰"""
    img_np = np.array(pil_img).copy()
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return pil_img
    # å°† mask resize åˆ°å›¾ç‰‡å°ºå¯¸
    h, w = img_np.shape[:2]
    mh, mw = mask_2d.shape
    if mh != h or mw != w:
        mask_2d = cv2.resize(mask_2d.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
    # çº¢è‰²å åŠ 
    overlay = img_np.copy()
    overlay[mask_2d > 0] = [255, 50, 50]
    img_np = (img_np * (1 - alpha) + overlay * alpha).astype(np.uint8)
    # è½®å»“
    contours, _ = cv2.findContours(
        (mask_2d > 0).astype(np.uint8) * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )
    cv2.drawContours(img_np, contours, -1, (0, 255, 0), 2)
    return Image.fromarray(img_np)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio å›è°ƒå‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€

def on_video_dir_change(video_dir_name):
    """åˆ‡æ¢è§†é¢‘ç›®å½•æ—¶ï¼šé‡ç½®ä¸€åˆ‡ï¼Œæ˜¾ç¤ºç¬¬ 0 å¸§"""
    if not video_dir_name:
        return None, 0, gr.update(maximum=0), "è¯·é€‰æ‹©è§†é¢‘ç›®å½•", [], [], None
    names = get_sorted_frame_names(video_dir_name)
    total = len(names)
    img = load_frame_image(video_dir_name, 0)
    info = f"ğŸ“‚ {video_dir_name} â€” å…± {total} å¸§ï¼Œå›¾ç‰‡å°ºå¯¸: {img.size[0]}x{img.size[1]}"
    return img, 0, gr.update(maximum=max(total - 1, 0)), info, [], [], None


def on_frame_change(video_dir_name, frame_idx, points_state, labels_state):
    """åˆ‡æ¢å¸§æ—¶ï¼šé‡æ–°ç»˜åˆ¶æ ‡è®°ç‚¹"""
    if not video_dir_name:
        return None
    img = load_frame_image(video_dir_name, frame_idx)
    if img is None:
        return None
    if points_state:
        img = draw_points_on_image(img, points_state, labels_state)
    return img


def on_image_click(video_dir_name, frame_idx, point_type, points_state, labels_state, evt: gr.SelectData):
    """ç”¨æˆ·åœ¨å›¾ç‰‡ä¸Šç‚¹å‡» â†’ æ·»åŠ ä¸€ä¸ªæ ‡è®°ç‚¹"""
    if not video_dir_name:
        return None, points_state, labels_state, "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"

    # evt.index åœ¨ Gradio Image ç»„ä»¶ä¸­æ˜¯ [x, y]ï¼ˆåƒç´ åæ ‡ï¼‰
    x, y = evt.index[0], evt.index[1]
    label = 1 if point_type == "æ­£æ ·æœ¬ (å‰æ™¯)" else 0

    points_state.append([x, y])
    labels_state.append(label)

    # é‡æ–°ç»˜åˆ¶
    img = load_frame_image(video_dir_name, frame_idx)
    img = draw_points_on_image(img, points_state, labels_state)

    # ç”Ÿæˆç‚¹åˆ—è¡¨æ–‡æœ¬
    lines = []
    for i, (pt, lbl) in enumerate(zip(points_state, labels_state)):
        tag = "æ­£æ ·æœ¬" if lbl == 1 else "è´Ÿæ ·æœ¬"
        lines.append(f"  {i + 1}. ({int(pt[0])}, {int(pt[1])})  {tag}")
    info = f"å·²é€‰ {len(points_state)} ä¸ªç‚¹ï¼š\n" + "\n".join(lines)

    return img, points_state, labels_state, info


def on_clear_points(video_dir_name, frame_idx):
    """æ¸…é™¤æ‰€æœ‰é€‰ç‚¹"""
    img = load_frame_image(video_dir_name, frame_idx) if video_dir_name else None
    return img, [], [], "å·²æ¸…é™¤æ‰€æœ‰ç‚¹"


def on_undo_point(video_dir_name, frame_idx, points_state, labels_state):
    """æ’¤é”€æœ€åä¸€ä¸ªç‚¹"""
    if points_state:
        points_state.pop()
        labels_state.pop()
    img = load_frame_image(video_dir_name, frame_idx) if video_dir_name else None
    if img is not None and points_state:
        img = draw_points_on_image(img, points_state, labels_state)

    if points_state:
        lines = []
        for i, (pt, lbl) in enumerate(zip(points_state, labels_state)):
            tag = "æ­£æ ·æœ¬" if lbl == 1 else "è´Ÿæ ·æœ¬"
            lines.append(f"  {i + 1}. ({int(pt[0])}, {int(pt[1])})  {tag}")
        info = f"å·²é€‰ {len(points_state)} ä¸ªç‚¹ï¼š\n" + "\n".join(lines)
    else:
        info = "å·²æ¸…é™¤æ‰€æœ‰ç‚¹"
    return img, points_state, labels_state, info


def on_preview_mask(video_dir_name, frame_idx, points_state, labels_state):
    """é¢„è§ˆå•å¸§ mask"""
    if not video_dir_name:
        return None, "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not points_state:
        return None, "è¯·å…ˆåœ¨å›¾ç‰‡ä¸Šç‚¹å‡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ ‡è®°ç‚¹"

    try:
        predictor = get_predictor()
        video_path = os.path.join(BASE_DIR, video_dir_name)
        inference_state = predictor.init_state(video_path=video_path)

        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)

        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=int(frame_idx),
            obj_id=1,
            points=points_np,
            labels=labels_np,
        )

        # å–ç¬¬ä¸€ä¸ªç›®æ ‡çš„ mask
        mask = (out_mask_logits[0] > 0.0).cpu().numpy()

        img = load_frame_image(video_dir_name, frame_idx)
        result_img = draw_mask_overlay(img, mask)
        result_img = draw_points_on_image(result_img, points_state, labels_state)

        # é‡Šæ”¾
        predictor.reset_state(inference_state)

        mask_pixels = int(np.sum(np.squeeze(mask) > 0))
        return result_img, f"Mask é¢„è§ˆæˆåŠŸï¼mask åƒç´ æ•°: {mask_pixels}"
    except Exception as e:
        traceback.print_exc()
        return None, f"é¢„è§ˆå¤±è´¥: {str(e)}"


def on_export_yolo(video_dir_name, frame_idx, points_state, labels_state, class_id, progress=gr.Progress()):
    """è¿è¡Œ SAM2 å…¨åºåˆ—ä¼ æ’­ â†’ å¯¼å‡º YOLO æ•°æ®é›†"""
    if not video_dir_name:
        return "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not points_state:
        return "è¯·å…ˆåœ¨å›¾ç‰‡ä¸Šç‚¹å‡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ ‡è®°ç‚¹"

    try:
        log_lines = []

        def log(msg):
            log_lines.append(msg)
            print(msg)

        predictor = get_predictor()
        video_path = os.path.join(BASE_DIR, video_dir_name)

        log(f"è§†é¢‘ç›®å½•: {video_path}")
        log(f"è®¾å¤‡: {get_device()}")
        log(f"æ ‡è®°å¸§: {frame_idx}, é€‰ç‚¹æ•°: {len(points_state)}, CLASS_ID: {class_id}")

        # åˆå§‹åŒ–
        progress(0.0, desc="åˆå§‹åŒ– SAM2 ...")
        inference_state = predictor.init_state(video_path=video_path)

        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)

        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=int(frame_idx),
            obj_id=1,
            points=points_np,
            labels=labels_np,
        )
        log(f"å·²æ·»åŠ æç¤ºç‚¹ï¼Œç›®æ ‡æ•°: {len(out_obj_ids)}")

        # æ­£å‘ä¼ æ’­
        progress(0.1, desc="æ­£å‘ä¼ æ’­ä¸­ ...")
        video_segments = {}
        for out_frame_idx, out_obj_ids_prop, out_mask_logits_prop in predictor.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {
                out_obj_id: (out_mask_logits_prop[i] > 0.0).cpu().numpy()
                for i, out_obj_id in enumerate(out_obj_ids_prop)
            }
        log(f"ä¼ æ’­å®Œæˆ: {len(video_segments)} å¸§æœ‰ mask")

        # é‡Šæ”¾
        predictor.reset_state(inference_state)

        # å¯¼å‡º YOLO æ•°æ®é›†
        progress(0.4, desc="å¯¼å‡º YOLO æ•°æ®é›† ...")
        output_dir = os.path.join(SCRIPT_DIR, 'yolo_dataset', video_dir_name)
        images_dir = os.path.join(output_dir, 'images')
        labels_dir = os.path.join(output_dir, 'labels')
        vis_dir = os.path.join(output_dir, 'images_vis')
        os.makedirs(images_dir, exist_ok=True)
        os.makedirs(labels_dir, exist_ok=True)
        os.makedirs(vis_dir, exist_ok=True)

        frame_names = get_sorted_frame_names(video_dir_name)
        total_frames = len(frame_names)
        saved_count = 0
        skipped_count = 0
        class_id_int = int(class_id)

        for fi in range(total_frames):
            progress(0.4 + 0.55 * (fi / total_frames), desc=f"å¤„ç†å¸§ {fi + 1}/{total_frames} ...")

            img_path = os.path.join(video_path, frame_names[fi])
            img = Image.open(img_path).convert('RGB')
            img_w, img_h = img.size

            label_lines = []
            bboxes_px = []

            if fi in video_segments:
                for obj_id, mask in video_segments[fi].items():
                    mask_2d = np.squeeze(mask)
                    if mask_2d.ndim != 2 or not np.any(mask_2d > 0):
                        continue
                    polygon = mask_to_yolo_seg(mask_2d, img_w, img_h, simplify_tolerance=2.0)
                    if polygon is None or len(polygon) < 6:
                        continue
                    polygon_array = np.array(polygon)
                    if np.any(polygon_array < 0) or np.any(polygon_array > 1):
                        continue
                    polygon_str = ' '.join([f'{coord:.6f}' for coord in polygon])
                    label_lines.append(f"{class_id_int} {polygon_str}\n")
                    bbox = mask_to_yolo_bbox(mask_2d, img_w, img_h)
                    if bbox is not None:
                        bboxes_px.append(bbox)

            # ä¿å­˜å›¾ç‰‡
            img_name = f'{fi:05d}.jpg'
            shutil.copy(img_path, os.path.join(images_dir, img_name))

            # ä¿å­˜æ ‡æ³¨
            label_name = f'{fi:05d}.txt'
            with open(os.path.join(labels_dir, label_name), 'w', encoding='utf-8') as f:
                if label_lines:
                    f.writelines(label_lines)

            if not label_lines:
                skipped_count += 1

            # å¯è§†åŒ–
            fig = plt.figure(figsize=(6, 4), dpi=100)
            ax = plt.gca()
            ax.axis('off')
            ax.set_title(f'frame {fi}')
            ax.imshow(img)
            if fi in video_segments:
                for obj_id, mask in video_segments[fi].items():
                    mask_2d = np.squeeze(mask)
                    if mask_2d.ndim != 2 or not np.any(mask_2d > 0):
                        continue
                    # åŠé€æ˜ mask
                    m = mask_2d.astype(np.float32)
                    h, w = m.shape
                    rgba = np.zeros((h, w, 4), dtype=np.float32)
                    rgba[..., 0] = 1.0
                    rgba[..., 1] = 0.2
                    rgba[..., 2] = 0.2
                    rgba[..., 3] = m * 0.45
                    ax.imshow(rgba)
                    for bbox in bboxes_px:
                        x_min, y_min, x_max, y_max = bbox
                        rect = Rectangle(
                            (x_min, y_min), x_max - x_min + 1, y_max - y_min + 1,
                            fill=False, linewidth=2, edgecolor='green',
                        )
                        ax.add_patch(rect)
            plt.savefig(os.path.join(vis_dir, img_name), bbox_inches='tight', pad_inches=0)
            plt.close(fig)

            saved_count += 1

        progress(1.0, desc="å®Œæˆï¼")

        log(f"\n{'=' * 50}")
        log(f"   å¯¼å‡ºå®Œæˆï¼")
        log(f"   æ€»å¸§æ•°: {total_frames}")
        log(f"   æœ‰ mask çš„å¸§: {saved_count - skipped_count}")
        log(f"   æ—  mask çš„å¸§: {skipped_count}")
        log(f"   è¾“å‡ºç›®å½•: {output_dir}")
        log(f"     â”œâ”€â”€ images/     (åŸå›¾)")
        log(f"     â”œâ”€â”€ labels/     (YOLO å®ä¾‹åˆ†å‰²label)")
        log(f"     â””â”€â”€ images_vis/ (å¯è§†åŒ–)")

        return "\n".join(log_lines)

    except Exception as e:
        traceback.print_exc()
        return f"å¯¼å‡ºå¤±è´¥: {str(e)}\n{traceback.format_exc()}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2: YOLO è®­ç»ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_yolo_datasets():
    """åˆ—å‡º yolo_dataset/ ä¸‹å¯ç”¨çš„æ•°æ®é›†"""
    if not os.path.isdir(YOLO_DATASET_DIR):
        return []
    return sorted([
        d for d in os.listdir(YOLO_DATASET_DIR)
        if os.path.isdir(os.path.join(YOLO_DATASET_DIR, d, 'images'))
    ])


def prepare_and_train_yolo(dataset_name, model_name, epochs, batch_size, imgsz,
                           class_name, val_ratio, progress=gr.Progress()):
    """å‡†å¤‡æ•°æ®é›† + è®­ç»ƒ YOLO"""
    if not dataset_name:
        return "å…ˆé€‰æ‹©æ•°æ®é›†", ""
    try:
        import random
        log_lines = []

        def log(msg):
            log_lines.append(msg)
            print(msg)

        # â”€â”€ 1. å‡†å¤‡æ•°æ®é›† (split train/val) â”€â”€
        progress(0.0, desc="å‡†å¤‡æ•°æ®é›†-è®­ç»ƒ/éªŒè¯é›†/æµ‹è¯•é›†")
        dataset_root = Path(YOLO_DATASET_DIR) / dataset_name
        images_dir = dataset_root / "images"
        labels_dir = dataset_root / "labels"

        train_img = dataset_root / "train" / "images"
        train_lbl = dataset_root / "train" / "labels"
        val_img = dataset_root / "val" / "images"
        val_lbl = dataset_root / "val" / "labels"
        for d in [train_img, train_lbl, val_img, val_lbl]:
            d.mkdir(parents=True, exist_ok=True)
            for f in d.iterdir():
                f.unlink()

        image_files = [f.stem for f in images_dir.glob("*.jpg")]
        log(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")

        # æ£€æµ‹ç±»åˆ«
        classes = set()
        for lf in labels_dir.glob("*.txt"):
            with open(lf) as f:
                for line in f:
                    if line.strip():
                        classes.add(int(line.strip().split()[0]))
        nc = max(len(classes), 1)
        log(f"æ£€æµ‹åˆ° {nc} ä¸ªç±»åˆ«: {sorted(classes)}")

        # åˆ†å‰²
        rng = random.Random(42)
        rng.shuffle(image_files)
        split_idx = int(len(image_files) * (1 - val_ratio))
        train_files = image_files[:split_idx]
        val_files = image_files[split_idx:]
        log(f"è®­ç»ƒé›†: {len(train_files)}, éªŒè¯é›†: {len(val_files)}")

        for stem in train_files:
            shutil.copy2(images_dir / f"{stem}.jpg", train_img / f"{stem}.jpg")
            src_lbl = labels_dir / f"{stem}.txt"
            if src_lbl.exists():
                shutil.copy2(src_lbl, train_lbl / f"{stem}.txt")

        for stem in val_files:
            shutil.copy2(images_dir / f"{stem}.jpg", val_img / f"{stem}.jpg")
            src_lbl = labels_dir / f"{stem}.txt"
            if src_lbl.exists():
                shutil.copy2(src_lbl, val_lbl / f"{stem}.txt")


        # â”€â”€ 2. æ›´æ–° dataset.yaml â”€â”€
        progress(0.05, desc="æ›´æ–°dataset.yaml")
        rel_path = os.path.relpath(dataset_root, YOLO_DIR)
        names_block = "\n".join([f"  {c}: {class_name}" for c in sorted(classes)]) if classes else f"  0: {class_name}"
        yaml_content = (
            f"path: {rel_path}\n"
            f"train: train/images\n"
            f"val: val/images\n\n"
            f"nc: {nc}\n\n"
            f"names:\n{names_block}\n"
        )
        yaml_path = Path(YOLO_DIR) / "dataset.yaml"
        yaml_path.write_text(yaml_content, encoding='utf-8')
        log(f"å·²æ›´æ–° {yaml_path}")
        log(f"dataset.yaml å†…å®¹:\n{yaml_content}")

        # â”€â”€ 3. è®­ç»ƒ â”€â”€
        progress(0.1, desc="å¼€å§‹ YOLO è®­ç»ƒ")
        from ultralytics import YOLO

        os.chdir(YOLO_DIR)
        model = YOLO(model_name)
        log(f"æ¨¡å‹: {model_name}, epochs={int(epochs)}, batch={int(batch_size)}, imgsz={int(imgsz)}")

        results = model.train(
            data=str(yaml_path),
            task="segment",
            epochs=int(epochs),
            imgsz=int(imgsz),
            batch=int(batch_size),
            device=0 if torch.cuda.is_available() else "cpu",
            workers=4,
            project="runs/segment",
            name=f"seg_{dataset_name}",
            save=True,
            save_period=10,
            plots=True,
            val=True,
            patience=50,
        )

        progress(0.95, desc="ä¿å­˜æ¨¡å‹")
        best_pt_src = Path(results.save_dir) / "weights" / "best.pt"
        best_pt_dst = Path(YOLO_DIR) / "best.pt"
        if best_pt_src.exists():
            shutil.copy2(best_pt_src, best_pt_dst)
            log(f"\nbest.pt å·²å¤åˆ¶åˆ°: {best_pt_dst}")
        else:
            best_pt_dst = best_pt_src
            log(f"\nbest.pt è·¯å¾„: {best_pt_src}")

        progress(1.0, desc="è®­ç»ƒå®Œæˆ")
        log(f"\n{'=' * 50}")
        log(f"è®­ç»ƒå®Œæˆ!")
        log(f"best.pt: {best_pt_dst}")
        log(f"è®­ç»ƒç›®å½•: {results.save_dir}")

        # scp æŒ‡ä»¤
        scp_cmd = f"scp nuounuou@172.26.211.82:{best_pt_dst} ./" 
        result_text = (
            f"best.pt è·¯å¾„:\n{best_pt_dst}\n\n"
            f"æ‹·è´åˆ°æœ¬åœ° (scp):\n{scp_cmd} (æ‹·è´å›å»çš„è·¯å¾„è‡ªå·±æ”¹ä¸€ä¸‹!bro)\n\n"
            f"è®­ç»ƒç›®å½•:\n{results.save_dir}"
        )
        return "\n".join(log_lines), result_text

    except Exception as e:
        traceback.print_exc()
        return f"è®­ç»ƒå¤±è´¥: {e}\n{traceback.format_exc()}", ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3: Shared Control Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€

SC_DIR = os.path.join(PROJECT_ROOT, 'shared_control_dataset')
SC_OUT_DIR = os.path.join(SC_DIR, 'registration_output')
RAY_MAX = 200


def _load_maze_rgb(maze_path):
    """åŠ è½½è¿·å®«å›¾ä¸º RGB numpy"""
    m = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    if m is None:
        return None
    if m.shape[2] == 4:
        a = m[:, :, 3:4].astype(np.float32) / 255
        bgr = m[:, :, :3].astype(np.float32)
        rgb = (bgr * a + 255 * (1 - a)).astype(np.uint8)
        return cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
    return cv2.cvtColor(m, cv2.COLOR_BGR2RGB)


def _draw_pts(img, pts, colors=((255,0,0),(0,200,0),(0,0,255),(255,165,0))):
    vis = img.copy()
    h, w = vis.shape[:2]
    r = max(5, min(h, w) // 60)
    for i, (x, y) in enumerate(pts):
        c = colors[i % len(colors)]
        cv2.circle(vis, (int(x), int(y)), r, c, -1)
        cv2.circle(vis, (int(x), int(y)), r, (255,255,255), 2)
        cv2.putText(vis, str(i+1), (int(x)+r+2, int(y)+r),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, c, 2)
    return vis


def sc_on_dataset_change(dataset_name):
    """é€‰æ‹©æ•°æ®é›† â†’ åŠ è½½ç¬¬ä¸€å¸§ + å¤åˆ¶æ•°æ® + åŠ è½½è¿·å®«å›¾"""
    if not dataset_name:
        return None, None, None, "è¯·é€‰æ‹©æ•°æ®é›†"
    src = os.path.join(YOLO_DATASET_DIR, dataset_name)
    # å¤åˆ¶ images_vis & labels åˆ° shared_control_dataset
    for sub in ("images_vis", "labels"):
        dst = os.path.join(SC_DIR, sub)
        src_sub = os.path.join(src, sub)
        if os.path.isdir(src_sub):
            if os.path.isdir(dst):
                shutil.rmtree(dst)
            shutil.copytree(src_sub, dst)
    # åŠ è½½ç¬¬ä¸€å¸§
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    if not imgs:
        return None, None, None, "æœªæ‰¾åˆ°å›¾åƒ"
    cam = cv2.cvtColor(cv2.imread(imgs[0]), cv2.COLOR_BGR2RGB)
    img_h, img_w = cam.shape[:2]
    n_labels = len(glob.glob(os.path.join(SC_DIR, "labels", "*.txt")))
    # åŠ è½½è¿·å®«å›¾
    maze_path = os.path.join(SC_DIR, "maze1.png")
    maze_img = _load_maze_rgb(maze_path) if os.path.exists(maze_path) else None
    info = f"å·²åŠ è½½ {len(imgs)} å¸§, {n_labels} æ ‡ç­¾, å°ºå¯¸ {img_w}x{img_h}"
    if maze_img is not None:
        info += f"\nè¿·å®«å›¾å·²åŠ è½½: {maze_path}"
    else:
        info += f"\nè¿·å®«å›¾ä¸å­˜åœ¨: {maze_path}"
    return maze_img, cam, None, info


def sc_load_maze():
    """åŠ è½½å›ºå®šè·¯å¾„çš„è¿·å®«å›¾"""
    maze_path = os.path.join(SC_DIR, "maze1.png")
    if not os.path.exists(maze_path):
        return None, f"è¿·å®«å›¾ä¸å­˜åœ¨: {maze_path}"
    rgb = _load_maze_rgb(maze_path)
    return rgb, f"è¿·å®«å›¾å·²åŠ è½½: {maze_path}"


def sc_click_maze(pts, evt: gr.SelectData):
    x, y = evt.index
    if len(pts) >= 4:
        pts = []
    pts.append([x, y])
    maze_path = os.path.join(SC_DIR, "maze1.png")
    rgb = _load_maze_rgb(maze_path)
    info = f"è¿·å®«ç‚¹ ({len(pts)}/4): " + ", ".join(f"({p[0]},{p[1]})" for p in pts)
    return _draw_pts(rgb, pts) if rgb is not None else None, pts, info


def sc_click_cam(pts, evt: gr.SelectData):
    x, y = evt.index
    if len(pts) >= 4:
        pts = []
    pts.append([x, y])
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    cam = cv2.cvtColor(cv2.imread(imgs[0]), cv2.COLOR_BGR2RGB) if imgs else None
    info = f"ç›¸æœºç‚¹ ({len(pts)}/4): " + ", ".join(f"({p[0]},{p[1]})" for p in pts)
    return _draw_pts(cam, pts) if cam is not None else None, pts, info


def sc_register(maze_pts, cam_pts, alpha_val):
    """é…å‡†è¿·å®«"""
    if len(maze_pts) != 4 or len(cam_pts) != 4:
        return None, "è¯·åœ¨ä¸¤å¼ å›¾ä¸Šå„ç‚¹å‡» 4 ä¸ªå¯¹åº”ç‚¹"
    os.makedirs(SC_OUT_DIR, exist_ok=True)
    maze_path = os.path.join(SC_DIR, "maze1.png")
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    if not imgs:
        return None, "æœªæ‰¾åˆ°å›¾åƒ"

    src, dst = np.float32(maze_pts), np.float32(cam_pts)
    H, _ = cv2.findHomography(src, dst)

    maze_raw = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    cam_bgr = cv2.imread(imgs[0])
    h, w = cam_bgr.shape[:2]
    bgr = maze_raw[:, :, :3]
    a = maze_raw[:, :, 3] if maze_raw.shape[2] == 4 else np.full(maze_raw.shape[:2], 255, np.uint8)
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)

    # warp
    w_alpha = cv2.warpPerspective(a, H, (w, h))
    mask = (w_alpha.astype(np.float32) / 255.0)[..., None] * (alpha_val / 100.0)
    overlay = np.zeros_like(cam_bgr); overlay[:] = (0, 128, 128)
    result = np.clip(cam_bgr * (1 - mask) + overlay * mask, 0, 255).astype(np.uint8)

    # ä¿å­˜å¢™å£/é€šé“ mask + å•åº”æ€§
    wall_src = ((a > 50) & (gray < 100)).astype(np.uint8) * 255
    wall_mask = (cv2.warpPerspective(wall_src, H, (w, h)) > 128).astype(np.uint8) * 255
    corr_src = ((a > 50) & (gray >= 100)).astype(np.uint8) * 255
    corr_mask = (cv2.warpPerspective(corr_src, H, (w, h)) > 128).astype(np.uint8) * 255

    cv2.imwrite(os.path.join(SC_OUT_DIR, "maze_wall_mask.png"), wall_mask)
    cv2.imwrite(os.path.join(SC_OUT_DIR, "maze_corridor_mask.png"), corr_mask)
    cv2.imwrite(os.path.join(SC_OUT_DIR, "registered.png"), result)
    np.save(os.path.join(SC_OUT_DIR, "homography.npy"), H)

    result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    return result_rgb, f"é…å‡†å®Œæˆ!\nH =\n{H}"


def _raycast(cx, cy, dx, dy, mask, hit_val, max_d=RAY_MAX):
    h, w = mask.shape
    for s in range(1, max_d):
        x, y = int(round(cx + dx*s)), int(round(cy + dy*s))
        if x < 0 or x >= w or y < 0 or y >= h:
            return float(s)
        if mask[y, x] == hit_val:
            return float(s)
    return float(max_d)


def _heading_dirs(vx, vy):
    sp = np.sqrt(vx**2 + vy**2)
    if sp < 0.01: vx, vy, sp = 0.0, 1.0, 1.0
    hx, hy = vx/sp, vy/sp
    return (hx, hy), (-hy, hx), (hy, -hx)


def sc_analyze(step_val, progress=gr.Progress()):
    """è¿è¡Œæœºå™¨äººåˆ†æ"""
    h_path = os.path.join(SC_OUT_DIR, "homography.npy")
    if not os.path.exists(h_path):
        return None, None, "è¯·å…ˆå®Œæˆé…å‡†"

    step = max(1, int(step_val))
    maze_path = os.path.join(SC_DIR, "maze1.png")
    maze = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    a, gray = maze[:,:,3], cv2.cvtColor(maze[:,:,:3], cv2.COLOR_BGR2GRAY)
    H = np.load(h_path)

    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    sample = cv2.imread(imgs[0])
    img_h, img_w = sample.shape[:2]

    wall_src = ((a > 50) & (gray < 100)).astype(np.uint8) * 255
    wall_mask = (cv2.warpPerspective(wall_src, H, (img_w, img_h)) > 128).astype(np.uint8) * 255
    corr_src = ((a > 50) & (gray >= 100)).astype(np.uint8) * 255
    corr_mask = (cv2.warpPerspective(corr_src, H, (img_w, img_h)) > 128).astype(np.uint8) * 255

    # è§£ææ ‡ç­¾
    label_dir = os.path.join(SC_DIR, "labels")
    label_files = sorted(glob.glob(os.path.join(label_dir, "*.txt")))
    all_pos = {}
    for lf in label_files:
        fid = int(os.path.splitext(os.path.basename(lf))[0])
        with open(lf) as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) < 7:
                    continue
                coords = list(map(float, parts[1:]))
                pts = np.array([[coords[i]*img_w, coords[i+1]*img_h]
                                for i in range(0, len(coords), 2)])
                all_pos[fid] = tuple(pts.mean(axis=0))
                break

    sampled = sorted(fid for fid in all_pos if fid % step == 0)
    if not sampled:
        return None, None, "æ— æœ‰æ•ˆæ ‡ç­¾"

    results = []
    prev_hd = None
    for i, fid in enumerate(progress.tqdm(sampled, desc="åˆ†æä¸­")):
        cx, cy = all_pos[fid]
        vx = vy = speed = 0.0
        if i > 0:
            px, py = all_pos[sampled[i-1]]
            vx, vy = (cx-px)/step, (cy-py)/step
            speed = np.sqrt(vx**2 + vy**2)

        heading = np.arctan2(vy, vx) if speed > 0.01 else (prev_hd or np.pi/2)
        curv = 0.0
        if prev_hd is not None and speed > 0.01:
            dth = (heading - prev_hd + np.pi) % (2*np.pi) - np.pi
            ds = speed * step
            curv = dth / ds if ds > 0.1 else 0.0
        prev_hd = heading

        _, (lx,ly), (rx,ry) = _heading_dirs(vx, vy)
        dl_w = _raycast(cx, cy, lx, ly, wall_mask, 255)
        dr_w = _raycast(cx, cy, rx, ry, wall_mask, 255)
        dl_c = _raycast(cx, cy, lx, ly, corr_mask, 0)
        dr_c = _raycast(cx, cy, rx, ry, corr_mask, 0)

        results.append({
            "frame": fid,
            "cx": round(float(cx),2), "cy": round(float(cy),2),
            "vx": round(float(vx),3), "vy": round(float(vy),3),
            "speed": round(float(speed),3),
            "heading_deg": round(float(np.degrees(heading)),1),
            "curvature": round(float(curv),4),
            "dist_l_wall": round(float(dl_w),1), "dist_r_wall": round(float(dr_w),1),
            "dist_l_corr": round(float(dl_c),1), "dist_r_corr": round(float(dr_c),1),
            "corridor_w": round(float(dl_c + dr_c),1),
        })

    # ä¿å­˜ CSV
    os.makedirs(SC_OUT_DIR, exist_ok=True)
    csv_path = os.path.join(SC_OUT_DIR, "robot_metrics.csv")
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=results[0].keys())
        w.writeheader()
        w.writerows(results)

    # å¯è§†åŒ–è½¨è¿¹
    vis = sample.copy()
    c1, _ = cv2.findContours(corr_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(vis, c1, -1, (200,180,0), 1)
    c2, _ = cv2.findContours(wall_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(vis, c2, -1, (0,0,200), 1)

    max_k = max((abs(r["curvature"]) for r in results[1:]), default=0.001) or 0.001
    for j in range(1, len(results)):
        p1 = (int(results[j-1]["cx"]), int(results[j-1]["cy"]))
        p2 = (int(results[j]["cx"]), int(results[j]["cy"]))
        kn = min(abs(results[j]["curvature"]) / max_k, 1.0)
        cv2.line(vis, p1, p2, (0, int(255*(1-kn)), int(255*kn)), 2)
    for j in range(0, len(results), max(1, len(results)//15)):
        r = results[j]
        pt = (int(r["cx"]), int(r["cy"]))
        (hx,hy),_,_ = _heading_dirs(r["vx"], r["vy"])
        cv2.arrowedLine(vis, pt, (int(r["cx"]+hx*15), int(r["cy"]+hy*15)), (255,255,255), 1, tipLength=0.3)
        cv2.circle(vis, pt, 3, (0,255,255), -1)

    vis_path = os.path.join(SC_OUT_DIR, "trajectory.png")
    cv2.imwrite(vis_path, vis)
    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)

    # CSV é¢„è§ˆæ–‡æœ¬
    lines = [",".join(results[0].keys())]
    for r in results[:10]:
        lines.append(",".join(str(v) for v in r.values()))
    if len(results) > 10:
        lines.append(f"... å…± {len(results)} è¡Œ")
    preview = "\n".join(lines)

    return vis_rgb, preview, f"åˆ†æå®Œæˆ! {len(results)} ä¸ªé‡‡æ ·ç‚¹\nCSV: {csv_path}\nè½¨è¿¹: {vis_path}"


def sc_reset():
    return None, None, None, [], [], "å·²é‡ç½®"


# Step 4-3 / 4-4 åç«¯é€»è¾‘å·²è¿ç§»è‡³ maze_processing.py (import mp)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ„å»º Gradio ç•Œé¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_app():
    video_dirs = list_video_dirs()

    with gr.Blocks(
        title="æ‹’ç»æ— æ•ˆåŠ ç­ï¼ï¼ï¼",
    ) as app:
        gr.Markdown("# SAM2 - YOLO â†’ SHARED CONTREOL ç«¯åˆ°ç«¯ æ‹’ç»æ— æ•ˆåŠ ç­ï¼ï¼ï¼")
        gr.Markdown("è§†é¢‘åˆ‡å¸§ â†’ ç‚¹å‡»é€‰ç›®æ ‡ â†’ samåˆ†å‰²ä¼ æ’­ â†’ å¯¼å‡º YOLO æ•°æ®é›† â†’ è®­ç»ƒ YOLO æ¨¡å‹ â†’ SHARED CONTREOL æ•°æ®é›†")

        # â”€â”€ Step 0: è§†é¢‘åˆ‡å¸§ â”€â”€
        with gr.Accordion("Step 0: è§†é¢‘åˆ‡å¸§", open=False):
            with gr.Row():
                video_file_dropdown = gr.Dropdown(
                    choices=list_video_files(), label="é€‰æ‹©è§†é¢‘æ–‡ä»¶",
                    info="notebooks/videos/ ä¸‹çš„è§†é¢‘",
                )
                frame_interval = gr.Number(value=3, label="å¸§é—´éš”", info="æ¯Nå¸§å–1å¸§", precision=0)
                extract_btn = gr.Button("åˆ‡å¸§", variant="primary")
            extract_log = gr.Textbox(label="åˆ‡å¸§ç»“æœ", lines=2, interactive=False)

        # â”€â”€ Step 1: SAM é€‰ç›®æ ‡åˆ†å‰² & yolo_dataset åˆ›å»º â”€â”€
        with gr.Accordion("Step 1: SAM é€‰ç›®æ ‡åˆ†å‰², yolo_dataset åˆ›å»º", open=False):
            # â”€â”€ State â”€â”€
            points_state = gr.State([])
            labels_state = gr.State([])

            with gr.Row():
                # â”€â”€ å·¦æ ï¼šè®¾ç½® â”€â”€
                with gr.Column(scale=1):
                    video_dir_dropdown = gr.Dropdown(
                        choices=video_dirs,
                        label="é€‰æ‹©è§†é¢‘å¸§ç›®å½•",
                        info="notebooks/video_to_img/ ä¸‹çš„å­æ–‡ä»¶å¤¹",
                    )
                    frame_slider = gr.Slider(
                        minimum=0, maximum=0, step=1, value=0,
                        label="å¸§ç´¢å¼•",
                        info="é€‰æ‹©è¦æ ‡æ³¨çš„å¸§",
                    )
                    point_type = gr.Radio(
                        choices=["æ­£æ ·æœ¬ (å‰æ™¯)", "è´Ÿæ ·æœ¬ (èƒŒæ™¯)"],
                        value="æ­£æ ·æœ¬ (å‰æ™¯)",
                        label="ç‚¹å‡»ç±»å‹",
                        info="æ­£æ ·æœ¬=ç›®æ ‡åŒºåŸŸï¼Œè´Ÿæ ·æœ¬=æ’é™¤åŒºåŸŸ",
                    )
                    class_id = gr.Number(value=0, label="CLASS_ID", info="YOLO ç±»åˆ« ID", precision=0)
                    info_box = gr.Textbox(label="ä¿¡æ¯", lines=6, interactive=False)

                    with gr.Row():
                        clear_btn = gr.Button("æ¸…é™¤æ‰€æœ‰ç‚¹", variant="secondary", size="sm")
                        undo_btn = gr.Button("æ’¤é”€ä¸Šä¸€ä¸ªç‚¹", variant="secondary", size="sm")

                # â”€â”€ å³æ ï¼šå›¾ç‰‡ â”€â”€
                with gr.Column(scale=2):
                    image_display = gr.Image(
                        label="ç‚¹å‡»å›¾ç‰‡é€‰æ‹©ç›®æ ‡ç‚¹ï¼ˆç»¿è‰²=æ­£æ ·æœ¬ï¼Œçº¢è‰²=è´Ÿæ ·æœ¬ï¼‰",
                        type="pil",
                        interactive=False,
                    )

            with gr.Row():
                export_btn = gr.Button("propagate & å¯¼å‡º YOLO å®ä¾‹åˆ†å‰²æ•°æ®é›†", variant="primary", size="lg")

            with gr.Row():
                preview_image = gr.Image(label="Mask é¢„è§ˆ", type="pil", interactive=False)

            export_log = gr.Textbox(label="logs", lines=15, interactive=False)

        # â”€â”€ Step 2: YOLO è®­ç»ƒ â”€â”€
        with gr.Accordion("Step 2: YOLO è®­ç»ƒ", open=False):
            with gr.Row():
                yolo_dataset_dropdown = gr.Dropdown(
                    choices=list_yolo_datasets(), label="é€‰æ‹©æ•°æ®é›†",
                    info="notebooks/yolo_dataset/ ä¸‹çš„æ•°æ®é›†",
                )
                yolo_model_name = gr.Textbox(value="yolo26s-seg.pt", label="æ¨¡å‹")
                yolo_class_name = gr.Textbox(value="magnet", label="ç±»åˆ«åç§°")
            with gr.Row():
                yolo_epochs = gr.Number(value=30, label="epochs", precision=0)
                yolo_batch = gr.Number(value=32, label="batch", precision=0)
                yolo_imgsz = gr.Number(value=640, label="imgsz", precision=0)
                yolo_val_ratio = gr.Number(value=0.2, label="valæ¯”ä¾‹")
            train_btn = gr.Button("å‡†å¤‡æ•°æ®é›† & å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
            train_log = gr.Textbox(label="è®­ç»ƒæ—¥å¿—", lines=15, interactive=False)
            train_result = gr.Textbox(label="best.pt è·¯å¾„ & scp æŒ‡ä»¤", lines=5, interactive=False)

        # â”€â”€ Step 3: Shared Control Dataset â”€â”€
        with gr.Accordion("Step 3: Shared Control Dataset (è¿·å®«é…å‡† + è¿åŠ¨åˆ†æ)", open=False):
            sc_maze_pts = gr.State([])
            sc_cam_pts = gr.State([])

            with gr.Row():
                sc_dataset_dd = gr.Dropdown(
                    choices=list_yolo_datasets(), label="é€‰æ‹© YOLO æ•°æ®é›†",
                    info="Step 1 å¯¼å‡ºçš„æ•°æ®é›† (images_vis + labels)",
                )
                sc_load_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")

            gr.Markdown(f"è¿·å®«å›¾å›ºå®šè·¯å¾„: `{os.path.join(SC_DIR, 'maze1.png')}`")
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### è¿·å®«å›¾ (ç‚¹å‡» 4 ä¸ªç‰¹å¾ç‚¹)")
                    sc_maze_img = gr.Image(label="è¿·å®«", interactive=False)
                    sc_maze_info = gr.Textbox(label="è¿·å®«ç‰¹å¾ç‚¹", value="(0/4)")
                with gr.Column():
                    gr.Markdown("#### ç›¸æœºå›¾ (ç‚¹å‡»å¯¹åº” 4 ä¸ªç‚¹)")
                    sc_cam_img = gr.Image(label="ç›¸æœº", interactive=False)
                    sc_cam_info = gr.Textbox(label="ç›¸æœºç‰¹å¾ç‚¹", value="(0/4)")

            with gr.Row():
                sc_alpha = gr.Slider(10, 100, value=70, step=5, label="å åŠ é€æ˜åº¦ %")
                sc_step = gr.Number(value=4, label="é‡‡æ ·é—´éš” (å¸§)", precision=0)
                sc_reg_btn = gr.Button("é…å‡†", variant="primary")
                sc_analyze_btn = gr.Button("è¿åŠ¨åˆ†æ", variant="primary")
                sc_reset_btn = gr.Button("é‡ç½®")

            with gr.Row():
                sc_result_img = gr.Image(label="é…å‡†/è½¨è¿¹ç»“æœ")
            sc_csv_preview = gr.Textbox(label="CSV é¢„è§ˆ", lines=8, interactive=False)
            sc_info = gr.Textbox(label="ä¿¡æ¯", lines=3, interactive=False)

            # Step 3 äº‹ä»¶
            sc_load_btn.click(
                sc_on_dataset_change, [sc_dataset_dd],
                [sc_maze_img, sc_cam_img, sc_result_img, sc_info],
            )
            sc_maze_img.select(
                sc_click_maze, [sc_maze_pts],
                [sc_maze_img, sc_maze_pts, sc_maze_info],
            )
            sc_cam_img.select(
                sc_click_cam, [sc_cam_pts],
                [sc_cam_img, sc_cam_pts, sc_cam_info],
            )
            sc_reg_btn.click(
                sc_register, [sc_maze_pts, sc_cam_pts, sc_alpha],
                [sc_result_img, sc_info],
            )
            sc_analyze_btn.click(
                sc_analyze, [sc_step],
                [sc_result_img, sc_csv_preview, sc_info],
            )
            sc_reset_btn.click(
                sc_reset, [],
                [sc_maze_img, sc_cam_img, sc_result_img, sc_maze_pts, sc_cam_pts, sc_info],
            )

        # â”€â”€ Step 4: æ¨¡æ¿å¯¹é½è¡¥å…¨ (æ¥è‡ª app_template_align) â”€â”€
        with gr.Accordion("Step 4: Shared Control Dataset (mask align + alphaæ ‡ç­¾ç”Ÿæˆ)", open=False):
                                   
            gr.Markdown(
                "Step 4 ä¼šè°ƒç”¨ `app_template_align.py` çš„åŒä¸€å¥—å›è°ƒï¼Œ"
            )

            with gr.Accordion("Step 4-1: mask align", open=False):
                with gr.Row():
                    t4_video_dir_dd = gr.Dropdown(
                        choices=ta.list_video_dirs(),
                        label="é€‰æ‹©è§†é¢‘å¸§ç›®å½•",
                        info="video_to_img/ ä¸‹çš„å­æ–‡ä»¶å¤¹",
                    )
                    t4_template_path = gr.Textbox(
                        value=ta.DEFAULT_TEMPLATE, label="maskè·¯å¾„"
                    )
                    t4_load_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")

                t4_pos_x_state = gr.State(None)
                t4_pos_y_state = gr.State(None)

                with gr.Row():
                    with gr.Column(scale=1):
                        t4_tpl_img = gr.Image(label="maskè½®å»“", interactive=False, height=220)
                        gr.Markdown("ç‚¹å‡»å³å›¾å®šä½ï¼Œæ»‘æ¡è°ƒç¼©æ”¾/æ—‹è½¬/è†¨èƒ€")
                        t4_scale = gr.Slider(0.01, 0.5, 0.075, step=0.005, label="ç¼©æ”¾")
                        t4_angle = gr.Slider(-180, 180, -92.5, step=0.5, label="æ—‹è½¬ (åº¦)")
                        t4_dilate = gr.Slider(0, 10, 3, step=1, label="maskè†¨èƒ€ (px)")
                        t4_alpha = gr.Slider(10, 80, 40, step=5, label="é€æ˜åº¦ %")
                        with gr.Row():
                            t4_update_btn = gr.Button("åˆ·æ–°", variant="secondary")
                            t4_save_btn = gr.Button("ä¿å­˜å¯¹é½", variant="primary")
                        t4_info_box = gr.Textbox(label="çŠ¶æ€", lines=3, interactive=False)
                    with gr.Column(scale=2):
                        t4_preview_img = gr.Image(
                            label="ç‚¹å‡» | ç»¿è½®å»“ | mask | çº¢åå­—ä¸­å¿ƒ",
                            interactive=False,
                        )
                        t4_preview_info = gr.Textbox(label="é¢„è§ˆ", lines=1, interactive=False)
                t4_save_result = gr.Textbox(label="ä¿å­˜ç»“æœ", lines=3, interactive=False)

            with gr.Accordion("Step 4-2: SAMä¼ æ’­ + mask align", open=False):
                t4_pts_state = gr.State([])
                t4_labels_state = gr.State([])

                with gr.Row():
                    with gr.Column(scale=1):
                        t4_frame_slider = gr.Slider(0, 1000, 0, step=1, label="æ ‡æ³¨å¸§ç´¢å¼•")
                        t4_point_type = gr.Radio(
                            ["æ­£æ ·æœ¬ (å‰æ™¯)", "è´Ÿæ ·æœ¬ (èƒŒæ™¯)"],
                            value="æ­£æ ·æœ¬ (å‰æ™¯)",
                            label="ç‚¹å‡»ç±»å‹",
                        )
                        t4_class_id = gr.Number(value=0, label="CLASS_ID", precision=0)
                        t4_track_angle = gr.Checkbox(value=True, label="è§’åº¦è¿½è¸ª")
                        t4_use_endpoints = gr.Checkbox(
                            value=True, label="ç«¯ç‚¹åœ†åŒ¹é…", info="é»˜è®¤å¼€å¯"
                        )
                        with gr.Row():
                            t4_clear_btn = gr.Button("æ¸…é™¤æ ‡æ³¨", variant="secondary")
                            t4_preview_btn = gr.Button("é¢„è§ˆ SAM Mask", variant="secondary")
                        t4_run_btn = gr.Button(
                            "ä¼ æ’­ + è¡¥å…¨ + å¯¼å‡º YOLO", variant="primary", size="lg"
                        )
                        t4_pts_info = gr.Textbox(label="æ ‡æ³¨", lines=2, interactive=False)
                    with gr.Column(scale=2):
                        t4_annotate_img = gr.Image(
                            label="ç‚¹å‡»æ ‡æ³¨ç›®æ ‡ (ç»¿=å‰æ™¯, çº¢=èƒŒæ™¯)", interactive=False
                        )

                with gr.Row():
                    t4_sam_preview_img = gr.Image(label="SAM mask vs æ¨¡æ¿", interactive=False)
                    t4_sam_preview_info = gr.Textbox(label="å¯¹æ¯”ä¿¡æ¯", lines=12, interactive=False)

                t4_log = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=12, interactive=False)

                with gr.Row():
                    t4_browse_slider = gr.Slider(0, 1000, 0, step=1, label="æµè§ˆå¸§ç´¢å¼•")
                with gr.Row():
                    t4_browse_img = gr.Image(label="å·¦: SAMåˆ†å‰² + mask align (images_vis)", interactive=False)
                    t4_browse_img_tpl_only = gr.Image(
                        label="å³: ä»…mask (images_vis_template_only)", interactive=False
                    )
                t4_browse_info = gr.Textbox(label="å¸§ä¿¡æ¯", lines=1, interactive=False)

            with gr.Accordion("Step 4-3: è¿·å®«é…å‡† (maze registration)", open=False):
                with gr.Row():
                    t43_dataset_dd = gr.Dropdown(
                        choices=mp.list_mask_align_datasets(),
                        label="é€‰æ‹© mask align æ•°æ®é›†",
                        info="mask_align_sam2_dataset/ ä¸‹çš„æ–‡ä»¶å¤¹",
                    )
                    t43_load_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")
                gr.Markdown(
                    f"è¿·å®«å›¾: `{mp.MAZE_PATH}` Â· "
                    "**ç‚¹å‡»å³å›¾è®¾ç½®è¿·å®«ä¸­å¿ƒï¼Œæ»‘æ¡è°ƒç¼©æ”¾/æ—‹è½¬ï¼Œå‚æ•°å¯ä¿å­˜/è‡ªåŠ¨åŠ è½½**"
                )

                t43_cx_state = gr.State(None)
                t43_cy_state = gr.State(None)

                with gr.Row():
                    with gr.Column(scale=1):
                        t43_maze_img = gr.Image(
                            label="è¿·å®«åŸå›¾", interactive=False, height=200
                        )
                        t43_scale = gr.Slider(
                            0.05, 2.0, 0.5, step=0.01, label="ç¼©æ”¾"
                        )
                        t43_angle = gr.Slider(
                            -180, 180, 0, step=1, label="æ—‹è½¬ (åº¦)"
                        )
                        t43_alpha = gr.Slider(
                            10, 100, 35, step=5, label="å åŠ é€æ˜åº¦ %"
                        )
                        t43_info = gr.Textbox(label="ä¿¡æ¯", lines=5, interactive=False)
                    with gr.Column(scale=2):
                        t43_overlay_img = gr.Image(
                            label="ç‚¹å‡»è®¾ç½®è¿·å®«ä¸­å¿ƒ | é¢„è§ˆå åŠ æ•ˆæœ",
                            interactive=False,
                        )
        # â”€â”€ äº‹ä»¶ç»‘å®š â”€â”€

        # åˆ‡å¸§
        def extract_and_refresh(video_file, interval):
            log_text, vd_update = on_extract_frames(video_file, interval)
            return log_text, vd_update, vd_update

        extract_btn.click(
            fn=extract_and_refresh,
            inputs=[video_file_dropdown, frame_interval],
            outputs=[extract_log, video_dir_dropdown, t4_video_dir_dd],
        )

        # åˆ‡æ¢è§†é¢‘ç›®å½•
        video_dir_dropdown.change(
            fn=on_video_dir_change,
            inputs=[video_dir_dropdown],
            outputs=[image_display, frame_slider, frame_slider, info_box, points_state, labels_state, preview_image],
        )

        # åˆ‡æ¢å¸§
        frame_slider.release(
            fn=on_frame_change,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state],
            outputs=[image_display],
        )

        # ç‚¹å‡»å›¾ç‰‡æ·»åŠ ç‚¹
        image_display.select(
            fn=on_image_click,
            inputs=[video_dir_dropdown, frame_slider, point_type, points_state, labels_state],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # æ¸…é™¤æ‰€æœ‰ç‚¹
        clear_btn.click(
            fn=on_clear_points,
            inputs=[video_dir_dropdown, frame_slider],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # æ’¤é”€ä¸Šä¸€ä¸ªç‚¹
        undo_btn.click(
            fn=on_undo_point,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # å¯¼å‡º YOLO æ•°æ®é›†ï¼Œå®Œæˆååˆ·æ–° Step 2 æ•°æ®é›†åˆ—è¡¨
        def export_and_refresh(*args):
            log = on_export_yolo(*args)
            return log, gr.update(choices=list_yolo_datasets())

        export_btn.click(
            fn=export_and_refresh,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state, class_id],
            outputs=[export_log, yolo_dataset_dropdown],
        )

        # YOLO è®­ç»ƒ
        train_btn.click(
            fn=prepare_and_train_yolo,
            inputs=[yolo_dataset_dropdown, yolo_model_name, yolo_epochs,
                    yolo_batch, yolo_imgsz, yolo_class_name, yolo_val_ratio],
            outputs=[train_log, train_result],
        )

        # â”€â”€ Step 4: æ¨¡æ¿å¯¹é½å›è°ƒç»‘å®š â”€â”€
        t4_load_btn.click(
            fn=ta.on_load_data,
            inputs=[t4_video_dir_dd, t4_template_path],
            outputs=[t4_tpl_img, t4_preview_img, t4_pos_x_state, t4_pos_y_state, t4_scale, t4_info_box],
        )
        t4_preview_img.select(
            fn=ta.on_click_frame,
            inputs=[t4_pos_x_state, t4_pos_y_state, t4_video_dir_dd, t4_template_path,
                    t4_scale, t4_angle, t4_dilate, t4_alpha],
            outputs=[t4_preview_img, t4_pos_x_state, t4_pos_y_state, t4_preview_info],
        )
        _t4_s_in = [t4_pos_x_state, t4_pos_y_state, t4_video_dir_dd, t4_template_path,
                    t4_scale, t4_angle, t4_dilate, t4_alpha]
        _t4_s_out = [t4_preview_img, t4_preview_info]
        for ctrl in [t4_scale, t4_angle, t4_dilate, t4_alpha]:
            ctrl.release(fn=ta.on_slider_change, inputs=_t4_s_in, outputs=_t4_s_out)
        t4_update_btn.click(fn=ta.on_slider_change, inputs=_t4_s_in, outputs=_t4_s_out)
        t4_save_btn.click(
            fn=ta.on_save_params,
            inputs=[t4_pos_x_state, t4_pos_y_state, t4_video_dir_dd, t4_template_path, t4_scale, t4_angle],
            outputs=[t4_save_result],
        )

        def t4_on_dir_or_frame(vdir, fi):
            img, info = ta.on_s2_load_frame(vdir, fi)
            frames = ta.get_sorted_frames(vdir) if vdir else []
            return (
                img, info,
                gr.update(maximum=max(len(frames) - 1, 0)),
                gr.update(maximum=max(len(frames) - 1, 0)),
                [], [],
            )

        t4_video_dir_dd.change(
            fn=t4_on_dir_or_frame,
            inputs=[t4_video_dir_dd, t4_frame_slider],
            outputs=[t4_annotate_img, t4_pts_info, t4_frame_slider, t4_browse_slider, t4_pts_state, t4_labels_state],
        )
        t4_frame_slider.release(
            fn=lambda vd, fi: ta.on_s2_load_frame(vd, fi),
            inputs=[t4_video_dir_dd, t4_frame_slider],
            outputs=[t4_annotate_img, t4_pts_info],
        )
        t4_annotate_img.select(
            fn=ta.on_s2_click,
            inputs=[t4_video_dir_dd, t4_frame_slider, t4_point_type, t4_pts_state, t4_labels_state],
            outputs=[t4_annotate_img, t4_pts_state, t4_labels_state, t4_pts_info],
        )
        t4_clear_btn.click(
            fn=ta.on_s2_clear,
            inputs=[t4_video_dir_dd, t4_frame_slider],
            outputs=[t4_annotate_img, t4_pts_state, t4_labels_state, t4_pts_info],
        )
        t4_preview_btn.click(
            fn=ta.on_s2_preview_mask,
            inputs=[t4_video_dir_dd, t4_template_path, t4_frame_slider,
                    t4_pts_state, t4_labels_state, t4_pos_x_state, t4_pos_y_state, t4_scale, t4_angle],
            outputs=[t4_sam_preview_img, t4_sam_preview_info],
        )
        t4_run_btn.click(
            fn=ta.on_s2_propagate_and_complete,
            inputs=[t4_video_dir_dd, t4_template_path, t4_frame_slider,
                    t4_pts_state, t4_labels_state, t4_class_id,
                    t4_pos_x_state, t4_pos_y_state, t4_scale, t4_angle, t4_dilate,
                    t4_track_angle, t4_use_endpoints],
            outputs=[t4_log, t4_browse_img],
        )
        def t4_on_browse_dual(vdir, fi):
            fi = int(fi)
            img_name = f"{fi:05d}.jpg"
            left = None
            right = None

            if vdir:
                out_dir = os.path.join(ta.SAM2_DATASET_DIR, vdir)
                left_path = os.path.join(out_dir, "images_vis", img_name)
                right_path = os.path.join(out_dir, "images_vis_template_only", img_name)
                if os.path.exists(left_path):
                    left = cv2.cvtColor(cv2.imread(left_path), cv2.COLOR_BGR2RGB)
                if os.path.exists(right_path):
                    right = cv2.cvtColor(cv2.imread(right_path), cv2.COLOR_BGR2RGB)

            # å¦‚æœç£ç›˜ç»“æœè¿˜æ²¡æœ‰ï¼Œå°±å›é€€åˆ°å†…å­˜å¯è§†åŒ–
            info = f"å¸§ {fi}"
            if left is None:
                left_mem, info_mem = ta.on_s2_browse(vdir, fi)
                left = left_mem
                info = info_mem
            else:
                if right is None:
                    info = f"å¸§ {fi} | æ‰¾ä¸åˆ° images_vis_template_only/{img_name}"
                else:
                    info = f"å¸§ {fi} | å·¦=images_vis | å³=images_vis_template_only"
            return left, right, info

        t4_browse_slider.release(
            fn=t4_on_browse_dual,
            inputs=[t4_video_dir_dd, t4_browse_slider],
            outputs=[t4_browse_img, t4_browse_img_tpl_only, t4_browse_info],
        )

        # â”€â”€ Step 4-3: è¿·å®«é…å‡†å›è°ƒç»‘å®š (æ¥è‡ª maze_processing.py) â”€â”€
        t43_load_btn.click(
            mp.on_load, [t43_dataset_dd],
            [t43_maze_img, t43_overlay_img,
             t43_cx_state, t43_cy_state, t43_scale, t43_angle, t43_info],
        )
        t43_overlay_img.select(
            mp.on_click,
            [t43_cx_state, t43_cy_state, t43_dataset_dd,
             t43_scale, t43_angle, t43_alpha],
            [t43_overlay_img, t43_cx_state, t43_cy_state, t43_info],
        )
        _t43_s_in = [t43_cx_state, t43_cy_state, t43_dataset_dd,
                     t43_scale, t43_angle, t43_alpha]
        _t43_s_out = [t43_overlay_img, t43_info]
        for ctrl in [t43_scale, t43_angle, t43_alpha]:
            ctrl.release(fn=mp.on_slider, inputs=_t43_s_in, outputs=_t43_s_out)

    return app


if __name__ == '__main__':
    app = build_app()
    app.launch(
        server_name='0.0.0.0',
        server_port=7860,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
    )
