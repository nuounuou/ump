#!/usr/bin/env python3
"""
SAM2 Video Segmentation â†’ YOLO Dataset â€” Gradio Web UI (å¸¦æ¨¡æ¿è¡¥å…¨ç‰ˆ)

åŠŸèƒ½å¢å¼º: æ”¯æŒä¸Šä¼ 2Dç‰©ä½“æ¨¡æ¿å›¾ç‰‡, å½“SAMä¼ æ’­ç»“æœå› é®æŒ¡å¯¼è‡´maskä¸å®Œæ•´æ—¶,
         åˆ©ç”¨æ¨¡æ¿è½®å»“å¯¹é½è¡¥å…¨, å¾—åˆ°å®Œæ•´çš„åˆ†å‰²maskã€‚

è¿è¡Œï¼š
cd /home/nuounuou/sam2/notebooks && python app_gradio_template.py

ç„¶åä½ çš„ç”µè„‘):
SSH ç«¯å£è½¬å‘:ssh -L 7861:localhost:7861 nuounuou@172.26.211.82
è®¿é—®:http://localhost:7861

å¦‚æœå‡ºç°ç«¯å£å ç”¨,æ€æ­»è¿›ç¨‹: kill $(lsof -t -i:7861) 2>/dev/null
é‡æ–°è¿è¡Œ:cd /home/nuounuou/sam2/notebooks && python app_gradio_template.py
"""

import os
import sys
import shutil
import subprocess
import numpy as np
import torch
import cv2
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from PIL import Image, ImageDraw
import gradio as gr
import traceback
import csv
import glob
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è·¯å¾„è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from sam2.build_sam import build_sam2_video_predictor

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.join(SCRIPT_DIR, 'video_to_img')
VIDEOS_DIR = os.path.join(SCRIPT_DIR, 'videos')
YOLO_DIR = os.path.join(SCRIPT_DIR, 'yolo')
YOLO_DATASET_DIR = os.path.join(SCRIPT_DIR, 'yolo_dataset')
SAM2_CHECKPOINT = os.path.join(PROJECT_ROOT, 'checkpoints', 'sam2.1_hiera_tiny.pt')
MODEL_CFG = 'configs/sam2.1/sam2.1_hiera_t.yaml'

# é»˜è®¤æ¨¡æ¿è·¯å¾„
DEFAULT_TEMPLATE_PATH = os.path.join(SCRIPT_DIR, 'chain-direct.JPG')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAM2 æ¨¡å‹å…¨å±€ç¼“å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
_predictor = None
_device = None


def get_device():
    global _device
    if _device is None:
        if torch.cuda.is_available():
            _device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            _device = torch.device('mps')
        else:
            _device = torch.device('cpu')
    return _device


def get_predictor():
    global _predictor
    if _predictor is None:
        device = get_device()
        print(f'[SAM2] Loading model on {device} ...')
        _predictor = build_sam2_video_predictor(MODEL_CFG, SAM2_CHECKPOINT, device=device)
        print('[SAM2] Model loaded.')
    return _predictor


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mask_to_yolo_seg(mask, img_w, img_h, simplify_tolerance=2.0):
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return None
    mask_uint8 = (mask_2d > 0).astype(np.uint8) * 255
    mask_h, mask_w = mask_2d.shape
    if mask_h != img_h or mask_w != img_w:
        mask_uint8 = cv2.resize(mask_uint8, (img_w, img_h), interpolation=cv2.INTER_NEAREST)
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) == 0:
        return None
    largest = max(contours, key=cv2.contourArea)
    if simplify_tolerance > 0:
        epsilon = simplify_tolerance * cv2.arcLength(largest, True) / 100.0
        largest = cv2.approxPolyDP(largest, epsilon, True)
    if len(largest) < 3:
        return None
    polygon = largest.reshape(-1, 2).astype(np.float32)
    polygon[:, 0] /= img_w
    polygon[:, 1] /= img_h
    return polygon.flatten().tolist()


def mask_to_yolo_bbox(mask, img_w, img_h):
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return None
    mask_h, mask_w = mask_2d.shape
    coords = np.column_stack(np.where(mask_2d > 0))
    if len(coords) == 0:
        return None
    y_coords, x_coords = coords[:, 0], coords[:, 1]
    x_min_mask, x_max_mask = float(x_coords.min()), float(x_coords.max())
    y_min_mask, y_max_mask = float(y_coords.min()), float(y_coords.max())
    if mask_h != img_h or mask_w != img_w:
        scale_x = img_w / mask_w
        scale_y = img_h / mask_h
        x_min = x_min_mask * scale_x
        x_max = x_max_mask * scale_x
        y_min = y_min_mask * scale_y
        y_max = y_max_mask * scale_y
    else:
        x_min, x_max = x_min_mask, x_max_mask
        y_min, y_max = y_min_mask, y_max_mask
    return int(x_min), int(y_min), int(x_max), int(y_max)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨¡æ¿å¯¹é½è¡¥å…¨å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_template_silhouette(template_path):
    """
    ä»2Dæ¨¡æ¿å›¾ç‰‡æå–ç‰©ä½“çš„äºŒå€¼è½®å»“ã€‚
    è‡ªåŠ¨åˆ¤æ–­ç‰©ä½“æ˜¯æ·±è‰²è¿˜æ˜¯æµ…è‰²ï¼Œæå–æœ€å¤§è¿é€šåŒºåŸŸã€‚

    Args:
        template_path: æ¨¡æ¿å›¾ç‰‡è·¯å¾„ (å¦‚ chain-direct.JPG)
    Returns:
        binary: äºŒå€¼åŒ–mask (H x W, uint8, 0/255), ç‰©ä½“åŒºåŸŸ=255
        contour: æœ€å¤§è½®å»“ (Nx1x2)
    """
    img = cv2.imread(template_path)
    if img is None:
        return None, None
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # å°è¯• OTSU æ­£/åä¸¤ç§
    _, bin_inv = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    _, bin_norm = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # é€‰æ‹©è¾¹ç¼˜åƒç´ å°‘çš„é‚£ä¸ª(ç‰©ä½“åœ¨ä¸­é—´ï¼Œä¸é è¾¹)
    def border_sum(b):
        return (np.sum(b[0, :] > 0) + np.sum(b[-1, :] > 0) +
                np.sum(b[:, 0] > 0) + np.sum(b[:, -1] > 0))
    binary = bin_inv if border_sum(bin_inv) < border_sum(bin_norm) else bin_norm

    # å½¢æ€å­¦æ¸…ç†
    kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_open, iterations=1)
    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_close, iterations=2)

    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return binary, None
    largest = max(contours, key=cv2.contourArea)

    # åªä¿ç•™æœ€å¤§è½®å»“ â†’ å¹²å‡€çš„äºŒå€¼mask
    clean = np.zeros_like(binary)
    cv2.drawContours(clean, [largest], -1, 255, -1)

    return clean, largest


def get_mask_pose(mask_uint8):
    """
    ä»äºŒå€¼maskæå–ä½å§¿ä¿¡æ¯ï¼šä¸­å¿ƒ(cx,cy), æ–¹å‘è§’angle, é¢ç§¯area, å°ºåº¦scaleã€‚
    æ–¹å‘ä½¿ç”¨å›¾åƒçŸ©çš„ä¸»è½´æ–¹å‘ã€‚

    Args:
        mask_uint8: äºŒå€¼mask (H x W, uint8, 0/255)
    Returns:
        dict åŒ…å« cx, cy, angle, area, scale, rect, contour; æˆ– None
    """
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None
    contour = max(contours, key=cv2.contourArea)
    M = cv2.moments(contour)
    if M["m00"] < 1:
        return None

    cx = M["m10"] / M["m00"]
    cy = M["m01"] / M["m00"]
    area = cv2.contourArea(contour)

    # ä¸»è½´æ–¹å‘ (äºŒé˜¶ä¸­å¿ƒçŸ©)
    mu20 = M["mu20"] / M["m00"]
    mu02 = M["mu02"] / M["m00"]
    mu11 = M["mu11"] / M["m00"]
    angle = 0.5 * np.arctan2(2 * mu11, mu20 - mu02)

    # æœ€å°å¤–æ¥çŸ©å½¢
    rect = cv2.minAreaRect(contour)

    return {
        "cx": cx, "cy": cy,
        "angle": angle,
        "area": area,
        "scale": np.sqrt(area),
        "rect": rect,
        "contour": contour,
    }


def warp_template_to_pose(template_binary, template_pose, target_cx, target_cy,
                           target_angle, output_size, scale_ratio):
    """
    å°†æ¨¡æ¿äºŒå€¼maské€šè¿‡ç›¸ä¼¼å˜æ¢æ”¾ç½®åˆ°ç›®æ ‡ä½å§¿ã€‚

    æ­¥éª¤: å¹³ç§»æ¨¡æ¿ä¸­å¿ƒåˆ°åŸç‚¹ â†’ ç¼©æ”¾ â†’ æ—‹è½¬ â†’ å¹³ç§»åˆ°ç›®æ ‡ä½ç½®

    Args:
        template_binary: æ¨¡æ¿äºŒå€¼mask (H_t x W_t, uint8)
        template_pose: æ¨¡æ¿çš„pose dict (å« cx, cy, angle)
        target_cx, target_cy: ç›®æ ‡å¸§ä¸­ç‰©ä½“çš„ä¸­å¿ƒåæ ‡
        target_angle: ç›®æ ‡å¸§ä¸­ç‰©ä½“çš„æ–¹å‘è§’
        output_size: (w, h) è¾“å‡ºå°ºå¯¸
        scale_ratio: ç¼©æ”¾æ¯” (å‚è€ƒå¸§scale / æ¨¡æ¿scale)
    Returns:
        warped_mask: å˜æ¢åçš„äºŒå€¼mask (H x W, uint8, 0/255)
    """
    angle_diff = target_angle - template_pose["angle"]
    cos_a = np.cos(angle_diff)
    sin_a = np.sin(angle_diff)

    tcx, tcy = template_pose["cx"], template_pose["cy"]

    # ä»¿å°„çŸ©é˜µ: T_target * R * S * T_origin^{-1}
    M = np.array([
        [scale_ratio * cos_a, -scale_ratio * sin_a,
         target_cx - scale_ratio * (cos_a * tcx - sin_a * tcy)],
        [scale_ratio * sin_a,  scale_ratio * cos_a,
         target_cy - scale_ratio * (sin_a * tcx + cos_a * tcy)],
    ], dtype=np.float64)

    w, h = output_size
    warped = cv2.warpAffine(template_binary, M, (w, h),
                             flags=cv2.INTER_NEAREST,
                             borderValue=0)
    return warped


def _normalize_mask_to_image(mask, img_w, img_h):
    """å°†mask resizeåˆ°å›¾ç‰‡å°ºå¯¸, è¿”å› uint8 (0/255)"""
    m = np.squeeze(mask)
    if m.ndim != 2:
        return None
    m_uint8 = (m > 0).astype(np.uint8) * 255
    mh, mw = m_uint8.shape
    if mh != img_h or mw != img_w:
        m_uint8 = cv2.resize(m_uint8, (img_w, img_h), interpolation=cv2.INTER_NEAREST)
    return m_uint8


def complete_video_masks(video_segments, template_path, ref_frame_idx,
                          img_w, img_h, completeness_thresh=0.7,
                          angle_smooth_alpha=0.5,
                          log_fn=None):
    """
    ç”¨2Dæ¨¡æ¿è¡¥å…¨è§†é¢‘æ‰€æœ‰å¸§çš„mask (æ ¸å¿ƒç®—æ³•)ã€‚

    åŸç†:
    1. ä»æ¨¡æ¿å›¾ç‰‡æå–å®Œæ•´çš„ç‰©ä½“äºŒå€¼è½®å»“
    2. ç”¨å‚è€ƒå¸§(æœ‰å®Œæ•´SAM mask)å»ºç«‹ æ¨¡æ¿â†’å›¾åƒ çš„ç¼©æ”¾æ˜ å°„
    3. é€å¸§æ£€æµ‹: å¦‚æœå½“å‰maské¢ç§¯ < å‚è€ƒå¸§é¢ç§¯ Ã— completeness_thresh â†’ è§¦å‘è¡¥å…¨
    4. ç”¨å¯è§éƒ¨åˆ†çš„centroidä¼°è®¡ä½ç½®, momentsä¼°è®¡æ–¹å‘, å›ºå®šç¼©æ”¾æ¯”
    5. å°†å®Œæ•´æ¨¡æ¿warpåˆ°ä¼°è®¡ä½å§¿, ä¸SAM partial maskå–å¹¶é›†

    Args:
        video_segments: {frame_idx: {obj_id: mask}} SAMä¼ æ’­ç»“æœ
        template_path: æ¨¡æ¿å›¾ç‰‡è·¯å¾„ (å¦‚ chain-direct.JPG)
        ref_frame_idx: å‚è€ƒå¸§ç´¢å¼• (æœ‰å®Œæ•´maskçš„å¸§, é€šå¸¸æ˜¯æ ‡æ³¨å¸§)
        img_w, img_h: è§†é¢‘å¸§å›¾åƒå°ºå¯¸
        completeness_thresh: é¢ç§¯æ¯”ä½äºæ­¤å€¼æ—¶è§¦å‘è¡¥å…¨ (0~1)
        angle_smooth_alpha: è§’åº¦å¹³æ»‘ç³»æ•° (0=å®Œå…¨ç”¨å†å², 1=å®Œå…¨ç”¨å½“å‰)
        log_fn: æ—¥å¿—å›è°ƒå‡½æ•°

    Returns:
        completed_segments: è¡¥å…¨åçš„ {frame_idx: {obj_id: mask(uint8)}}
        info_str: ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
    """
    if log_fn is None:
        log_fn = print

    # â”€â”€ 1. åŠ è½½æ¨¡æ¿ â”€â”€
    template_binary, template_contour = load_template_silhouette(template_path)
    if template_binary is None:
        return video_segments, "âŒ æ¨¡æ¿å›¾ç‰‡åŠ è½½å¤±è´¥, è¯·æ£€æŸ¥è·¯å¾„"
    template_pose = get_mask_pose(template_binary)
    if template_pose is None:
        return video_segments, "âŒ æ¨¡æ¿è½®å»“æå–å¤±è´¥, è¯·æ£€æŸ¥æ¨¡æ¿å›¾ç‰‡"
    log_fn(f"[æ¨¡æ¿] å°ºå¯¸: {template_binary.shape[1]}x{template_binary.shape[0]}, "
           f"é¢ç§¯: {template_pose['area']:.0f}pxÂ²")

    # â”€â”€ 2. è·å–å‚è€ƒå¸§çš„å®Œæ•´mask â†’ å»ºç«‹ç¼©æ”¾æ˜ å°„ â”€â”€
    ref_mask_uint8 = None
    ref_area = 0
    ref_obj_id = None
    if ref_frame_idx in video_segments:
        for obj_id, mask in video_segments[ref_frame_idx].items():
            m = _normalize_mask_to_image(mask, img_w, img_h)
            if m is not None and np.sum(m > 0) > 0:
                ref_mask_uint8 = m
                ref_area = np.sum(m > 0)
                ref_obj_id = obj_id
                break

    if ref_mask_uint8 is None or ref_area == 0:
        return video_segments, "âŒ å‚è€ƒå¸§æ— æœ‰æ•ˆmask, æ— æ³•å»ºç«‹æ˜ å°„"

    ref_pose = get_mask_pose(ref_mask_uint8)
    if ref_pose is None:
        return video_segments, "âŒ å‚è€ƒå¸§maskå§¿æ€æå–å¤±è´¥"

    # å›ºå®šç¼©æ”¾æ¯”: å‚è€ƒå¸§å°ºåº¦ / æ¨¡æ¿å°ºåº¦
    scale_ratio = ref_pose["scale"] / max(template_pose["scale"], 1e-6)
    log_fn(f"[å‚è€ƒå¸§ {ref_frame_idx}] é¢ç§¯: {ref_area}px, "
           f"ç¼©æ”¾æ¯”: {scale_ratio:.4f}, æ–¹å‘: {np.degrees(ref_pose['angle']):.1f}Â°")

    # â”€â”€ 3. é€å¸§å¤„ç† â”€â”€
    completed_segments = {}
    n_completed = 0
    n_kept = 0
    n_empty = 0
    prev_angle = ref_pose["angle"]  # ç”¨äºè§’åº¦å¹³æ»‘

    for fi in sorted(video_segments.keys()):
        completed_segments[fi] = {}
        for obj_id, mask in video_segments[fi].items():
            m_uint8 = _normalize_mask_to_image(mask, img_w, img_h)
            if m_uint8 is None:
                completed_segments[fi][obj_id] = mask
                continue

            current_area = np.sum(m_uint8 > 0)

            # å®Œå…¨ç©º â†’ è·³è¿‡
            if current_area == 0:
                completed_segments[fi][obj_id] = mask
                n_empty += 1
                continue

            completeness = current_area / max(ref_area, 1)

            # è¶³å¤Ÿå®Œæ•´ â†’ ä¸è¡¥å…¨
            if completeness >= completeness_thresh:
                completed_segments[fi][obj_id] = mask
                n_kept += 1
                # æ›´æ–°è§’åº¦å†å²
                cur_pose = get_mask_pose(m_uint8)
                if cur_pose is not None:
                    prev_angle = cur_pose["angle"]
                continue

            # â”€â”€ éœ€è¦è¡¥å…¨ â”€â”€
            cur_pose = get_mask_pose(m_uint8)
            if cur_pose is None:
                completed_segments[fi][obj_id] = mask
                n_empty += 1
                continue

            # ä¼°è®¡ä½ç½®: ç”¨å¯è§éƒ¨åˆ†çš„centroid
            target_cx = cur_pose["cx"]
            target_cy = cur_pose["cy"]

            # ä¼°è®¡è§’åº¦: å¹³æ»‘ (éƒ¨åˆ†é®æŒ¡æ—¶momentsæ–¹å‘ä¸å‡†, å’Œå†å²æ··åˆ)
            raw_angle = cur_pose["angle"]
            # å¤„ç†è§’åº¦ç¯ç»•
            angle_diff = (raw_angle - prev_angle + np.pi) % (2 * np.pi) - np.pi
            smoothed_angle = prev_angle + angle_smooth_alpha * angle_diff
            target_angle = smoothed_angle

            # warp æ¨¡æ¿åˆ°å½“å‰ä¼°è®¡ä½å§¿
            warped = warp_template_to_pose(
                template_binary, template_pose,
                target_cx, target_cy, target_angle,
                (img_w, img_h), scale_ratio
            )

            # éªŒè¯: è®¡ç®—é‡å  (warped âˆ© SAM) / SAM_area
            overlap = np.sum((warped > 0) & (m_uint8 > 0))
            overlap_ratio = overlap / max(current_area, 1)

            if overlap_ratio > 0.25:
                # å¯¹é½OK â†’ åˆå¹¶ (å–å¹¶é›†)
                merged = np.maximum(warped, m_uint8)
                completed_segments[fi][obj_id] = (merged > 0).astype(np.uint8)
                n_completed += 1
                prev_angle = target_angle
                log_fn(f"  å¸§ {fi}: âœ… è¡¥å…¨ "
                       f"(å®Œæ•´åº¦={completeness:.1%}, é‡å ç‡={overlap_ratio:.1%})")
            else:
                # å¯¹é½å¤±è´¥ â†’ ä¿ç•™åŸå§‹SAMç»“æœ
                completed_segments[fi][obj_id] = mask
                n_kept += 1
                log_fn(f"  å¸§ {fi}: âš ï¸ è·³è¿‡è¡¥å…¨ "
                       f"(é‡å ç‡={overlap_ratio:.1%} å¤ªä½, å¯¹é½å¯èƒ½å¤±è´¥)")

    info = (f"âœ… æ¨¡æ¿è¡¥å…¨å®Œæˆ!\n"
            f"   æ€»å¸§: {len(video_segments)}\n"
            f"   è¡¥å…¨: {n_completed} å¸§\n"
            f"   ä¿ç•™åŸå§‹: {n_kept} å¸§\n"
            f"   ç©ºmask: {n_empty} å¸§\n"
            f"   å®Œæ•´åº¦é˜ˆå€¼: {completeness_thresh:.0%}")
    log_fn(info)
    return completed_segments, info


def preview_template_extraction(template_path):
    """
    é¢„è§ˆæ¨¡æ¿è½®å»“æå–ç»“æœ: å·¦=åŸå›¾+è½®å»“ç»¿çº¿, å³=æå–çš„äºŒå€¼mask
    Returns: PIL Image æˆ– None
    """
    if not template_path or not os.path.exists(template_path):
        return None, "æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨"
    binary, contour = load_template_silhouette(template_path)
    if binary is None:
        return None, "æ¨¡æ¿åŠ è½½å¤±è´¥"

    img = cv2.imread(template_path)
    if img is None:
        return None, "å›¾ç‰‡è¯»å–å¤±è´¥"

    # å·¦å›¾: åŸå›¾ + ç»¿è‰²è½®å»“
    left = img.copy()
    if contour is not None:
        cv2.drawContours(left, [contour], -1, (0, 255, 0), 3)
        pose = get_mask_pose(binary)
        if pose:
            # ç”»ä¸­å¿ƒåå­— + ä¸»è½´æ–¹å‘ç®­å¤´
            cx, cy = int(pose["cx"]), int(pose["cy"])
            cv2.drawMarker(left, (cx, cy), (0, 0, 255), cv2.MARKER_CROSS, 20, 2)
            # ä¸»è½´æ–¹å‘
            length = 60
            dx = int(length * np.cos(pose["angle"]))
            dy = int(length * np.sin(pose["angle"]))
            cv2.arrowedLine(left, (cx, cy), (cx + dx, cy + dy),
                            (255, 0, 0), 2, tipLength=0.3)

    # å³å›¾: äºŒå€¼mask (ç°åº¦è½¬å½©è‰²)
    right = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)

    # æ‹¼æ¥, é«˜åº¦å¯¹é½
    h1, w1 = left.shape[:2]
    h2, w2 = right.shape[:2]
    if h1 != h2:
        scale = h1 / h2
        right = cv2.resize(right, (int(w2 * scale), h1))

    vis = np.hstack([left, right])
    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)

    area = int(np.sum(binary > 0))
    info = (f"æ¨¡æ¿æå–æˆåŠŸ!\n"
            f"  è½®å»“ç‚¹æ•°: {len(contour) if contour is not None else 0}\n"
            f"  ç‰©ä½“é¢ç§¯: {area} pxÂ²\n"
            f"  å·¦=åŸå›¾+è½®å»“(ç»¿), å³=äºŒå€¼mask\n"
            f"  çº¢åå­—=ä¸­å¿ƒ, è“ç®­å¤´=ä¸»è½´æ–¹å‘")
    return Image.fromarray(vis_rgb), info


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è§†é¢‘åˆ‡å¸§ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_video_files():
    """åˆ—å‡º videos/ ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
    if not os.path.isdir(VIDEOS_DIR):
        return []
    exts = ('.mp4', '.avi', '.mov', '.mkv')
    return sorted([f for f in os.listdir(VIDEOS_DIR) if f.lower().endswith(exts)])


def on_extract_frames(video_file, frame_interval):
    """ä»è§†é¢‘ä¸­æŠ½å¸§ï¼Œä¿å­˜åˆ° video_to_img/"""
    if not video_file:
        return "è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶", gr.update()
    video_path = os.path.join(VIDEOS_DIR, video_file)
    output_name = Path(video_file).stem
    output_path = os.path.join(BASE_DIR, output_name)
    os.makedirs(output_path, exist_ok=True)
    frame_interval = max(1, int(frame_interval))
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vf", f"select='not(mod(n,{frame_interval}))'",
        "-vsync", "0", "-q:v", "2", "-start_number", "0",
        os.path.join(output_path, "%05d.jpg"),
    ]
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True,
                       encoding='utf-8', errors='ignore')
        n = len([f for f in os.listdir(output_path) if f.lower().endswith('.jpg')])
        new_dirs = list_video_dirs()
        return (f"åˆ‡å¸§å®Œæˆ: {output_name}, å…± {n} å¸§ (æ¯ {frame_interval} å¸§å– 1 å¸§)\n"
                f"ä¿å­˜è‡³: {output_path}"),\
               gr.update(choices=new_dirs, value=output_name)
    except Exception as e:
        return f"åˆ‡å¸§å¤±è´¥: {e}", gr.update()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¸§/ç›®å½•ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_video_dirs():
    """åˆ—å‡º video_to_img ä¸‹æ‰€æœ‰åŒ…å« jpg çš„å­æ–‡ä»¶å¤¹"""
    dirs = []
    if not os.path.isdir(BASE_DIR):
        return dirs
    for d in sorted(os.listdir(BASE_DIR)):
        full = os.path.join(BASE_DIR, d)
        if os.path.isdir(full):
            jpgs = [f for f in os.listdir(full) if f.lower().endswith(('.jpg', '.jpeg'))]
            if jpgs:
                dirs.append(d)
    return dirs


def get_sorted_frame_names(video_dir_name):
    """è·å–æŸä¸ªè§†é¢‘ç›®å½•ä¸‹æ‰€æœ‰æ’åºåçš„å¸§æ–‡ä»¶å"""
    d = os.path.join(BASE_DIR, video_dir_name)
    names = [f for f in os.listdir(d) if f.lower().endswith(('.jpg', '.jpeg'))]
    names.sort(key=lambda p: int(os.path.splitext(p)[0]))
    return names


def load_frame_image(video_dir_name, frame_idx):
    """åŠ è½½æŒ‡å®šå¸§å›¾ç‰‡ï¼Œè¿”å› PIL Image"""
    names = get_sorted_frame_names(video_dir_name)
    if not names:
        return None
    frame_idx = max(0, min(frame_idx, len(names) - 1))
    path = os.path.join(BASE_DIR, video_dir_name, names[frame_idx])
    return Image.open(path).convert('RGB')


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å›¾ç‰‡ä¸Šç»˜åˆ¶æ ‡è®°ç‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def draw_points_on_image(pil_img, points, labels):
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶é€‰ä¸­çš„ç‚¹ï¼Œæ­£æ ·æœ¬=ç»¿è‰²ï¼Œè´Ÿæ ·æœ¬=çº¢è‰²"""
    img_draw = pil_img.copy()
    draw = ImageDraw.Draw(img_draw)
    r = 6
    for i, (pt, lbl) in enumerate(zip(points, labels)):
        x, y = int(pt[0]), int(pt[1])
        color = (0, 255, 0) if lbl == 1 else (255, 0, 0)
        outline = (255, 255, 255)
        draw.ellipse([x - r, y - r, x + r, y + r], fill=color, outline=outline, width=2)
        # æ˜Ÿå·æ ‡è®°
        draw.line([(x - r - 2, y), (x + r + 2, y)], fill=outline, width=1)
        draw.line([(x, y - r - 2), (x, y + r + 2)], fill=outline, width=1)
        # ç¼–å·
        draw.text((x + r + 4, y - r), str(i + 1), fill='white')
    return img_draw


def draw_mask_overlay(pil_img, mask, alpha=0.45, color=(255, 50, 50)):
    """åœ¨å›¾ç‰‡ä¸Šå åŠ  maskï¼ˆåŠé€æ˜ï¼‰"""
    img_np = np.array(pil_img).copy()
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return pil_img
    # å°† mask resize åˆ°å›¾ç‰‡å°ºå¯¸
    h, w = img_np.shape[:2]
    mh, mw = mask_2d.shape
    if mh != h or mw != w:
        mask_2d = cv2.resize(mask_2d.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
    # é¢œè‰²å åŠ 
    overlay = img_np.copy()
    overlay[mask_2d > 0] = list(color)
    img_np = (img_np * (1 - alpha) + overlay * alpha).astype(np.uint8)
    # è½®å»“
    contours, _ = cv2.findContours(
        (mask_2d > 0).astype(np.uint8) * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )
    cv2.drawContours(img_np, contours, -1, (0, 255, 0), 2)
    return Image.fromarray(img_np)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio å›è°ƒå‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€

def on_video_dir_change(video_dir_name):
    """åˆ‡æ¢è§†é¢‘ç›®å½•æ—¶ï¼šé‡ç½®ä¸€åˆ‡ï¼Œæ˜¾ç¤ºç¬¬ 0 å¸§"""
    if not video_dir_name:
        return None, 0, gr.update(maximum=0), "è¯·é€‰æ‹©è§†é¢‘ç›®å½•", [], [], None
    names = get_sorted_frame_names(video_dir_name)
    total = len(names)
    img = load_frame_image(video_dir_name, 0)
    info = f"ğŸ“‚ {video_dir_name} â€” å…± {total} å¸§ï¼Œå›¾ç‰‡å°ºå¯¸: {img.size[0]}x{img.size[1]}"
    return img, 0, gr.update(maximum=max(total - 1, 0)), info, [], [], None


def on_frame_change(video_dir_name, frame_idx, points_state, labels_state):
    """åˆ‡æ¢å¸§æ—¶ï¼šé‡æ–°ç»˜åˆ¶æ ‡è®°ç‚¹"""
    if not video_dir_name:
        return None
    img = load_frame_image(video_dir_name, frame_idx)
    if img is None:
        return None
    if points_state:
        img = draw_points_on_image(img, points_state, labels_state)
    return img


def on_image_click(video_dir_name, frame_idx, point_type, points_state, labels_state, evt: gr.SelectData):
    """ç”¨æˆ·åœ¨å›¾ç‰‡ä¸Šç‚¹å‡» â†’ æ·»åŠ ä¸€ä¸ªæ ‡è®°ç‚¹"""
    if not video_dir_name:
        return None, points_state, labels_state, "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"

    # evt.index åœ¨ Gradio Image ç»„ä»¶ä¸­æ˜¯ [x, y]ï¼ˆåƒç´ åæ ‡ï¼‰
    x, y = evt.index[0], evt.index[1]
    label = 1 if point_type == "æ­£æ ·æœ¬ (å‰æ™¯)" else 0

    points_state.append([x, y])
    labels_state.append(label)

    # é‡æ–°ç»˜åˆ¶
    img = load_frame_image(video_dir_name, frame_idx)
    img = draw_points_on_image(img, points_state, labels_state)

    # ç”Ÿæˆç‚¹åˆ—è¡¨æ–‡æœ¬
    lines = []
    for i, (pt, lbl) in enumerate(zip(points_state, labels_state)):
        tag = "æ­£æ ·æœ¬" if lbl == 1 else "è´Ÿæ ·æœ¬"
        lines.append(f"  {i + 1}. ({int(pt[0])}, {int(pt[1])})  {tag}")
    info = f"å·²é€‰ {len(points_state)} ä¸ªç‚¹ï¼š\n" + "\n".join(lines)

    return img, points_state, labels_state, info


def on_clear_points(video_dir_name, frame_idx):
    """æ¸…é™¤æ‰€æœ‰é€‰ç‚¹"""
    img = load_frame_image(video_dir_name, frame_idx) if video_dir_name else None
    return img, [], [], "å·²æ¸…é™¤æ‰€æœ‰ç‚¹"


def on_undo_point(video_dir_name, frame_idx, points_state, labels_state):
    """æ’¤é”€æœ€åä¸€ä¸ªç‚¹"""
    if points_state:
        points_state.pop()
        labels_state.pop()
    img = load_frame_image(video_dir_name, frame_idx) if video_dir_name else None
    if img is not None and points_state:
        img = draw_points_on_image(img, points_state, labels_state)

    if points_state:
        lines = []
        for i, (pt, lbl) in enumerate(zip(points_state, labels_state)):
            tag = "æ­£æ ·æœ¬" if lbl == 1 else "è´Ÿæ ·æœ¬"
            lines.append(f"  {i + 1}. ({int(pt[0])}, {int(pt[1])})  {tag}")
        info = f"å·²é€‰ {len(points_state)} ä¸ªç‚¹ï¼š\n" + "\n".join(lines)
    else:
        info = "å·²æ¸…é™¤æ‰€æœ‰ç‚¹"
    return img, points_state, labels_state, info


def on_preview_mask(video_dir_name, frame_idx, points_state, labels_state):
    """é¢„è§ˆå•å¸§ mask"""
    if not video_dir_name:
        return None, "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not points_state:
        return None, "è¯·å…ˆåœ¨å›¾ç‰‡ä¸Šç‚¹å‡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ ‡è®°ç‚¹"

    try:
        predictor = get_predictor()
        video_path = os.path.join(BASE_DIR, video_dir_name)
        inference_state = predictor.init_state(video_path=video_path)

        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)

        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=int(frame_idx),
            obj_id=1,
            points=points_np,
            labels=labels_np,
        )

        # å–ç¬¬ä¸€ä¸ªç›®æ ‡çš„ mask
        mask = (out_mask_logits[0] > 0.0).cpu().numpy()

        img = load_frame_image(video_dir_name, frame_idx)
        result_img = draw_mask_overlay(img, mask)
        result_img = draw_points_on_image(result_img, points_state, labels_state)

        # é‡Šæ”¾
        predictor.reset_state(inference_state)

        mask_pixels = int(np.sum(np.squeeze(mask) > 0))
        return result_img, f"Mask é¢„è§ˆæˆåŠŸï¼mask åƒç´ æ•°: {mask_pixels}"
    except Exception as e:
        traceback.print_exc()
        return None, f"é¢„è§ˆå¤±è´¥: {str(e)}"


def on_export_yolo(video_dir_name, frame_idx, points_state, labels_state, class_id,
                   enable_completion, template_path, completion_thresh,
                   angle_smooth, progress=gr.Progress()):
    """
    è¿è¡Œ SAM2 å…¨åºåˆ—ä¼ æ’­ â†’ (å¯é€‰) æ¨¡æ¿è¡¥å…¨ â†’ å¯¼å‡º YOLO æ•°æ®é›†

    ç›¸æ¯”åŸç‰ˆæ–°å¢:
    - enable_completion: æ˜¯å¦å¯ç”¨æ¨¡æ¿è¡¥å…¨
    - template_path: 2Dæ¨¡æ¿å›¾ç‰‡è·¯å¾„
    - completion_thresh: å®Œæ•´åº¦é˜ˆå€¼
    - angle_smooth: è§’åº¦å¹³æ»‘ç³»æ•°
    """
    if not video_dir_name:
        return "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not points_state:
        return "è¯·å…ˆåœ¨å›¾ç‰‡ä¸Šç‚¹å‡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ ‡è®°ç‚¹"

    try:
        log_lines = []

        def log(msg):
            log_lines.append(msg)
            print(msg)

        predictor = get_predictor()
        video_path = os.path.join(BASE_DIR, video_dir_name)

        log(f"è§†é¢‘ç›®å½•: {video_path}")
        log(f"è®¾å¤‡: {get_device()}")
        log(f"æ ‡è®°å¸§: {frame_idx}, é€‰ç‚¹æ•°: {len(points_state)}, CLASS_ID: {class_id}")
        if enable_completion:
            log(f"ğŸ”§ æ¨¡æ¿è¡¥å…¨å·²å¯ç”¨: {template_path}")
            log(f"   å®Œæ•´åº¦é˜ˆå€¼: {completion_thresh}, è§’åº¦å¹³æ»‘: {angle_smooth}")

        # åˆå§‹åŒ–
        progress(0.0, desc="åˆå§‹åŒ– SAM2 ...")
        inference_state = predictor.init_state(video_path=video_path)

        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)

        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=int(frame_idx),
            obj_id=1,
            points=points_np,
            labels=labels_np,
        )
        log(f"å·²æ·»åŠ æç¤ºç‚¹ï¼Œç›®æ ‡æ•°: {len(out_obj_ids)}")

        # æ­£å‘ä¼ æ’­
        progress(0.1, desc="æ­£å‘ä¼ æ’­ä¸­ ...")
        video_segments = {}
        for out_frame_idx, out_obj_ids_prop, out_mask_logits_prop in predictor.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {
                out_obj_id: (out_mask_logits_prop[i] > 0.0).cpu().numpy()
                for i, out_obj_id in enumerate(out_obj_ids_prop)
            }
        log(f"ä¼ æ’­å®Œæˆ: {len(video_segments)} å¸§æœ‰ mask")

        # é‡Šæ”¾
        predictor.reset_state(inference_state)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ–°å¢: æ¨¡æ¿è¡¥å…¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if enable_completion and template_path and os.path.exists(template_path):
            progress(0.3, desc="æ¨¡æ¿å¯¹é½è¡¥å…¨ä¸­ ...")
            # è·å–å›¾ç‰‡å°ºå¯¸
            frame_names_tmp = get_sorted_frame_names(video_dir_name)
            sample_path = os.path.join(video_path, frame_names_tmp[0])
            sample_img = Image.open(sample_path)
            _iw, _ih = sample_img.size

            log(f"\n{'â”€' * 40}")
            log(f"å¼€å§‹æ¨¡æ¿è¡¥å…¨ (å›¾ç‰‡å°ºå¯¸: {_iw}x{_ih})")

            video_segments, comp_info = complete_video_masks(
                video_segments,
                template_path=template_path,
                ref_frame_idx=int(frame_idx),
                img_w=_iw, img_h=_ih,
                completeness_thresh=float(completion_thresh),
                angle_smooth_alpha=float(angle_smooth),
                log_fn=log,
            )
            log(f"{'â”€' * 40}\n")
        elif enable_completion:
            log(f"âš ï¸ æ¨¡æ¿è¡¥å…¨å·²å¯ç”¨ä½†æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¡¥å…¨ç»“æŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # å¯¼å‡º YOLO æ•°æ®é›†
        progress(0.4, desc="å¯¼å‡º YOLO æ•°æ®é›† ...")
        output_dir = os.path.join(SCRIPT_DIR, 'yolo_dataset', video_dir_name)
        images_dir = os.path.join(output_dir, 'images')
        labels_dir = os.path.join(output_dir, 'labels')
        vis_dir = os.path.join(output_dir, 'images_vis')
        os.makedirs(images_dir, exist_ok=True)
        os.makedirs(labels_dir, exist_ok=True)
        os.makedirs(vis_dir, exist_ok=True)

        frame_names = get_sorted_frame_names(video_dir_name)
        total_frames = len(frame_names)
        saved_count = 0
        skipped_count = 0
        class_id_int = int(class_id)

        for fi in range(total_frames):
            progress(0.4 + 0.55 * (fi / total_frames), desc=f"å¤„ç†å¸§ {fi + 1}/{total_frames} ...")

            img_path = os.path.join(video_path, frame_names[fi])
            img = Image.open(img_path).convert('RGB')
            img_w, img_h = img.size

            label_lines = []
            bboxes_px = []

            if fi in video_segments:
                for obj_id, mask in video_segments[fi].items():
                    mask_2d = np.squeeze(mask)
                    if mask_2d.ndim != 2 or not np.any(mask_2d > 0):
                        continue
                    polygon = mask_to_yolo_seg(mask_2d, img_w, img_h, simplify_tolerance=2.0)
                    if polygon is None or len(polygon) < 6:
                        continue
                    polygon_array = np.array(polygon)
                    if np.any(polygon_array < 0) or np.any(polygon_array > 1):
                        continue
                    polygon_str = ' '.join([f'{coord:.6f}' for coord in polygon])
                    label_lines.append(f"{class_id_int} {polygon_str}\n")
                    bbox = mask_to_yolo_bbox(mask_2d, img_w, img_h)
                    if bbox is not None:
                        bboxes_px.append(bbox)

            # ä¿å­˜å›¾ç‰‡
            img_name = f'{fi:05d}.jpg'
            shutil.copy(img_path, os.path.join(images_dir, img_name))

            # ä¿å­˜æ ‡æ³¨
            label_name = f'{fi:05d}.txt'
            with open(os.path.join(labels_dir, label_name), 'w', encoding='utf-8') as f:
                if label_lines:
                    f.writelines(label_lines)

            if not label_lines:
                skipped_count += 1

            # å¯è§†åŒ–
            fig = plt.figure(figsize=(6, 4), dpi=100)
            ax = plt.gca()
            ax.axis('off')
            ax.set_title(f'frame {fi}')
            ax.imshow(img)
            if fi in video_segments:
                for obj_id, mask in video_segments[fi].items():
                    mask_2d = np.squeeze(mask)
                    if mask_2d.ndim != 2 or not np.any(mask_2d > 0):
                        continue
                    # åŠé€æ˜ mask
                    m = mask_2d.astype(np.float32)
                    h, w = m.shape
                    rgba = np.zeros((h, w, 4), dtype=np.float32)
                    rgba[..., 0] = 1.0
                    rgba[..., 1] = 0.2
                    rgba[..., 2] = 0.2
                    rgba[..., 3] = m * 0.45
                    ax.imshow(rgba)
                    for bbox in bboxes_px:
                        x_min, y_min, x_max, y_max = bbox
                        rect = Rectangle(
                            (x_min, y_min), x_max - x_min + 1, y_max - y_min + 1,
                            fill=False, linewidth=2, edgecolor='green',
                        )
                        ax.add_patch(rect)
            plt.savefig(os.path.join(vis_dir, img_name), bbox_inches='tight', pad_inches=0)
            plt.close(fig)

            saved_count += 1

        progress(1.0, desc="å®Œæˆï¼")

        log(f"\n{'=' * 50}")
        log(f"   å¯¼å‡ºå®Œæˆï¼")
        log(f"   æ€»å¸§æ•°: {total_frames}")
        log(f"   æœ‰ mask çš„å¸§: {saved_count - skipped_count}")
        log(f"   æ—  mask çš„å¸§: {skipped_count}")
        if enable_completion:
            log(f"   (å·²å¯ç”¨æ¨¡æ¿è¡¥å…¨)")
        log(f"   è¾“å‡ºç›®å½•: {output_dir}")
        log(f"     â”œâ”€â”€ images/     (åŸå›¾)")
        log(f"     â”œâ”€â”€ labels/     (YOLO å®ä¾‹åˆ†å‰²label)")
        log(f"     â””â”€â”€ images_vis/ (å¯è§†åŒ–)")

        return "\n".join(log_lines)

    except Exception as e:
        traceback.print_exc()
        return f"å¯¼å‡ºå¤±è´¥: {str(e)}\n{traceback.format_exc()}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2: YOLO è®­ç»ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_yolo_datasets():
    """åˆ—å‡º yolo_dataset/ ä¸‹å¯ç”¨çš„æ•°æ®é›†"""
    if not os.path.isdir(YOLO_DATASET_DIR):
        return []
    return sorted([
        d for d in os.listdir(YOLO_DATASET_DIR)
        if os.path.isdir(os.path.join(YOLO_DATASET_DIR, d, 'images'))
    ])


def prepare_and_train_yolo(dataset_name, model_name, epochs, batch_size, imgsz,
                           class_name, val_ratio, progress=gr.Progress()):
    """å‡†å¤‡æ•°æ®é›† + è®­ç»ƒ YOLO"""
    if not dataset_name:
        return "å…ˆé€‰æ‹©æ•°æ®é›†", ""
    try:
        import random
        log_lines = []

        def log(msg):
            log_lines.append(msg)
            print(msg)

        # â”€â”€ 1. å‡†å¤‡æ•°æ®é›† (split train/val) â”€â”€
        progress(0.0, desc="å‡†å¤‡æ•°æ®é›†-è®­ç»ƒ/éªŒè¯é›†/æµ‹è¯•é›†")
        dataset_root = Path(YOLO_DATASET_DIR) / dataset_name
        images_dir = dataset_root / "images"
        labels_dir = dataset_root / "labels"

        train_img = dataset_root / "train" / "images"
        train_lbl = dataset_root / "train" / "labels"
        val_img = dataset_root / "val" / "images"
        val_lbl = dataset_root / "val" / "labels"
        for d in [train_img, train_lbl, val_img, val_lbl]:
            d.mkdir(parents=True, exist_ok=True)
            for f in d.iterdir():
                f.unlink()

        image_files = [f.stem for f in images_dir.glob("*.jpg")]
        log(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")

        # æ£€æµ‹ç±»åˆ«
        classes = set()
        for lf in labels_dir.glob("*.txt"):
            with open(lf) as f:
                for line in f:
                    if line.strip():
                        classes.add(int(line.strip().split()[0]))
        nc = max(len(classes), 1)
        log(f"æ£€æµ‹åˆ° {nc} ä¸ªç±»åˆ«: {sorted(classes)}")

        # åˆ†å‰²
        rng = random.Random(42)
        rng.shuffle(image_files)
        split_idx = int(len(image_files) * (1 - val_ratio))
        train_files = image_files[:split_idx]
        val_files = image_files[split_idx:]
        log(f"è®­ç»ƒé›†: {len(train_files)}, éªŒè¯é›†: {len(val_files)}")

        for stem in train_files:
            shutil.copy2(images_dir / f"{stem}.jpg", train_img / f"{stem}.jpg")
            src_lbl = labels_dir / f"{stem}.txt"
            if src_lbl.exists():
                shutil.copy2(src_lbl, train_lbl / f"{stem}.txt")

        for stem in val_files:
            shutil.copy2(images_dir / f"{stem}.jpg", val_img / f"{stem}.jpg")
            src_lbl = labels_dir / f"{stem}.txt"
            if src_lbl.exists():
                shutil.copy2(src_lbl, val_lbl / f"{stem}.txt")


        # â”€â”€ 2. æ›´æ–° dataset.yaml â”€â”€
        progress(0.05, desc="æ›´æ–°dataset.yaml")
        rel_path = os.path.relpath(dataset_root, YOLO_DIR)
        names_block = "\n".join([f"  {c}: {class_name}" for c in sorted(classes)]) if classes else f"  0: {class_name}"
        yaml_content = (
            f"path: {rel_path}\n"
            f"train: train/images\n"
            f"val: val/images\n\n"
            f"nc: {nc}\n\n"
            f"names:\n{names_block}\n"
        )
        yaml_path = Path(YOLO_DIR) / "dataset.yaml"
        yaml_path.write_text(yaml_content, encoding='utf-8')
        log(f"å·²æ›´æ–° {yaml_path}")
        log(f"dataset.yaml å†…å®¹:\n{yaml_content}")

        # â”€â”€ 3. è®­ç»ƒ â”€â”€
        progress(0.1, desc="å¼€å§‹ YOLO è®­ç»ƒ")
        from ultralytics import YOLO

        os.chdir(YOLO_DIR)
        model = YOLO(model_name)
        log(f"æ¨¡å‹: {model_name}, epochs={int(epochs)}, batch={int(batch_size)}, imgsz={int(imgsz)}")

        results = model.train(
            data=str(yaml_path),
            task="segment",
            epochs=int(epochs),
            imgsz=int(imgsz),
            batch=int(batch_size),
            device=0 if torch.cuda.is_available() else "cpu",
            workers=4,
            project="runs/segment",
            name=f"seg_{dataset_name}",
            save=True,
            save_period=10,
            plots=True,
            val=True,
            patience=50,
        )

        progress(0.95, desc="ä¿å­˜æ¨¡å‹")
        best_pt_src = Path(results.save_dir) / "weights" / "best.pt"
        best_pt_dst = Path(YOLO_DIR) / "best.pt"
        if best_pt_src.exists():
            shutil.copy2(best_pt_src, best_pt_dst)
            log(f"\nbest.pt å·²å¤åˆ¶åˆ°: {best_pt_dst}")
        else:
            best_pt_dst = best_pt_src
            log(f"\nbest.pt è·¯å¾„: {best_pt_src}")

        progress(1.0, desc="è®­ç»ƒå®Œæˆ")
        log(f"\n{'=' * 50}")
        log(f"è®­ç»ƒå®Œæˆ!")
        log(f"best.pt: {best_pt_dst}")
        log(f"è®­ç»ƒç›®å½•: {results.save_dir}")

        # scp æŒ‡ä»¤
        scp_cmd = f"scp nuounuou@172.26.211.82:{best_pt_dst} ./"
        result_text = (
            f"best.pt è·¯å¾„:\n{best_pt_dst}\n\n"
            f"æ‹·è´åˆ°æœ¬åœ° (scp):\n{scp_cmd} (æ‹·è´å›å»çš„è·¯å¾„è‡ªå·±æ”¹ä¸€ä¸‹!bro)\n\n"
            f"è®­ç»ƒç›®å½•:\n{results.save_dir}"
        )
        return "\n".join(log_lines), result_text

    except Exception as e:
        traceback.print_exc()
        return f"è®­ç»ƒå¤±è´¥: {e}\n{traceback.format_exc()}", ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3: Shared Control Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€

SC_DIR = os.path.join(PROJECT_ROOT, 'shared_control_dataset')
SC_OUT_DIR = os.path.join(SC_DIR, 'registration_output')
RAY_MAX = 200


def _load_maze_rgb(maze_path):
    """åŠ è½½è¿·å®«å›¾ä¸º RGB numpy"""
    m = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    if m is None:
        return None
    if m.shape[2] == 4:
        a = m[:, :, 3:4].astype(np.float32) / 255
        bgr = m[:, :, :3].astype(np.float32)
        rgb = (bgr * a + 255 * (1 - a)).astype(np.uint8)
        return cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
    return cv2.cvtColor(m, cv2.COLOR_BGR2RGB)


def _draw_pts(img, pts, colors=((255,0,0),(0,200,0),(0,0,255),(255,165,0))):
    vis = img.copy()
    h, w = vis.shape[:2]
    r = max(5, min(h, w) // 60)
    for i, (x, y) in enumerate(pts):
        c = colors[i % len(colors)]
        cv2.circle(vis, (int(x), int(y)), r, c, -1)
        cv2.circle(vis, (int(x), int(y)), r, (255,255,255), 2)
        cv2.putText(vis, str(i+1), (int(x)+r+2, int(y)+r),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, c, 2)
    return vis


def sc_on_dataset_change(dataset_name):
    """é€‰æ‹©æ•°æ®é›† â†’ åŠ è½½ç¬¬ä¸€å¸§ + å¤åˆ¶æ•°æ® + åŠ è½½è¿·å®«å›¾"""
    if not dataset_name:
        return None, None, None, "è¯·é€‰æ‹©æ•°æ®é›†"
    src = os.path.join(YOLO_DATASET_DIR, dataset_name)
    # å¤åˆ¶ images_vis & labels åˆ° shared_control_dataset
    for sub in ("images_vis", "labels"):
        dst = os.path.join(SC_DIR, sub)
        src_sub = os.path.join(src, sub)
        if os.path.isdir(src_sub):
            if os.path.isdir(dst):
                shutil.rmtree(dst)
            shutil.copytree(src_sub, dst)
    # åŠ è½½ç¬¬ä¸€å¸§
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    if not imgs:
        return None, None, None, "æœªæ‰¾åˆ°å›¾åƒ"
    cam = cv2.cvtColor(cv2.imread(imgs[0]), cv2.COLOR_BGR2RGB)
    img_h, img_w = cam.shape[:2]
    n_labels = len(glob.glob(os.path.join(SC_DIR, "labels", "*.txt")))
    # åŠ è½½è¿·å®«å›¾
    maze_path = os.path.join(SC_DIR, "maze1.png")
    maze_img = _load_maze_rgb(maze_path) if os.path.exists(maze_path) else None
    info = f"å·²åŠ è½½ {len(imgs)} å¸§, {n_labels} æ ‡ç­¾, å°ºå¯¸ {img_w}x{img_h}"
    if maze_img is not None:
        info += f"\nè¿·å®«å›¾å·²åŠ è½½: {maze_path}"
    else:
        info += f"\nè¿·å®«å›¾ä¸å­˜åœ¨: {maze_path}"
    return maze_img, cam, None, info


def sc_load_maze():
    """åŠ è½½å›ºå®šè·¯å¾„çš„è¿·å®«å›¾"""
    maze_path = os.path.join(SC_DIR, "maze1.png")
    if not os.path.exists(maze_path):
        return None, f"è¿·å®«å›¾ä¸å­˜åœ¨: {maze_path}"
    rgb = _load_maze_rgb(maze_path)
    return rgb, f"è¿·å®«å›¾å·²åŠ è½½: {maze_path}"


def sc_click_maze(pts, evt: gr.SelectData):
    x, y = evt.index
    if len(pts) >= 4:
        pts = []
    pts.append([x, y])
    maze_path = os.path.join(SC_DIR, "maze1.png")
    rgb = _load_maze_rgb(maze_path)
    info = f"è¿·å®«ç‚¹ ({len(pts)}/4): " + ", ".join(f"({p[0]},{p[1]})" for p in pts)
    return _draw_pts(rgb, pts) if rgb is not None else None, pts, info


def sc_click_cam(pts, evt: gr.SelectData):
    x, y = evt.index
    if len(pts) >= 4:
        pts = []
    pts.append([x, y])
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    cam = cv2.cvtColor(cv2.imread(imgs[0]), cv2.COLOR_BGR2RGB) if imgs else None
    info = f"ç›¸æœºç‚¹ ({len(pts)}/4): " + ", ".join(f"({p[0]},{p[1]})" for p in pts)
    return _draw_pts(cam, pts) if cam is not None else None, pts, info


def sc_register(maze_pts, cam_pts, alpha_val):
    """é…å‡†è¿·å®«"""
    if len(maze_pts) != 4 or len(cam_pts) != 4:
        return None, "è¯·åœ¨ä¸¤å¼ å›¾ä¸Šå„ç‚¹å‡» 4 ä¸ªå¯¹åº”ç‚¹"
    os.makedirs(SC_OUT_DIR, exist_ok=True)
    maze_path = os.path.join(SC_DIR, "maze1.png")
    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    if not imgs:
        return None, "æœªæ‰¾åˆ°å›¾åƒ"

    src, dst = np.float32(maze_pts), np.float32(cam_pts)
    H, _ = cv2.findHomography(src, dst)

    maze_raw = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    cam_bgr = cv2.imread(imgs[0])
    h, w = cam_bgr.shape[:2]
    bgr = maze_raw[:, :, :3]
    a = maze_raw[:, :, 3] if maze_raw.shape[2] == 4 else np.full(maze_raw.shape[:2], 255, np.uint8)
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)

    # warp
    w_alpha = cv2.warpPerspective(a, H, (w, h))
    mask = (w_alpha.astype(np.float32) / 255.0)[..., None] * (alpha_val / 100.0)
    overlay = np.zeros_like(cam_bgr); overlay[:] = (0, 128, 128)
    result = np.clip(cam_bgr * (1 - mask) + overlay * mask, 0, 255).astype(np.uint8)

    # ä¿å­˜å¢™å£/é€šé“ mask + å•åº”æ€§
    wall_src = ((a > 50) & (gray < 100)).astype(np.uint8) * 255
    wall_mask = (cv2.warpPerspective(wall_src, H, (w, h)) > 128).astype(np.uint8) * 255
    corr_src = ((a > 50) & (gray >= 100)).astype(np.uint8) * 255
    corr_mask = (cv2.warpPerspective(corr_src, H, (w, h)) > 128).astype(np.uint8) * 255

    cv2.imwrite(os.path.join(SC_OUT_DIR, "maze_wall_mask.png"), wall_mask)
    cv2.imwrite(os.path.join(SC_OUT_DIR, "maze_corridor_mask.png"), corr_mask)
    cv2.imwrite(os.path.join(SC_OUT_DIR, "registered.png"), result)
    np.save(os.path.join(SC_OUT_DIR, "homography.npy"), H)

    result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    return result_rgb, f"é…å‡†å®Œæˆ!\nH =\n{H}"


def _raycast(cx, cy, dx, dy, mask, hit_val, max_d=RAY_MAX):
    h, w = mask.shape
    for s in range(1, max_d):
        x, y = int(round(cx + dx*s)), int(round(cy + dy*s))
        if x < 0 or x >= w or y < 0 or y >= h:
            return float(s)
        if mask[y, x] == hit_val:
            return float(s)
    return float(max_d)


def _heading_dirs(vx, vy):
    sp = np.sqrt(vx**2 + vy**2)
    if sp < 0.01: vx, vy, sp = 0.0, 1.0, 1.0
    hx, hy = vx/sp, vy/sp
    return (hx, hy), (-hy, hx), (hy, -hx)


def sc_analyze(step_val, progress=gr.Progress()):
    """è¿è¡Œæœºå™¨äººåˆ†æ"""
    h_path = os.path.join(SC_OUT_DIR, "homography.npy")
    if not os.path.exists(h_path):
        return None, None, "è¯·å…ˆå®Œæˆé…å‡†"

    step = max(1, int(step_val))
    maze_path = os.path.join(SC_DIR, "maze1.png")
    maze = cv2.imread(maze_path, cv2.IMREAD_UNCHANGED)
    a, gray = maze[:,:,3], cv2.cvtColor(maze[:,:,:3], cv2.COLOR_BGR2GRAY)
    H = np.load(h_path)

    imgs = sorted(glob.glob(os.path.join(SC_DIR, "images_vis", "*.jpg")))
    sample = cv2.imread(imgs[0])
    img_h, img_w = sample.shape[:2]

    wall_src = ((a > 50) & (gray < 100)).astype(np.uint8) * 255
    wall_mask = (cv2.warpPerspective(wall_src, H, (img_w, img_h)) > 128).astype(np.uint8) * 255
    corr_src = ((a > 50) & (gray >= 100)).astype(np.uint8) * 255
    corr_mask = (cv2.warpPerspective(corr_src, H, (img_w, img_h)) > 128).astype(np.uint8) * 255

    # è§£ææ ‡ç­¾
    label_dir = os.path.join(SC_DIR, "labels")
    label_files = sorted(glob.glob(os.path.join(label_dir, "*.txt")))
    all_pos = {}
    for lf in label_files:
        fid = int(os.path.splitext(os.path.basename(lf))[0])
        with open(lf) as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) < 7:
                    continue
                coords = list(map(float, parts[1:]))
                pts = np.array([[coords[i]*img_w, coords[i+1]*img_h]
                                for i in range(0, len(coords), 2)])
                all_pos[fid] = tuple(pts.mean(axis=0))
                break

    sampled = sorted(fid for fid in all_pos if fid % step == 0)
    if not sampled:
        return None, None, "æ— æœ‰æ•ˆæ ‡ç­¾"

    results = []
    prev_hd = None
    for i, fid in enumerate(progress.tqdm(sampled, desc="åˆ†æä¸­")):
        cx, cy = all_pos[fid]
        vx = vy = speed = 0.0
        if i > 0:
            px, py = all_pos[sampled[i-1]]
            vx, vy = (cx-px)/step, (cy-py)/step
            speed = np.sqrt(vx**2 + vy**2)

        heading = np.arctan2(vy, vx) if speed > 0.01 else (prev_hd or np.pi/2)
        curv = 0.0
        if prev_hd is not None and speed > 0.01:
            dth = (heading - prev_hd + np.pi) % (2*np.pi) - np.pi
            ds = speed * step
            curv = dth / ds if ds > 0.1 else 0.0
        prev_hd = heading

        _, (lx,ly), (rx,ry) = _heading_dirs(vx, vy)
        dl_w = _raycast(cx, cy, lx, ly, wall_mask, 255)
        dr_w = _raycast(cx, cy, rx, ry, wall_mask, 255)
        dl_c = _raycast(cx, cy, lx, ly, corr_mask, 0)
        dr_c = _raycast(cx, cy, rx, ry, corr_mask, 0)

        results.append({
            "frame": fid,
            "cx": round(float(cx),2), "cy": round(float(cy),2),
            "vx": round(float(vx),3), "vy": round(float(vy),3),
            "speed": round(float(speed),3),
            "heading_deg": round(float(np.degrees(heading)),1),
            "curvature": round(float(curv),4),
            "dist_l_wall": round(float(dl_w),1), "dist_r_wall": round(float(dr_w),1),
            "dist_l_corr": round(float(dl_c),1), "dist_r_corr": round(float(dr_c),1),
            "corridor_w": round(float(dl_c + dr_c),1),
        })

    # ä¿å­˜ CSV
    os.makedirs(SC_OUT_DIR, exist_ok=True)
    csv_path = os.path.join(SC_OUT_DIR, "robot_metrics.csv")
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=results[0].keys())
        w.writeheader()
        w.writerows(results)

    # å¯è§†åŒ–è½¨è¿¹
    vis = sample.copy()
    c1, _ = cv2.findContours(corr_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(vis, c1, -1, (200,180,0), 1)
    c2, _ = cv2.findContours(wall_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(vis, c2, -1, (0,0,200), 1)

    max_k = max((abs(r["curvature"]) for r in results[1:]), default=0.001) or 0.001
    for j in range(1, len(results)):
        p1 = (int(results[j-1]["cx"]), int(results[j-1]["cy"]))
        p2 = (int(results[j]["cx"]), int(results[j]["cy"]))
        kn = min(abs(results[j]["curvature"]) / max_k, 1.0)
        cv2.line(vis, p1, p2, (0, int(255*(1-kn)), int(255*kn)), 2)
    for j in range(0, len(results), max(1, len(results)//15)):
        r = results[j]
        pt = (int(r["cx"]), int(r["cy"]))
        (hx,hy),_,_ = _heading_dirs(r["vx"], r["vy"])
        cv2.arrowedLine(vis, pt, (int(r["cx"]+hx*15), int(r["cy"]+hy*15)), (255,255,255), 1, tipLength=0.3)
        cv2.circle(vis, pt, 3, (0,255,255), -1)

    vis_path = os.path.join(SC_OUT_DIR, "trajectory.png")
    cv2.imwrite(vis_path, vis)
    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)

    # CSV é¢„è§ˆæ–‡æœ¬
    lines = [",".join(results[0].keys())]
    for r in results[:10]:
        lines.append(",".join(str(v) for v in r.values()))
    if len(results) > 10:
        lines.append(f"... å…± {len(results)} è¡Œ")
    preview = "\n".join(lines)

    return vis_rgb, preview, f"åˆ†æå®Œæˆ! {len(results)} ä¸ªé‡‡æ ·ç‚¹\nCSV: {csv_path}\nè½¨è¿¹: {vis_path}"


def sc_reset():
    return None, None, None, [], [], "å·²é‡ç½®"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ„å»º Gradio ç•Œé¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_app():
    video_dirs = list_video_dirs()

    with gr.Blocks(
        title="æ‹’ç»æ— æ•ˆåŠ ç­ï¼ï¼ï¼(æ¨¡æ¿è¡¥å…¨ç‰ˆ)",
    ) as app:
        gr.Markdown("# SAM2 - YOLO â†’ SHARED CONTROL ç«¯åˆ°ç«¯ (æ¨¡æ¿è¡¥å…¨ç‰ˆ)")
        gr.Markdown(
            "è§†é¢‘åˆ‡å¸§ â†’ ç‚¹å‡»é€‰ç›®æ ‡ â†’ SAMåˆ†å‰²ä¼ æ’­ â†’ **2Dæ¨¡æ¿å¯¹é½è¡¥å…¨é®æŒ¡** â†’ "
            "å¯¼å‡º YOLO æ•°æ®é›† â†’ è®­ç»ƒ YOLO æ¨¡å‹ â†’ SHARED CONTROL æ•°æ®é›†"
        )

        # â”€â”€ Step 0: è§†é¢‘åˆ‡å¸§ â”€â”€
        with gr.Accordion("Step 0: è§†é¢‘åˆ‡å¸§", open=False):
            with gr.Row():
                video_file_dropdown = gr.Dropdown(
                    choices=list_video_files(), label="é€‰æ‹©è§†é¢‘æ–‡ä»¶",
                    info="notebooks/videos/ ä¸‹çš„è§†é¢‘",
                )
                frame_interval = gr.Number(value=3, label="å¸§é—´éš”", info="æ¯Nå¸§å–1å¸§", precision=0)
                extract_btn = gr.Button("åˆ‡å¸§", variant="primary")
            extract_log = gr.Textbox(label="åˆ‡å¸§ç»“æœ", lines=2, interactive=False)

        # â”€â”€ Step 1: SAM é€‰ç›®æ ‡åˆ†å‰² & yolo_dataset åˆ›å»º â”€â”€
        with gr.Accordion("Step 1: SAM é€‰ç›®æ ‡åˆ†å‰² + æ¨¡æ¿è¡¥å…¨, yolo_dataset åˆ›å»º", open=True):
            # â”€â”€ State â”€â”€
            points_state = gr.State([])
            labels_state = gr.State([])

            with gr.Row():
                # â”€â”€ å·¦æ ï¼šè®¾ç½® â”€â”€
                with gr.Column(scale=1):
                    video_dir_dropdown = gr.Dropdown(
                        choices=video_dirs,
                        label="é€‰æ‹©è§†é¢‘å¸§ç›®å½•",
                        info="notebooks/video_to_img/ ä¸‹çš„å­æ–‡ä»¶å¤¹",
                    )
                    frame_slider = gr.Slider(
                        minimum=0, maximum=0, step=1, value=0,
                        label="å¸§ç´¢å¼•",
                        info="é€‰æ‹©è¦æ ‡æ³¨çš„å¸§ (å»ºè®®é€‰ç‰©ä½“å®Œå…¨å¯è§çš„å¸§)",
                    )
                    point_type = gr.Radio(
                        choices=["æ­£æ ·æœ¬ (å‰æ™¯)", "è´Ÿæ ·æœ¬ (èƒŒæ™¯)"],
                        value="æ­£æ ·æœ¬ (å‰æ™¯)",
                        label="ç‚¹å‡»ç±»å‹",
                        info="æ­£æ ·æœ¬=ç›®æ ‡åŒºåŸŸï¼Œè´Ÿæ ·æœ¬=æ’é™¤åŒºåŸŸ",
                    )
                    class_id = gr.Number(value=0, label="CLASS_ID", info="YOLO ç±»åˆ« ID", precision=0)
                    info_box = gr.Textbox(label="ä¿¡æ¯", lines=6, interactive=False)

                    with gr.Row():
                        clear_btn = gr.Button("æ¸…é™¤æ‰€æœ‰ç‚¹", variant="secondary", size="sm")
                        undo_btn = gr.Button("æ’¤é”€ä¸Šä¸€ä¸ªç‚¹", variant="secondary", size="sm")

                    # â”€â”€ æ–°å¢: æ¨¡æ¿è¡¥å…¨è®¾ç½® â”€â”€
                    with gr.Accordion("ğŸ”§ æ¨¡æ¿è¡¥å…¨ (é®æŒ¡ä¿®å¤)", open=True):
                        gr.Markdown(
                            "**åŸç†**: å½“ç‰©ä½“è¢«é®æŒ¡å¯¼è‡´ SAM mask ä¸å®Œæ•´æ—¶ï¼Œ"
                            "ç”¨å·²çŸ¥çš„2Då®Œæ•´è½®å»“æ¨¡æ¿å¯¹é½åˆ°å½“å‰ä½ç½®ï¼Œè¡¥å…¨ç¼ºå¤±åŒºåŸŸã€‚\n\n"
                            "**ä½¿ç”¨æ­¥éª¤**: 1) æä¾›ç‰©ä½“2Dæ¨¡æ¿å›¾ â†’ 2) é¢„è§ˆè½®å»“æå– â†’ "
                            "3) å¯¼å‡ºæ—¶è‡ªåŠ¨è¡¥å…¨"
                        )
                        enable_completion = gr.Checkbox(
                            value=False, label="å¯ç”¨æ¨¡æ¿è¡¥å…¨",
                            info="å‹¾é€‰åå¯¼å‡ºæ—¶è‡ªåŠ¨å¯¹é®æŒ¡å¸§è¿›è¡Œè¡¥å…¨"
                        )
                        template_path_input = gr.Textbox(
                            value=DEFAULT_TEMPLATE_PATH,
                            label="æ¨¡æ¿å›¾ç‰‡è·¯å¾„",
                            info="ç‰©ä½“çš„å®Œæ•´2Dè½®å»“å›¾ (å¦‚ chain-direct.JPG)",
                            lines=1,
                        )
                        completion_thresh = gr.Slider(
                            minimum=0.3, maximum=0.95, value=0.7, step=0.05,
                            label="å®Œæ•´åº¦é˜ˆå€¼",
                            info="maské¢ç§¯/å‚è€ƒå¸§é¢ç§¯ ä½äºæ­¤å€¼æ—¶è§¦å‘è¡¥å…¨ (0.7=é¢ç§¯å°‘äº70%å°±è¡¥å…¨)"
                        )
                        angle_smooth_slider = gr.Slider(
                            minimum=0.0, maximum=1.0, value=0.5, step=0.1,
                            label="è§’åº¦å¹³æ»‘ç³»æ•°",
                            info="0=å®Œå…¨ç”¨å†å²è§’åº¦, 1=å®Œå…¨ç”¨å½“å‰å¸§è§’åº¦ (é®æŒ¡ä¸¥é‡æ—¶å»ºè®®ä½å€¼)"
                        )
                        with gr.Row():
                            template_preview_btn = gr.Button(
                                "é¢„è§ˆæ¨¡æ¿è½®å»“", variant="secondary", size="sm"
                            )
                        template_preview_img = gr.Image(
                            label="æ¨¡æ¿è½®å»“é¢„è§ˆ (å·¦=åŸå›¾+è½®å»“, å³=äºŒå€¼mask)",
                            type="pil", interactive=False,
                        )
                        template_preview_info = gr.Textbox(
                            label="æ¨¡æ¿ä¿¡æ¯", lines=3, interactive=False
                        )

                # â”€â”€ å³æ ï¼šå›¾ç‰‡ â”€â”€
                with gr.Column(scale=2):
                    image_display = gr.Image(
                        label="ç‚¹å‡»å›¾ç‰‡é€‰æ‹©ç›®æ ‡ç‚¹ï¼ˆç»¿è‰²=æ­£æ ·æœ¬ï¼Œçº¢è‰²=è´Ÿæ ·æœ¬ï¼‰",
                        type="pil",
                        interactive=False,
                    )

            with gr.Row():
                export_btn = gr.Button(
                    "propagate â†’ (æ¨¡æ¿è¡¥å…¨) â†’ å¯¼å‡º YOLO å®ä¾‹åˆ†å‰²æ•°æ®é›†",
                    variant="primary", size="lg"
                )

            with gr.Row():
                preview_image = gr.Image(label="Mask é¢„è§ˆ", type="pil", interactive=False)

            export_log = gr.Textbox(label="logs", lines=20, interactive=False)

        # â”€â”€ Step 2: YOLO è®­ç»ƒ â”€â”€
        with gr.Accordion("Step 2: YOLO è®­ç»ƒ", open=False):
            with gr.Row():
                yolo_dataset_dropdown = gr.Dropdown(
                    choices=list_yolo_datasets(), label="é€‰æ‹©æ•°æ®é›†",
                    info="notebooks/yolo_dataset/ ä¸‹çš„æ•°æ®é›†",
                )
                yolo_model_name = gr.Textbox(value="yolo26s-seg.pt", label="æ¨¡å‹")
                yolo_class_name = gr.Textbox(value="magnet", label="ç±»åˆ«åç§°")
            with gr.Row():
                yolo_epochs = gr.Number(value=30, label="epochs", precision=0)
                yolo_batch = gr.Number(value=32, label="batch", precision=0)
                yolo_imgsz = gr.Number(value=640, label="imgsz", precision=0)
                yolo_val_ratio = gr.Number(value=0.2, label="valæ¯”ä¾‹")
            train_btn = gr.Button("å‡†å¤‡æ•°æ®é›† & å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
            train_log = gr.Textbox(label="è®­ç»ƒæ—¥å¿—", lines=15, interactive=False)
            train_result = gr.Textbox(label="best.pt è·¯å¾„ & scp æŒ‡ä»¤", lines=5, interactive=False)

        # â”€â”€ Step 3: Shared Control Dataset â”€â”€
        with gr.Accordion("Step 3: Shared Control Dataset (è¿·å®«é…å‡† + è¿åŠ¨åˆ†æ)", open=False):
            sc_maze_pts = gr.State([])
            sc_cam_pts = gr.State([])

            with gr.Row():
                sc_dataset_dd = gr.Dropdown(
                    choices=list_yolo_datasets(), label="é€‰æ‹© YOLO æ•°æ®é›†",
                    info="Step 1 å¯¼å‡ºçš„æ•°æ®é›† (images_vis + labels)",
                )
                sc_load_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")

            gr.Markdown(f"è¿·å®«å›¾å›ºå®šè·¯å¾„: `{os.path.join(SC_DIR, 'maze1.png')}`")
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### è¿·å®«å›¾ (ç‚¹å‡» 4 ä¸ªç‰¹å¾ç‚¹)")
                    sc_maze_img = gr.Image(label="è¿·å®«", interactive=False)
                    sc_maze_info = gr.Textbox(label="è¿·å®«ç‰¹å¾ç‚¹", value="(0/4)")
                with gr.Column():
                    gr.Markdown("#### ç›¸æœºå›¾ (ç‚¹å‡»å¯¹åº” 4 ä¸ªç‚¹)")
                    sc_cam_img = gr.Image(label="ç›¸æœº", interactive=False)
                    sc_cam_info = gr.Textbox(label="ç›¸æœºç‰¹å¾ç‚¹", value="(0/4)")

            with gr.Row():
                sc_alpha = gr.Slider(10, 100, value=70, step=5, label="å åŠ é€æ˜åº¦ %")
                sc_step = gr.Number(value=4, label="é‡‡æ ·é—´éš” (å¸§)", precision=0)
                sc_reg_btn = gr.Button("é…å‡†", variant="primary")
                sc_analyze_btn = gr.Button("è¿åŠ¨åˆ†æ", variant="primary")
                sc_reset_btn = gr.Button("é‡ç½®")

            with gr.Row():
                sc_result_img = gr.Image(label="é…å‡†/è½¨è¿¹ç»“æœ")
            sc_csv_preview = gr.Textbox(label="CSV é¢„è§ˆ", lines=8, interactive=False)
            sc_info = gr.Textbox(label="ä¿¡æ¯", lines=3, interactive=False)

            # Step 3 äº‹ä»¶
            sc_load_btn.click(
                sc_on_dataset_change, [sc_dataset_dd],
                [sc_maze_img, sc_cam_img, sc_result_img, sc_info],
            )
            sc_maze_img.select(
                sc_click_maze, [sc_maze_pts],
                [sc_maze_img, sc_maze_pts, sc_maze_info],
            )
            sc_cam_img.select(
                sc_click_cam, [sc_cam_pts],
                [sc_cam_img, sc_cam_pts, sc_cam_info],
            )
            sc_reg_btn.click(
                sc_register, [sc_maze_pts, sc_cam_pts, sc_alpha],
                [sc_result_img, sc_info],
            )
            sc_analyze_btn.click(
                sc_analyze, [sc_step],
                [sc_result_img, sc_csv_preview, sc_info],
            )
            sc_reset_btn.click(
                sc_reset, [],
                [sc_maze_img, sc_cam_img, sc_result_img, sc_maze_pts, sc_cam_pts, sc_info],
            )

        # â”€â”€ äº‹ä»¶ç»‘å®š â”€â”€

        # åˆ‡å¸§
        extract_btn.click(
            fn=on_extract_frames,
            inputs=[video_file_dropdown, frame_interval],
            outputs=[extract_log, video_dir_dropdown],
        )

        # åˆ‡æ¢è§†é¢‘ç›®å½•
        video_dir_dropdown.change(
            fn=on_video_dir_change,
            inputs=[video_dir_dropdown],
            outputs=[image_display, frame_slider, frame_slider, info_box, points_state, labels_state, preview_image],
        )

        # åˆ‡æ¢å¸§
        frame_slider.release(
            fn=on_frame_change,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state],
            outputs=[image_display],
        )

        # ç‚¹å‡»å›¾ç‰‡æ·»åŠ ç‚¹
        image_display.select(
            fn=on_image_click,
            inputs=[video_dir_dropdown, frame_slider, point_type, points_state, labels_state],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # æ¸…é™¤æ‰€æœ‰ç‚¹
        clear_btn.click(
            fn=on_clear_points,
            inputs=[video_dir_dropdown, frame_slider],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # æ’¤é”€ä¸Šä¸€ä¸ªç‚¹
        undo_btn.click(
            fn=on_undo_point,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state],
            outputs=[image_display, points_state, labels_state, info_box],
        )

        # æ¨¡æ¿é¢„è§ˆ
        template_preview_btn.click(
            fn=preview_template_extraction,
            inputs=[template_path_input],
            outputs=[template_preview_img, template_preview_info],
        )

        # å¯¼å‡º YOLO æ•°æ®é›† (å¸¦æ¨¡æ¿è¡¥å…¨)ï¼Œå®Œæˆååˆ·æ–° Step 2 æ•°æ®é›†åˆ—è¡¨
        def export_and_refresh(vdir, fidx, pts, lbls, cid,
                               en_comp, tpl_path, comp_th, ang_sm):
            log = on_export_yolo(vdir, fidx, pts, lbls, cid,
                                 en_comp, tpl_path, comp_th, ang_sm)
            return log, gr.update(choices=list_yolo_datasets())

        export_btn.click(
            fn=export_and_refresh,
            inputs=[video_dir_dropdown, frame_slider, points_state, labels_state, class_id,
                    enable_completion, template_path_input, completion_thresh, angle_smooth_slider],
            outputs=[export_log, yolo_dataset_dropdown],
        )

        # YOLO è®­ç»ƒ
        train_btn.click(
            fn=prepare_and_train_yolo,
            inputs=[yolo_dataset_dropdown, yolo_model_name, yolo_epochs,
                    yolo_batch, yolo_imgsz, yolo_class_name, yolo_val_ratio],
            outputs=[train_log, train_result],
        )

    return app


if __name__ == '__main__':
    app = build_app()
    app.launch(
        server_name='0.0.0.0',
        server_port=7861,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
    )
