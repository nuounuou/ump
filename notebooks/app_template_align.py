#!/usr/bin/env python3
"""
æ¨¡æ¿å¯¹é½è¡¥å…¨å·¥å…· â€” ç‹¬ç«‹ Gradio åº”ç”¨

è§£å†³é—®é¢˜: SAM2ä¼ æ’­æ—¶ç‰©ä½“è¢«é®æŒ¡ â†’ maskä¸å®Œæ•´ â†’ ç”¨å·²çŸ¥2Dæ¨¡æ¿å¯¹é½è¡¥å…¨

æµç¨‹:
  Step 1: æ‰‹åŠ¨å¯¹é½æ¨¡æ¿åˆ°ç¬¬ä¸€å¸§ (ç‚¹å‡»å®šä½ + ç¼©æ”¾/æ—‹è½¬æ»‘æ¡)
  Step 2: SAMä¼ æ’­ + é€å¸§æ¨¡æ¿è¡¥å…¨ (æ ¹æ®SAM maskåŠ¨æ€è¿½è¸ªä½å§¿) + å¯¼å‡ºYOLO

è¿è¡Œ:
  cd /home/nuounuou/sam2/notebooks && python app_template_align.py

SSH ç«¯å£è½¬å‘:
  ssh -L 7862:localhost:7862 nuounuou@172.26.211.82
  è®¿é—®: http://localhost:7862
"""

import os
import sys
import shutil
import numpy as np
import torch
import cv2
from PIL import Image, ImageDraw
import gradio as gr
import traceback
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è·¯å¾„è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from sam2.build_sam import build_sam2_video_predictor

BASE_DIR = os.path.join(SCRIPT_DIR, 'video_to_img')
DEFAULT_TEMPLATE = os.path.join(SCRIPT_DIR, 'chain-direct.JPG')
YOLO_DATASET_DIR = os.path.join(SCRIPT_DIR, 'yolo_dataset')
SAM2_CHECKPOINT = os.path.join(PROJECT_ROOT, 'checkpoints', 'sam2.1_hiera_tiny.pt')
MODEL_CFG = 'configs/sam2.1/sam2.1_hiera_t.yaml'


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   SAM2 æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_predictor = None
_device = None


def get_device():
    global _device
    if _device is None:
        if torch.cuda.is_available():
            _device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            _device = torch.device('mps')
        else:
            _device = torch.device('cpu')
    return _device


def get_predictor():
    global _predictor
    if _predictor is None:
        device = get_device()
        print(f'[SAM2] Loading model on {device} ...')
        _predictor = build_sam2_video_predictor(MODEL_CFG, SAM2_CHECKPOINT, device=device)
        print('[SAM2] Model loaded.')
    return _predictor


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   å·¥å…·å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def list_video_dirs():
    if not os.path.isdir(BASE_DIR):
        return []
    dirs = []
    for d in sorted(os.listdir(BASE_DIR)):
        full = os.path.join(BASE_DIR, d)
        if os.path.isdir(full):
            jpgs = [f for f in os.listdir(full) if f.lower().endswith(('.jpg', '.jpeg'))]
            if jpgs:
                dirs.append(d)
    return dirs


def get_sorted_frames(video_dir_name):
    d = os.path.join(BASE_DIR, video_dir_name)
    names = [f for f in os.listdir(d) if f.lower().endswith(('.jpg', '.jpeg'))]
    names.sort(key=lambda p: int(os.path.splitext(p)[0]))
    return names


def load_template(template_path):
    """åŠ è½½æ¨¡æ¿å›¾, æå–å®å¿ƒäºŒå€¼mask + æœ€å¤§è½®å»“ + è´¨å¿ƒ"""
    bgr = cv2.imread(template_path)
    if bgr is None:
        return None, None, None, None
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    _, bin_inv = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    k_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    bin_connected = cv2.dilate(bin_inv, k_dilate, iterations=2)
    contours, _ = cv2.findContours(bin_connected, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return bgr, bin_inv, None, None
    largest = max(contours, key=cv2.contourArea)
    filled = np.zeros(gray.shape, dtype=np.uint8)
    cv2.drawContours(filled, [largest], -1, 255, -1)
    M = cv2.moments(largest)
    if M["m00"] > 0:
        cx = M["m10"] / M["m00"]
        cy = M["m01"] / M["m00"]
    else:
        cx, cy = filled.shape[1] / 2, filled.shape[0] / 2
    return bgr, filled, largest, (cx, cy)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   ç«¯ç‚¹åœ†æ£€æµ‹ â€” ç”¨ä¸¤ç«¯åœ†å½¢ç‰¹å¾ä»£æ›¿è´¨å¿ƒè¿›è¡ŒåŒ¹é…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def find_endpoint_circles(mask_uint8):
    """
    ç”¨è·ç¦»å˜æ¢ç²¾ç¡®æ£€æµ‹ mask ä¸¤ç«¯çš„å†…åˆ‡åœ†ç‰¹å¾ã€‚

    åŸç†:
      1. è·ç¦»å˜æ¢: dist[y,x] = è¯¥ç‚¹åˆ°æœ€è¿‘è¾¹ç•Œçš„è·ç¦» = è¯¥ç‚¹å¤„æœ€å¤§å†…åˆ‡åœ†åŠå¾„
      2. å…¨å±€æœ€å¤§å€¼ â†’ ç¬¬ä¸€ä¸ªç«¯ç‚¹åœ†å¿ƒ (æœ€å¤§å†…åˆ‡åœ†)
      3. æŠ‘åˆ¶ç¬¬ä¸€ä¸ªåœ†é™„è¿‘åŒºåŸŸ â†’ å‰©ä½™æœ€å¤§å€¼ = ç¬¬äºŒä¸ªç«¯ç‚¹
      4. å†…åˆ‡åœ†åœ†å¿ƒå’ŒåŠå¾„ç²¾ç¡®å¯¹åº”ç‰©ä½“ä¸¤ç«¯çš„åœ†å½¢é¼“åŒ…ä¸­å¿ƒ

    æ—§æ–¹æ³•(è½®å»“æœ€è¿œç‚¹+å¤–æ¥åœ†)çš„é—®é¢˜:
      - åœ†å¿ƒåœ¨è½®å»“è¾¹ç¼˜è€Œéåœ†å½¢å‡ ä½•ä¸­å¿ƒ
      - åŠå¾„æ˜¯å¤–æ¥åœ†,å¤§äºå®é™…å†…åˆ‡åœ†
      - åå·®å¯è¾¾ç‰©ä½“å®½åº¦çš„40%

    Returns:
        dict: {
            "p1": (x,y), "r1": float,   # ç«¯ç‚¹1å†…åˆ‡åœ†åœ†å¿ƒå’ŒåŠå¾„
            "p2": (x,y), "r2": float,   # ç«¯ç‚¹2å†…åˆ‡åœ†åœ†å¿ƒå’ŒåŠå¾„
            "midpoint": (x,y),           # ä¸¤åœ†å¿ƒçš„ä¸­ç‚¹
            "dist": float,              # ä¸¤åœ†å¿ƒè·ç¦»
            "angle_deg": float,         # p1â†’p2 çš„è§’åº¦ (åº¦)
        }
        or None if detection fails.
    """
    if mask_uint8 is None or np.sum(mask_uint8 > 0) < 100:
        return None

    # â”€â”€ 1. è·ç¦»å˜æ¢ â”€â”€
    dist_map = cv2.distanceTransform(mask_uint8, cv2.DIST_L2, 5)
    max_r = float(dist_map.max())
    if max_r < 3:
        return None

    # â”€â”€ 2. ç¬¬ä¸€ä¸ªç«¯ç‚¹: å…¨å±€æœ€å¤§å€¼ â”€â”€
    y1, x1 = np.unravel_index(np.argmax(dist_map), dist_map.shape)
    r1 = float(dist_map[y1, x1])

    # â”€â”€ 3. æŠ‘åˆ¶ç¬¬ä¸€ä¸ªç«¯ç‚¹å‘¨å›´, æ‰¾ç¬¬äºŒä¸ª â”€â”€
    # æŠ‘åˆ¶åŠå¾„ = 3å€å†…åˆ‡åœ†åŠå¾„, ç¡®ä¿å®Œå…¨è¦†ç›–ç¬¬ä¸€ä¸ªåœ†å½¢é¼“åŒ…
    suppress_r = max(int(r1 * 3), 10)
    dist_suppressed = dist_map.copy()
    cv2.circle(dist_suppressed, (int(x1), int(y1)), suppress_r, 0, -1)

    remaining_max = float(dist_suppressed.max())
    if remaining_max < max(r1 * 0.15, 3):
        # ç¬¬äºŒä¸ªç«¯ç‚¹å¤ªå¼±, å¯èƒ½maskå¤ªå°æˆ–åªæœ‰ä¸€ç«¯å¯è§
        return None

    y2, x2 = np.unravel_index(np.argmax(dist_suppressed), dist_suppressed.shape)
    r2 = float(dist_map[y2, x2])  # ç”¨åŸå§‹è·ç¦»å˜æ¢å€¼

    # â”€â”€ 4. ç²¾åŒ–: åœ¨å³°å€¼é™„è¿‘åšäºšåƒç´ ç²¾ç¡®å®šä½ (åŠ æƒè´¨å¿ƒ) â”€â”€
    for xi, yi, ri, idx in [(x1, y1, r1, 1), (x2, y2, r2, 2)]:
        refine_r = max(int(ri * 0.5), 3)
        y_lo = max(0, yi - refine_r)
        y_hi = min(dist_map.shape[0], yi + refine_r + 1)
        x_lo = max(0, xi - refine_r)
        x_hi = min(dist_map.shape[1], xi + refine_r + 1)
        patch = dist_map[y_lo:y_hi, x_lo:x_hi]
        # åªç”¨é«˜äºå³°å€¼80%çš„åƒç´ åšåŠ æƒè´¨å¿ƒ
        weight = np.maximum(patch - ri * 0.8, 0)
        total_w = weight.sum()
        if total_w > 0:
            ys_local, xs_local = np.mgrid[0:patch.shape[0], 0:patch.shape[1]]
            cx_local = float(np.sum(xs_local * weight) / total_w)
            cy_local = float(np.sum(ys_local * weight) / total_w)
            refined_x = x_lo + cx_local
            refined_y = y_lo + cy_local
            if idx == 1:
                x1, y1 = refined_x, refined_y
            else:
                x2, y2 = refined_x, refined_y

    # â”€â”€ 5. è®¡ç®—å‡ ä½•ä¿¡æ¯ â”€â”€
    x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)
    dd = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)
    angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))

    if dd < max_r * 0.5:
        # ä¸¤ä¸ª"ç«¯ç‚¹"å¤ªè¿‘, ä¸å¤ªå¯èƒ½æ˜¯çœŸçš„ä¸¤ç«¯
        return None

    return {
        "p1": (x1, y1), "r1": r1,
        "p2": (x2, y2), "r2": r2,
        "midpoint": ((x1 + x2) / 2, (y1 + y2) / 2),
        "dist": dd,
        "angle_deg": angle,
    }


def get_template_endpoints(tpl_binary):
    """ä»æ¨¡æ¿äºŒå€¼ mask æå–ä¸¤ç«¯åœ†å½¢ç‰¹å¾ (æ¨¡æ¿åæ ‡ç³»)"""
    return find_endpoint_circles(tpl_binary, end_percentile=15)


def find_bottom_tpl_key(tpl_ep, tpl_center, ref_pos_x, ref_pos_y, ref_scale, ref_angle_deg):
    """
    ç¡®å®šæ¨¡æ¿å“ªä¸ªç«¯ç‚¹åœ¨å¸§åæ ‡ä¸­å¤„äºåº•éƒ¨(Yæœ€å¤§)ã€‚
    é€šè¿‡Step1çš„ä»¿å°„å˜æ¢å°†æ¨¡æ¿ç«¯ç‚¹æ˜ å°„åˆ°å¸§åæ ‡æ¥åˆ¤æ–­ã€‚

    Returns: "p1" æˆ– "p2"
    """
    if tpl_ep is None:
        return "p1"
    M = make_affine(tpl_center, ref_pos_x, ref_pos_y, ref_scale, ref_angle_deg)
    p1_frame = M @ np.array([tpl_ep["p1"][0], tpl_ep["p1"][1], 1.0])
    p2_frame = M @ np.array([tpl_ep["p2"][0], tpl_ep["p2"][1], 1.0])
    return "p1" if p1_frame[1] >= p2_frame[1] else "p2"


def align_from_anchor(tpl_ep, tpl_center, anchor_key, sam_anchor_pos,
                      scale, angle_deg):
    """
    ä»å•ä¸ªé”šç‚¹ç«¯ç‚¹ + å›ºå®š scale/angle è®¡ç®—æ¨¡æ¿ä¸­å¿ƒä½ç½®ã€‚

    åŸç†:
      å·²çŸ¥æ¨¡æ¿é”šç‚¹åœ¨æ¨¡æ¿ç©ºé—´çš„åæ ‡, ä»¥åŠæ¨¡æ¿è´¨å¿ƒåæ ‡ã€‚
      é”šç‚¹â†’è´¨å¿ƒ çš„å‘é‡ç»è¿‡ æ—‹è½¬+ç¼©æ”¾ å˜æ¢å,
      åŠ ä¸ŠSAMé”šç‚¹çš„å¸§åæ ‡ = æ¨¡æ¿è´¨å¿ƒåœ¨å¸§ä¸­çš„ä½ç½®ã€‚

    Args:
        tpl_ep: æ¨¡æ¿ç«¯ç‚¹ä¿¡æ¯
        tpl_center: æ¨¡æ¿è´¨å¿ƒ (cx, cy) æ¨¡æ¿ç©ºé—´
        anchor_key: "p1" æˆ– "p2" â€” ä½¿ç”¨å“ªä¸ªæ¨¡æ¿ç«¯ç‚¹ä½œä¸ºé”š
        sam_anchor_pos: SAMä¸­è¯¥é”šç‚¹çš„æ£€æµ‹ä½ç½® (x, y) å¸§ç©ºé—´
        scale: ç¼©æ”¾ (å›ºå®š, ç‰©ä½“ä¸å˜å½¢)
        angle_deg: æ—‹è½¬è§’ (åº¦)
    """
    tpl_anchor = tpl_ep[anchor_key]
    tpl_cx, tpl_cy = tpl_center

    # é”šç‚¹ â†’ è´¨å¿ƒ å‘é‡ (æ¨¡æ¿ç©ºé—´)
    dx = tpl_cx - tpl_anchor[0]
    dy = tpl_cy - tpl_anchor[1]

    # æ—‹è½¬ + ç¼©æ”¾
    rad = np.radians(angle_deg)
    cos_a, sin_a = np.cos(rad), np.sin(rad)
    dx_frame = scale * (cos_a * dx - sin_a * dy)
    dy_frame = scale * (sin_a * dx + cos_a * dy)

    # æ¨¡æ¿è´¨å¿ƒ = SAMé”šç‚¹ + å˜æ¢åçš„å‘é‡
    pos_x = sam_anchor_pos[0] + dx_frame
    pos_y = sam_anchor_pos[1] + dy_frame

    return {
        "pos_x": float(pos_x), "pos_y": float(pos_y),
        "scale": scale, "angle_deg": angle_deg,
        "method": f"anchor_{anchor_key}",
    }


def compute_angle_from_endpoints(sam_bottom_pos, sam_top_pos,
                                  tpl_ep, bottom_tpl_key):
    """
    ä»ä¸¤å¯¹å¯¹åº”ç«¯ç‚¹è®¡ç®—æ—‹è½¬è§’åº¦ã€‚

    æ—‹è½¬è§’ = SAMç«¯ç‚¹è¿çº¿è§’åº¦ - æ¨¡æ¿ç«¯ç‚¹è¿çº¿è§’åº¦
    """
    top_tpl_key = "p2" if bottom_tpl_key == "p1" else "p1"

    dx_f = sam_top_pos[0] - sam_bottom_pos[0]
    dy_f = sam_top_pos[1] - sam_bottom_pos[1]

    tpl_b = tpl_ep[bottom_tpl_key]
    tpl_t = tpl_ep[top_tpl_key]
    dx_t = tpl_t[0] - tpl_b[0]
    dy_t = tpl_t[1] - tpl_b[1]

    sam_angle = np.degrees(np.arctan2(dy_f, dx_f))
    tpl_angle = np.degrees(np.arctan2(dy_t, dx_t))
    return sam_angle - tpl_angle


# â”€â”€ å…¨å±€ç¼“å­˜ â”€â”€
_cache = {}


def _get_cached(video_dir_name, template_path):
    key = (video_dir_name, template_path)
    if key not in _cache:
        tpl_bgr, tpl_binary, tpl_contour, tpl_center = load_template(template_path)
        tpl_endpoints = get_template_endpoints(tpl_binary) if tpl_binary is not None else None
        frames = get_sorted_frames(video_dir_name)
        frame_path = os.path.join(BASE_DIR, video_dir_name, frames[0])
        frame_bgr = cv2.imread(frame_path)
        _cache[key] = {
            "tpl_bgr": tpl_bgr, "tpl_binary": tpl_binary,
            "tpl_contour": tpl_contour, "tpl_center": tpl_center,
            "tpl_endpoints": tpl_endpoints,
            "frame_bgr": frame_bgr, "frames": frames,
        }
    return _cache[key]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   Pose ä¼°è®¡ + æ¨¡æ¿ Warp + è¡¥å…¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_mask_pose(mask_uint8):
    """
    ä»äºŒå€¼ mask æå– pose: è´¨å¿ƒ(cx,cy), ä¸»è½´è§’åº¦, é¢ç§¯, minAreaRect
    Returns dict or None
    """
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None
    largest = max(contours, key=cv2.contourArea)
    area = cv2.contourArea(largest)
    if area < 10:
        return None
    M = cv2.moments(largest)
    if M["m00"] < 1:
        return None
    cx = M["m10"] / M["m00"]
    cy = M["m01"] / M["m00"]

    # ä¸»è½´è§’åº¦ (ç”¨ moments æ¯” minAreaRect æ›´ç¨³å®š)
    mu20 = M["mu20"]
    mu02 = M["mu02"]
    mu11 = M["mu11"]
    angle_rad = 0.5 * np.arctan2(2 * mu11, mu20 - mu02)

    # minAreaRect ä½œä¸ºè¡¥å……
    rect = cv2.minAreaRect(largest)
    rect_w, rect_h = rect[1]

    return {
        "cx": cx, "cy": cy,
        "angle_rad": angle_rad,
        "angle_deg": np.degrees(angle_rad),
        "area": area,
        "rect_w": rect_w, "rect_h": rect_h,
    }


def make_affine(tpl_center, pos_x, pos_y, scale, angle_deg):
    """æ„å»ºä»¿å°„çŸ©é˜µ: æ¨¡æ¿ä¸­å¿ƒ â†’ åŸç‚¹ â†’ ç¼©æ”¾ â†’ æ—‹è½¬ â†’ å¹³ç§»åˆ°(pos_x, pos_y)"""
    tcx, tcy = tpl_center
    angle_rad = np.radians(angle_deg)
    cos_a = np.cos(angle_rad)
    sin_a = np.sin(angle_rad)
    return np.array([
        [scale * cos_a, -scale * sin_a, pos_x - scale * (cos_a * tcx - sin_a * tcy)],
        [scale * sin_a,  scale * cos_a, pos_y - scale * (sin_a * tcx + cos_a * tcy)],
    ], dtype=np.float64)


def warp_template(tpl_binary, tpl_center, pos_x, pos_y, scale, angle_deg, fw, fh):
    """Warp æ¨¡æ¿åˆ°æŒ‡å®šä½å§¿, è¿”å› warped mask (0/255)"""
    M = make_affine(tpl_center, pos_x, pos_y, scale, angle_deg)
    return cv2.warpAffine(tpl_binary, M, (fw, fh), flags=cv2.INTER_NEAREST)


def complete_single_mask(sam_mask_uint8, tpl_binary, tpl_center,
                         ref_scale, ref_angle_deg,
                         tpl_endpoints=None, use_endpoint_matching=True,
                         bottom_tpl_key="p1",
                         prev_bottom_pos=None, prev_top_pos=None,
                         last_good_angle=None):
    """
    ç”¨æ¨¡æ¿è¡¥å…¨å•å¸§ SAM mask â€” åŸºäºåº•éƒ¨ç«¯ç‚¹é”šå®šç­–ç•¥ã€‚

    æ ¸å¿ƒæ€æƒ³:
      ç‰©ä½“ä¸å˜å½¢ â†’ scaleå›ºå®š (Step1æ ¡å‡†å€¼)
      ç”¨SAM maskä¸­æœ€é ä¸‹(Yæœ€å¤§)çš„åœ†ç«¯ç‚¹ä½œä¸ºé”šç‚¹å®šä½æ¨¡æ¿
      ä¸¤ç«¯éƒ½å¯è§æ—¶è®¡ç®—è§’åº¦, é®æŒ¡æ—¶ä¿æŒä¸Šä¸€å¸§è§’åº¦

    å¯¹é½ä¼˜å…ˆçº§:
      1. æ£€æµ‹SAM maskä¸¤ç«¯åœ†å½¢
      2. é€šè¿‡ä¸ä¸Šä¸€å¸§æœ€è¿‘é‚»åŒ¹é…ç¡®å®šå“ªä¸ªæ˜¯åº•éƒ¨ç«¯ç‚¹
      3. æ£€æŸ¥ç«¯ç‚¹é—´è· â†’ åˆ¤æ–­ä¸¤ç«¯æ˜¯å¦éƒ½æ­£ç¡®æ£€æµ‹
         - åˆç† â†’ åŒç«¯ç‚¹ç®—è§’åº¦ + åº•éƒ¨é”šç‚¹å®šä½
         - ä¸åˆç†(é®æŒ¡) â†’ åº•éƒ¨é”šç‚¹ + ä¸Šä¸€å¸§è§’åº¦ + å›ºå®šscale
      4. æ— ç«¯ç‚¹ â†’ è´¨å¿ƒfallback + å›ºå®šscale/angle

    Returns: (complete_mask, warped_template, tracking_info)
      tracking_info: dict with bottom_pos, top_pos, angle_deg, method
    """
    fh, fw = sam_mask_uint8.shape[:2]

    if np.sum(sam_mask_uint8 > 0) < 50:
        return sam_mask_uint8, np.zeros_like(sam_mask_uint8), None

    top_tpl_key = "p2" if bottom_tpl_key == "p1" else "p1"
    used_angle = last_good_angle if last_good_angle is not None else ref_angle_deg
    current_bottom_pos = None
    current_top_pos = None
    align = None

    # â”€â”€ ç­–ç•¥1: ç«¯ç‚¹é”šå®š â”€â”€
    if use_endpoint_matching and tpl_endpoints is not None:
        sam_ep = find_endpoint_circles(sam_mask_uint8)

        if sam_ep is not None:
            sp1 = np.array(sam_ep["p1"])
            sp2 = np.array(sam_ep["p2"])

            # åŒ¹é…åº•éƒ¨ç«¯ç‚¹: è·ä¸Šä¸€å¸§åº•éƒ¨ä½ç½®æœ€è¿‘çš„
            if prev_bottom_pos is not None:
                prev_b = np.array(prev_bottom_pos)
                d1 = np.linalg.norm(sp1 - prev_b)
                d2 = np.linalg.norm(sp2 - prev_b)
                if d1 <= d2:
                    sam_bottom = (float(sp1[0]), float(sp1[1]))
                    sam_top = (float(sp2[0]), float(sp2[1]))
                else:
                    sam_bottom = (float(sp2[0]), float(sp2[1]))
                    sam_top = (float(sp1[0]), float(sp1[1]))
            else:
                # é¦–æ¬¡: Yå€¼æœ€å¤§(å›¾åƒåº•éƒ¨)çš„ä¸ºåº•éƒ¨
                if sp1[1] >= sp2[1]:
                    sam_bottom = (float(sp1[0]), float(sp1[1]))
                    sam_top = (float(sp2[0]), float(sp2[1]))
                else:
                    sam_bottom = (float(sp2[0]), float(sp2[1]))
                    sam_top = (float(sp1[0]), float(sp1[1]))

            current_bottom_pos = sam_bottom
            current_top_pos = sam_top

            # æ£€æŸ¥ç«¯ç‚¹é—´è· â†’ åˆ¤æ–­ä¸¤ç«¯æ˜¯å¦éƒ½æ­£ç¡®æ£€æµ‹
            expected_dist = tpl_endpoints["dist"] * ref_scale
            actual_dist = np.linalg.norm(
                np.array(sam_bottom) - np.array(sam_top))
            dist_ratio = actual_dist / max(expected_dist, 1)

            if 0.65 < dist_ratio < 1.5:
                # ä¸¤ç«¯éƒ½æ­£ç¡® â†’ ä»ä¸¤ç«¯ç‚¹è®¡ç®—è§’åº¦
                used_angle = compute_angle_from_endpoints(
                    sam_bottom, sam_top, tpl_endpoints, bottom_tpl_key)
                align = align_from_anchor(
                    tpl_endpoints, tpl_center,
                    bottom_tpl_key, sam_bottom,
                    ref_scale, used_angle)
                align["method"] = f"dual_endpoint(ratio={dist_ratio:.2f})"
            else:
                # é®æŒ¡ â†’ åªç”¨åº•éƒ¨é”šç‚¹ + å·²çŸ¥è§’åº¦ + å›ºå®šscale
                align = align_from_anchor(
                    tpl_endpoints, tpl_center,
                    bottom_tpl_key, sam_bottom,
                    ref_scale, used_angle)
                align["method"] = f"bottom_anchor(ratio={dist_ratio:.2f})"

    # â”€â”€ ç­–ç•¥2: è´¨å¿ƒ fallback â”€â”€
    if align is None:
        pose = get_mask_pose(sam_mask_uint8)
        if pose is not None:
            align = {
                "pos_x": pose["cx"], "pos_y": pose["cy"],
                "scale": ref_scale, "angle_deg": used_angle,
                "method": "centroid_fallback",
            }
        else:
            return sam_mask_uint8, np.zeros_like(sam_mask_uint8), None

    # â”€â”€ Warp å®Œæ•´æ¨¡æ¿ (å§‹ç»ˆæ˜¯å®Œæ•´å½¢çŠ¶!) â”€â”€
    warped = warp_template(tpl_binary, tpl_center,
                           align["pos_x"], align["pos_y"],
                           align["scale"], align["angle_deg"], fw, fh)

    # â”€â”€ åˆå¹¶: SAM âˆª æ¨¡æ¿ = å®Œæ•´mask â”€â”€
    complete = np.maximum(sam_mask_uint8, warped)

    # â”€â”€ è¿½è¸ªä¿¡æ¯ (ä¼ ç»™ä¸‹ä¸€å¸§) â”€â”€
    tracking = {
        "pos": (align["pos_x"], align["pos_y"]),
        "scale": align["scale"],
        "angle_deg": align["angle_deg"],
        "method": align["method"],
        "bottom_pos": current_bottom_pos,
        "top_pos": current_top_pos,
    }

    return complete, warped, tracking


def build_overlay(frame_bgr, tpl_binary, tpl_contour, tpl_center,
                  pos_x, pos_y, scale, angle_deg, alpha):
    """æ„å»ºå åŠ å¯è§†åŒ– (Step 1 ç”¨)"""
    fh, fw = frame_bgr.shape[:2]
    M = make_affine(tpl_center, pos_x, pos_y, scale, angle_deg)
    warped_mask = cv2.warpAffine(tpl_binary, M, (fw, fh), flags=cv2.INTER_NEAREST)

    warped_contour = None
    if tpl_contour is not None:
        pts = tpl_contour.reshape(-1, 2).astype(np.float64)
        ones = np.ones((pts.shape[0], 1))
        pts_h = np.hstack([pts, ones])
        warped_pts = (M @ pts_h.T).T
        warped_contour = warped_pts.reshape(-1, 1, 2).astype(np.int32)

    vis = frame_bgr.copy()
    overlay_color = np.zeros_like(vis)
    overlay_color[:] = (0, 200, 200)
    mask_3c = (warped_mask > 0).astype(np.float32)[..., None]
    vis = (vis * (1 - mask_3c * alpha) + overlay_color * mask_3c * alpha).astype(np.uint8)
    if warped_contour is not None:
        cv2.drawContours(vis, [warped_contour], -1, (0, 255, 0), 2)
    cx_i, cy_i = int(pos_x), int(pos_y)
    cv2.drawMarker(vis, (cx_i, cy_i), (0, 0, 255), cv2.MARKER_CROSS, 20, 2)

    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)
    warped_area = int(np.sum(warped_mask > 0))
    return vis_rgb, warped_mask, warped_area


def build_completion_vis(frame_bgr, sam_mask, complete_mask, warped_tpl):
    """æ„å»ºè¡¥å…¨ç»“æœå¯è§†åŒ– (Step 2 ç”¨)"""
    vis = frame_bgr.copy()
    h, w = vis.shape[:2]

    # SAMåŸå§‹mask â†’ è“è‰²
    sam_2d = np.squeeze(sam_mask)
    if sam_2d.shape[:2] != (h, w):
        sam_2d = cv2.resize(sam_2d.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
    sam_only = (sam_2d > 0).astype(np.uint8)

    # æ¨¡æ¿è¡¥å…¨éƒ¨åˆ† â†’ é»„è‰² (warped_tpl ä¸­æœ‰ä½† SAM ä¸­æ²¡æœ‰çš„éƒ¨åˆ†)
    tpl_only = ((warped_tpl > 0) & (sam_2d == 0)).astype(np.uint8)

    # SAM åŒºåŸŸ: è“è‰²åŠé€æ˜
    blue_overlay = np.zeros_like(vis)
    blue_overlay[:] = (200, 100, 0)  # BGR
    sam_3c = sam_only[..., None].astype(np.float32)
    vis = (vis * (1 - sam_3c * 0.4) + blue_overlay * sam_3c * 0.4).astype(np.uint8)

    # æ¨¡æ¿è¡¥å…¨åŒºåŸŸ: é»„è‰²åŠé€æ˜
    yellow_overlay = np.zeros_like(vis)
    yellow_overlay[:] = (0, 200, 200)  # BGR
    tpl_3c = tpl_only[..., None].astype(np.float32)
    vis = (vis * (1 - tpl_3c * 0.5) + yellow_overlay * tpl_3c * 0.5).astype(np.uint8)

    # æœ€ç»ˆè½®å»“: ç»¿è‰²
    comp_2d = (complete_mask > 0).astype(np.uint8) * 255
    contours, _ = cv2.findContours(comp_2d, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(vis, contours, -1, (0, 255, 0), 2)

    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)


def mask_to_yolo_seg(mask, img_w, img_h, simplify_tolerance=2.0):
    """mask â†’ YOLO åˆ†å‰²å¤šè¾¹å½¢"""
    mask_2d = np.squeeze(mask)
    if mask_2d.ndim != 2:
        return None
    mask_uint8 = (mask_2d > 0).astype(np.uint8) * 255
    mh, mw = mask_2d.shape
    if mh != img_h or mw != img_w:
        mask_uint8 = cv2.resize(mask_uint8, (img_w, img_h), interpolation=cv2.INTER_NEAREST)
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None
    largest = max(contours, key=cv2.contourArea)
    if simplify_tolerance > 0:
        epsilon = simplify_tolerance * cv2.arcLength(largest, True) / 100.0
        largest = cv2.approxPolyDP(largest, epsilon, True)
    if len(largest) < 3:
        return None
    polygon = largest.reshape(-1, 2).astype(np.float32)
    polygon[:, 0] /= img_w
    polygon[:, 1] /= img_h
    return polygon.flatten().tolist()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   Step 1 å›è°ƒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def on_load_data(video_dir_name, template_path):
    global _cache
    _cache.clear()
    if not video_dir_name:
        return None, None, None, None, gr.update(), "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not template_path or not os.path.exists(template_path):
        return None, None, None, None, gr.update(), f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}"
    try:
        data = _get_cached(video_dir_name, template_path)
        tpl_bgr, tpl_binary, tpl_contour, tpl_center = (
            data["tpl_bgr"], data["tpl_binary"], data["tpl_contour"], data["tpl_center"])
        frame_bgr, frames = data["frame_bgr"], data["frames"]
        if tpl_bgr is None:
            return None, None, None, None, gr.update(), "æ¨¡æ¿å›¾ç‰‡è¯»å–å¤±è´¥"
        fh, fw = frame_bgr.shape[:2]
        th, tw = tpl_bgr.shape[:2]
        tpl_vis = cv2.cvtColor(tpl_bgr, cv2.COLOR_BGR2RGB).copy()
        if tpl_contour is not None:
            cv2.drawContours(tpl_vis, [tpl_contour], -1, (0, 255, 0), 4)
            cv2.drawMarker(tpl_vis, (int(tpl_center[0]), int(tpl_center[1])),
                           (255, 0, 0), cv2.MARKER_CROSS, 30, 3)
        init_x, init_y = float(fw / 2), float(fh / 2)
        tpl_area = np.sum(tpl_binary > 0) if tpl_binary is not None else tw * th
        init_scale = np.sqrt(fw * fh * 0.08 / max(tpl_area, 1))
        init_scale = round(float(max(0.02, min(init_scale, 2.0))), 3)
        vis_rgb, _, _ = build_overlay(
            frame_bgr, tpl_binary, tpl_contour, tpl_center,
            init_x, init_y, init_scale, 0.0, 0.4)
        info = (f"âœ… åŠ è½½æˆåŠŸ! è§†é¢‘: {video_dir_name} ({len(frames)}å¸§, {fw}x{fh}), "
                f"æ¨¡æ¿: {tw}x{th}\n\nğŸ‘† ç‚¹å‡»é¢„è§ˆå›¾å®šä½ â†’ æ»‘æ¡è°ƒç¼©æ”¾/æ—‹è½¬")
        return tpl_vis, vis_rgb, init_x, init_y, gr.update(value=init_scale), info
    except Exception as e:
        traceback.print_exc()
        return None, None, None, None, gr.update(), f"åŠ è½½å¤±è´¥: {e}"


def on_click_frame(pos_x, pos_y, video_dir_name, template_path,
                   scale, angle_deg, alpha, evt: gr.SelectData):
    x, y = float(evt.index[0]), float(evt.index[1])
    try:
        data = _get_cached(video_dir_name, template_path)
        vis_rgb, _, warped_area = build_overlay(
            data["frame_bgr"], data["tpl_binary"], data["tpl_contour"],
            data["tpl_center"], x, y, scale, angle_deg, alpha / 100.0)
        info = f"ğŸ“ ({x:.0f}, {y:.0f}), ç¼©æ”¾: {scale:.3f}, æ—‹è½¬: {angle_deg:.1f}Â°, è¦†ç›–: {warped_area}pxÂ²"
        return vis_rgb, x, y, info
    except Exception as e:
        return None, pos_x, pos_y, f"é”™è¯¯: {e}"


def on_slider_change(pos_x, pos_y, video_dir_name, template_path,
                     scale, angle_deg, alpha):
    if not video_dir_name or not template_path:
        return None, "è¯·å…ˆåŠ è½½æ•°æ®"
    if pos_x is None or pos_y is None:
        return None, "è¯·å…ˆç‚¹å‡»é¢„è§ˆå›¾è®¾ç½®ä½ç½®"
    try:
        data = _get_cached(video_dir_name, template_path)
        vis_rgb, _, warped_area = build_overlay(
            data["frame_bgr"], data["tpl_binary"], data["tpl_contour"],
            data["tpl_center"], float(pos_x), float(pos_y),
            scale, angle_deg, alpha / 100.0)
        info = f"ğŸ“ ({pos_x:.0f}, {pos_y:.0f}), ç¼©æ”¾: {scale:.3f}, æ—‹è½¬: {angle_deg:.1f}Â°, è¦†ç›–: {warped_area}pxÂ²"
        return vis_rgb, info
    except Exception as e:
        return None, f"é”™è¯¯: {e}"


def on_save_params(pos_x, pos_y, video_dir_name, template_path, scale, angle_deg):
    if not video_dir_name or not template_path:
        return "è¯·å…ˆåŠ è½½æ•°æ®"
    if pos_x is None or pos_y is None:
        return "è¯·å…ˆç‚¹å‡»é¢„è§ˆå›¾è®¾ç½®ä½ç½®"
    try:
        pos_x, pos_y = float(pos_x), float(pos_y)
        data = _get_cached(video_dir_name, template_path)
        tpl_center = data["tpl_center"]
        fh, fw = data["frame_bgr"].shape[:2]
        M = make_affine(tpl_center, pos_x, pos_y, scale, angle_deg)
        out_dir = os.path.join(SCRIPT_DIR, 'template_align_output')
        os.makedirs(out_dir, exist_ok=True)
        np.savez(os.path.join(out_dir, "align_params.npz"), affine_M=M,
                 pos_x=pos_x, pos_y=pos_y, scale=float(scale),
                 angle_deg=float(angle_deg),
                 tpl_center_x=tpl_center[0], tpl_center_y=tpl_center[1])
        warped_mask = cv2.warpAffine(data["tpl_binary"], M, (fw, fh), flags=cv2.INTER_NEAREST)
        cv2.imwrite(os.path.join(out_dir, "template_mask_frame0.png"), warped_mask)
        return (f"âœ… å·²ä¿å­˜! ä½ç½®:({pos_x:.0f},{pos_y:.0f}), "
                f"ç¼©æ”¾:{scale:.4f}, æ—‹è½¬:{angle_deg:.1f}Â°\n"
                f"ğŸ“ {out_dir}")
    except Exception as e:
        traceback.print_exc()
        return f"ä¿å­˜å¤±è´¥: {e}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   Step 2 å›è°ƒ: SAMä¼ æ’­ + æ¨¡æ¿è¡¥å…¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# å…¨å±€å­˜å‚¨ SAM ä¼ æ’­ç»“æœ + è¡¥å…¨ç»“æœ
_propagation_results = {}


def on_s2_load_frame(video_dir_name, frame_idx):
    """Step 2: åŠ è½½æ ‡æ³¨å¸§"""
    if not video_dir_name:
        return None, "è¯·å…ˆåœ¨ Step 1 é€‰æ‹©è§†é¢‘ç›®å½•"
    frames = get_sorted_frames(video_dir_name)
    if not frames:
        return None, "æ— å¸§"
    idx = max(0, min(int(frame_idx), len(frames) - 1))
    path = os.path.join(BASE_DIR, video_dir_name, frames[idx])
    img = Image.open(path).convert('RGB')
    return img, f"å¸§ {idx}/{len(frames)-1}, å°ºå¯¸: {img.size[0]}x{img.size[1]}"


def on_s2_click(video_dir_name, frame_idx, point_type,
                points_state, labels_state, evt: gr.SelectData):
    """Step 2: ç‚¹å‡»æ·»åŠ æ ‡æ³¨ç‚¹"""
    if not video_dir_name:
        return None, points_state, labels_state, "è¯·é€‰æ‹©è§†é¢‘ç›®å½•"
    x, y = evt.index[0], evt.index[1]
    label = 1 if point_type == "æ­£æ ·æœ¬ (å‰æ™¯)" else 0
    points_state.append([x, y])
    labels_state.append(label)
    # ç»˜åˆ¶
    frames = get_sorted_frames(video_dir_name)
    idx = max(0, min(int(frame_idx), len(frames) - 1))
    img = Image.open(os.path.join(BASE_DIR, video_dir_name, frames[idx])).convert('RGB')
    draw = ImageDraw.Draw(img)
    for i, (pt, lbl) in enumerate(zip(points_state, labels_state)):
        c = (0, 255, 0) if lbl == 1 else (255, 0, 0)
        r = 6
        draw.ellipse([pt[0]-r, pt[1]-r, pt[0]+r, pt[1]+r], fill=c, outline='white', width=2)
        draw.text((pt[0]+r+4, pt[1]-r), str(i+1), fill='white')
    info = f"å·²é€‰ {len(points_state)} ä¸ªç‚¹"
    return img, points_state, labels_state, info


def on_s2_clear(video_dir_name, frame_idx):
    """æ¸…é™¤æ ‡æ³¨ç‚¹"""
    if video_dir_name:
        frames = get_sorted_frames(video_dir_name)
        idx = max(0, min(int(frame_idx), len(frames) - 1))
        img = Image.open(os.path.join(BASE_DIR, video_dir_name, frames[idx])).convert('RGB')
        return img, [], [], "å·²æ¸…é™¤"
    return None, [], [], "å·²æ¸…é™¤"


def on_s2_preview_mask(video_dir_name, template_path, frame_idx,
                       points_state, labels_state,
                       pos_x, pos_y, scale, angle_deg):
    """
    é¢„è§ˆå•å¸§ SAM maskï¼Œæ˜¾ç¤ºç«¯ç‚¹åœ†æ£€æµ‹ + æ¨¡æ¿è½®å»“å¯¹æ¯”ã€‚
    """
    if not video_dir_name:
        return None, "è¯·å…ˆé€‰æ‹©è§†é¢‘ç›®å½•"
    if not points_state:
        return None, "è¯·å…ˆåœ¨å›¾ä¸Šç‚¹å‡»æ ‡æ³¨è‡³å°‘ä¸€ä¸ªç‚¹"

    try:
        # 1. è¿è¡ŒSAMè·å–å•å¸§mask
        predictor = get_predictor()
        video_path = os.path.join(BASE_DIR, video_dir_name)
        inference_state = predictor.init_state(video_path=video_path)

        frame_idx = int(frame_idx)
        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)

        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=frame_idx, obj_id=1,
            points=points_np, labels=labels_np)

        sam_mask = (out_mask_logits[0] > 0.0).cpu().numpy()
        predictor.reset_state(inference_state)

        # 2. åŠ è½½å¸§
        frames = get_sorted_frames(video_dir_name)
        idx = max(0, min(frame_idx, len(frames) - 1))
        frame_bgr = cv2.imread(os.path.join(video_path, frames[idx]))
        fh, fw = frame_bgr.shape[:2]

        sam_2d = np.squeeze(sam_mask).astype(np.uint8)
        if sam_2d.shape[:2] != (fh, fw):
            sam_2d = cv2.resize(sam_2d, (fw, fh), interpolation=cv2.INTER_NEAREST)
        sam_2d = (sam_2d > 0).astype(np.uint8) * 255

        # 3. SAM mask pose (è´¨å¿ƒæ–¹æ³•)
        sam_pose = get_mask_pose(sam_2d)

        # 4. SAM ç«¯ç‚¹æ£€æµ‹
        sam_ep = find_endpoint_circles(sam_2d)

        # 5. å åŠ å¯è§†åŒ–
        vis = frame_bgr.copy()

        # SAM mask è“è‰²åŠé€æ˜
        blue_overlay = np.zeros_like(vis)
        blue_overlay[:] = (200, 100, 0)
        sam_3c = (sam_2d > 0).astype(np.float32)[..., None]
        vis = (vis * (1 - sam_3c * 0.4) + blue_overlay * sam_3c * 0.4).astype(np.uint8)

        # SAM mask çº¢è‰²è½®å»“
        sam_contours, _ = cv2.findContours(sam_2d, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(vis, sam_contours, -1, (0, 0, 255), 2)

        # SAM è´¨å¿ƒ (å°åå­—, æ ‡æ³¨ "CEN")
        if sam_pose:
            scx, scy = int(sam_pose["cx"]), int(sam_pose["cy"])
            cv2.drawMarker(vis, (scx, scy), (200, 200, 0), cv2.MARKER_CROSS, 15, 1)
            cv2.putText(vis, "CEN", (scx + 10, scy - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200, 200, 0), 1)

        # SAM ç«¯ç‚¹åœ†æ£€æµ‹ + åº•éƒ¨/é¡¶éƒ¨è¯†åˆ«
        sam_bottom = None
        sam_top = None
        bottom_tpl_key = None

        if sam_ep:
            # è¯†åˆ«åº•éƒ¨(Yæœ€å¤§)å’Œé¡¶éƒ¨ç«¯ç‚¹
            sp1 = np.array(sam_ep["p1"])
            sp2 = np.array(sam_ep["p2"])
            if sp1[1] >= sp2[1]:
                sam_bottom = (float(sp1[0]), float(sp1[1]))
                sam_top = (float(sp2[0]), float(sp2[1]))
                sam_bottom_r = sam_ep["r1"]
                sam_top_r = sam_ep["r2"]
            else:
                sam_bottom = (float(sp2[0]), float(sp2[1]))
                sam_top = (float(sp1[0]), float(sp1[1]))
                sam_bottom_r = sam_ep["r2"]
                sam_top_r = sam_ep["r1"]

            # åº•éƒ¨ç«¯ç‚¹ â€” ç»¿è‰²å¤§åœˆ + æ ‡"BOTTOM"
            bx, by = int(sam_bottom[0]), int(sam_bottom[1])
            cv2.circle(vis, (bx, by), int(sam_bottom_r), (0, 255, 0), 3)
            cv2.circle(vis, (bx, by), 5, (0, 255, 0), -1)
            cv2.putText(vis, "BOTTOM", (bx + int(sam_bottom_r) + 5, by),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            # é¡¶éƒ¨ç«¯ç‚¹ â€” é»„è‰²åœˆ + æ ‡"TOP"
            tx, ty = int(sam_top[0]), int(sam_top[1])
            cv2.circle(vis, (tx, ty), int(sam_top_r), (0, 255, 255), 2)
            cv2.circle(vis, (tx, ty), 4, (0, 255, 255), -1)
            cv2.putText(vis, "TOP", (tx + int(sam_top_r) + 5, ty),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
            # ç«¯ç‚¹è¿çº¿
            cv2.line(vis, (bx, by), (tx, ty), (0, 255, 255), 1, cv2.LINE_AA)

        # 6. ä¿¡æ¯é¢æ¿
        info_lines = []
        info_lines.append(f"â”â”â” SAM Mask â”â”â”")
        info_lines.append(f"åƒç´ : {int(np.sum(sam_2d > 0))}")
        if sam_pose:
            info_lines.append(f"è´¨å¿ƒ: ({sam_pose['cx']:.0f}, {sam_pose['cy']:.0f})")

        info_lines.append(f"\nâ”â”â” ç«¯ç‚¹æ£€æµ‹ â”â”â”")
        if sam_ep and sam_bottom:
            info_lines.append(f"âœ… æ£€æµ‹åˆ°ä¸¤ç«¯åœ†å½¢!")
            info_lines.append(f"ğŸ”½ BOTTOM: ({sam_bottom[0]:.0f},{sam_bottom[1]:.0f}), r={sam_bottom_r:.1f}")
            info_lines.append(f"ğŸ”¼ TOP: ({sam_top[0]:.0f},{sam_top[1]:.0f}), r={sam_top_r:.1f}")
            info_lines.append(f"ç«¯ç‚¹è·ç¦»: {sam_ep['dist']:.1f}px")
            info_lines.append(f"ç«¯ç‚¹è§’åº¦: {sam_ep['angle_deg']:.1f}Â°")
        else:
            info_lines.append("âŒ æœªæ£€æµ‹åˆ°ç«¯ç‚¹åœ†")

        # 7. åº•éƒ¨ç«¯ç‚¹é”šå®šå¯¹é½é¢„è§ˆ
        if pos_x is not None and pos_y is not None and template_path:
            pos_x, pos_y = float(pos_x), float(pos_y)
            data = _get_cached(video_dir_name, template_path)
            tpl_binary = data["tpl_binary"]
            tpl_contour = data["tpl_contour"]
            tpl_center = data["tpl_center"]
            tpl_endpoints = data.get("tpl_endpoints")

            if tpl_binary is not None:
                # Step1 å¯¹é½ (ç»¿è‰²è½®å»“)
                M_s1 = make_affine(tpl_center, pos_x, pos_y, scale, angle_deg)
                warped_s1 = cv2.warpAffine(tpl_binary, M_s1, (fw, fh),
                                           flags=cv2.INTER_NEAREST)
                if tpl_contour is not None:
                    pts = tpl_contour.reshape(-1, 2).astype(np.float64)
                    ones = np.ones((pts.shape[0], 1))
                    pts_h = np.hstack([pts, ones])
                    wc_s1 = (M_s1 @ pts_h.T).T.reshape(-1, 1, 2).astype(np.int32)
                    cv2.drawContours(vis, [wc_s1], -1, (0, 255, 0), 2)

                iou_s1 = int(np.sum((sam_2d > 0) & (warped_s1 > 0))) / max(
                    int(np.sum((sam_2d > 0) | (warped_s1 > 0))), 1)

                info_lines.append(f"\nâ”â”â” Step1 å¯¹é½ (ç»¿) â”â”â”")
                info_lines.append(f"ä¸­å¿ƒ: ({pos_x:.0f},{pos_y:.0f})")
                info_lines.append(f"ç¼©æ”¾: {scale:.4f}, è§’åº¦: {angle_deg:.1f}Â°")
                info_lines.append(f"IoU: {iou_s1:.1%}")

                # ç¡®å®šåº•éƒ¨æ¨¡æ¿ç«¯ç‚¹
                if tpl_endpoints:
                    bottom_tpl_key = find_bottom_tpl_key(
                        tpl_endpoints, tpl_center, pos_x, pos_y, scale, angle_deg)
                    top_tpl_key = "p2" if bottom_tpl_key == "p1" else "p1"

                    # æ˜¾ç¤ºStep1å¯¹é½åæ¨¡æ¿ç«¯ç‚¹çš„ä½ç½®
                    p_b_frame = M_s1 @ np.array([
                        tpl_endpoints[bottom_tpl_key][0],
                        tpl_endpoints[bottom_tpl_key][1], 1.0])
                    p_t_frame = M_s1 @ np.array([
                        tpl_endpoints[top_tpl_key][0],
                        tpl_endpoints[top_tpl_key][1], 1.0])
                    # Step1æ¨¡æ¿åº•éƒ¨ç«¯ç‚¹ â€” çº¢è‰²ä¸‰è§’
                    cv2.drawMarker(vis,
                                   (int(p_b_frame[0]), int(p_b_frame[1])),
                                   (0, 0, 255), cv2.MARKER_TRIANGLE_DOWN, 15, 2)
                    cv2.putText(vis, "TPL_B",
                                (int(p_b_frame[0]) + 10, int(p_b_frame[1]) + 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1)

                    info_lines.append(f"æ¨¡æ¿åº•éƒ¨ç«¯ç‚¹({bottom_tpl_key}): "
                                      f"({p_b_frame[0]:.0f},{p_b_frame[1]:.0f})")

                # åº•éƒ¨ç«¯ç‚¹é”šå®šå¯¹é½ (å“çº¢è‰²è½®å»“)
                if sam_ep and sam_bottom and tpl_endpoints and bottom_tpl_key:
                    # æ£€æŸ¥ç«¯ç‚¹é—´è·
                    expected_dist = tpl_endpoints["dist"] * scale
                    actual_dist = np.linalg.norm(
                        np.array(sam_bottom) - np.array(sam_top))
                    dist_ratio = actual_dist / max(expected_dist, 1)

                    if 0.65 < dist_ratio < 1.5:
                        # ä¸¤ç«¯éƒ½OK â†’ ä»ä¸¤ç«¯è®¡ç®—è§’åº¦
                        anchor_angle = compute_angle_from_endpoints(
                            sam_bottom, sam_top, tpl_endpoints, bottom_tpl_key)
                        anchor_align = align_from_anchor(
                            tpl_endpoints, tpl_center,
                            bottom_tpl_key, sam_bottom,
                            scale, anchor_angle)
                        method_str = f"dual_endpoint(ratio={dist_ratio:.2f})"
                    else:
                        # é®æŒ¡ â†’ åº•éƒ¨é”šç‚¹ + Step1è§’åº¦
                        anchor_align = align_from_anchor(
                            tpl_endpoints, tpl_center,
                            bottom_tpl_key, sam_bottom,
                            scale, angle_deg)
                        method_str = f"bottom_anchor(ratio={dist_ratio:.2f})"

                    # Warp æ¨¡æ¿ (å“çº¢è‰²)
                    M_anchor = make_affine(tpl_center,
                                           anchor_align["pos_x"],
                                           anchor_align["pos_y"],
                                           anchor_align["scale"],
                                           anchor_align["angle_deg"])
                    warped_anchor = cv2.warpAffine(tpl_binary, M_anchor, (fw, fh),
                                                    flags=cv2.INTER_NEAREST)
                    if tpl_contour is not None:
                        pts = tpl_contour.reshape(-1, 2).astype(np.float64)
                        ones = np.ones((pts.shape[0], 1))
                        pts_h = np.hstack([pts, ones])
                        wc_a = (M_anchor @ pts_h.T).T.reshape(-1, 1, 2).astype(np.int32)
                        cv2.drawContours(vis, [wc_a], -1, (255, 0, 255), 2)

                    # å®Œæ•´maské¢„è§ˆ (union)
                    complete_preview = np.maximum(sam_2d, warped_anchor)
                    comp_contours, _ = cv2.findContours(
                        complete_preview, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(vis, comp_contours, -1, (255, 255, 0), 1)

                    iou_anchor = int(np.sum((sam_2d > 0) & (warped_anchor > 0))) / max(
                        int(np.sum((sam_2d > 0) | (warped_anchor > 0))), 1)

                    info_lines.append(f"\nâ”â”â” ğŸ”½ åº•éƒ¨é”šå®šå¯¹é½ (ç´«) â”â”â”")
                    info_lines.append(f"æ–¹æ³•: {method_str}")
                    info_lines.append(f"ä¸­å¿ƒ: ({anchor_align['pos_x']:.0f},"
                                      f"{anchor_align['pos_y']:.0f})")
                    info_lines.append(f"è§’åº¦: {anchor_align['angle_deg']:.1f}Â°")
                    info_lines.append(f"IoU: {iou_anchor:.1%}")
                    info_lines.append(f"å®Œæ•´maskåƒç´ : {int(np.sum(complete_preview > 0))}")

                    # SAMåº•éƒ¨ç«¯ç‚¹ vs æ¨¡æ¿åº•éƒ¨ç«¯ç‚¹ çš„åå·®
                    if sam_bottom:
                        db = np.sqrt((sam_bottom[0] - p_b_frame[0])**2 +
                                     (sam_bottom[1] - p_b_frame[1])**2)
                        info_lines.append(f"\nSAMåº•éƒ¨â†”Step1åº•éƒ¨åç§»: {db:.1f}px")
                        if db > 10:
                            info_lines.append("  âš ï¸ åç§»è¾ƒå¤§,å»ºè®®è°ƒæ•´Step1å¯¹é½")

                    info_lines.append(f"\nğŸŸ¢ç»¿=Step1  ğŸŸ£ç´«=åº•éƒ¨é”šå®š  ğŸŸ¡é»„=å®Œæ•´è½®å»“")

            # å›¾ä¾‹
            cv2.putText(vis, "Red=SAM Green=Step1 Purple=Anchor Cyan=EP",
                        (10, fh - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 1)
        else:
            info_lines.append("\n(Step 1 æœªè®¾ç½®å¯¹é½, ä»…æ˜¾ç¤º SAM mask)")
            cv2.putText(vis, "Red=SAM  Green=BOTTOM  Cyan=TOP",
                        (10, fh - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)
        return vis_rgb, "\n".join(info_lines)

    except Exception as e:
        traceback.print_exc()
        return None, f"é¢„è§ˆå¤±è´¥: {e}"


def on_s2_propagate_and_complete(
    video_dir_name, template_path, frame_idx,
    points_state, labels_state, class_id,
    pos_x, pos_y, scale, angle_deg,
    track_angle, use_endpoint_matching, progress=gr.Progress()
):
    """
    æ ¸å¿ƒæµç¨‹:
    1. SAM ä¼ æ’­è·å–æ‰€æœ‰å¸§çš„ partial mask
    2. å‚è€ƒå¸§: ç”¨æ‰‹åŠ¨å¯¹é½å‚æ•°å»ºç«‹ "æ¨¡æ¿â†”SAM mask" çš„è§’åº¦åç§»
    3. æ¯å¸§: SAM mask è´¨å¿ƒå®šä½ + è§’åº¦è¿½è¸ª â†’ warp æ¨¡æ¿ â†’ åˆå¹¶
    4. å¯¼å‡º YOLO
    """
    global _propagation_results

    if not video_dir_name or not template_path:
        return "è¯·å…ˆå®Œæˆ Step 1", None
    if not points_state:
        return "è¯·å…ˆæ ‡æ³¨è‡³å°‘ä¸€ä¸ªç‚¹", None
    if pos_x is None or pos_y is None:
        return "è¯·å…ˆåœ¨ Step 1 å®Œæˆæ¨¡æ¿å¯¹é½ (ä½ç½®æœªè®¾å®š)", None

    pos_x, pos_y = float(pos_x), float(pos_y)
    frame_idx = int(frame_idx)
    class_id_int = int(class_id)

    try:
        log_lines = []
        def log(msg):
            log_lines.append(msg)
            print(msg)

        # â”€â”€ 1. åŠ è½½æ¨¡æ¿æ•°æ® â”€â”€
        _, tpl_binary, tpl_contour, tpl_center = load_template(template_path)
        if tpl_binary is None:
            return "æ¨¡æ¿åŠ è½½å¤±è´¥", None

        tpl_endpoints = get_template_endpoints(tpl_binary)

        video_path = os.path.join(BASE_DIR, video_dir_name)
        frames = get_sorted_frames(video_dir_name)
        sample_bgr = cv2.imread(os.path.join(video_path, frames[0]))
        fh, fw = sample_bgr.shape[:2]

        log(f"ğŸ“‚ è§†é¢‘: {video_dir_name}, {len(frames)} å¸§, {fw}x{fh}")
        log(f"ğŸ“ æ¨¡æ¿: {os.path.basename(template_path)}")
        log(f"ğŸ¯ æ ‡æ³¨å¸§: {frame_idx}, é€‰ç‚¹: {len(points_state)}")
        log(f"ğŸ“ Step1 å¯¹é½: pos=({pos_x:.0f},{pos_y:.0f}), "
            f"scale={scale:.4f}, angle={angle_deg:.1f}Â°")
        log(f"ğŸ”„ è§’åº¦è¿½è¸ª: {'å¼€å¯' if track_angle else 'å…³é—­'}")
        log(f"ğŸ”µ ç«¯ç‚¹åŒ¹é…: {'å¼€å¯' if use_endpoint_matching else 'å…³é—­'}")
        if tpl_endpoints:
            log(f"   æ¨¡æ¿ç«¯ç‚¹: EP1=({tpl_endpoints['p1'][0]:.0f},{tpl_endpoints['p1'][1]:.0f}), "
                f"EP2=({tpl_endpoints['p2'][0]:.0f},{tpl_endpoints['p2'][1]:.0f}), "
                f"è·ç¦»={tpl_endpoints['dist']:.0f}")
        else:
            log(f"   âš ï¸ æ¨¡æ¿ç«¯ç‚¹æ£€æµ‹å¤±è´¥, å°†ä½¿ç”¨è´¨å¿ƒæ–¹æ³•")
            use_endpoint_matching = False

        # â”€â”€ 2. SAM ä¼ æ’­ â”€â”€
        progress(0.0, desc="åˆå§‹åŒ– SAM2 ...")
        predictor = get_predictor()
        inference_state = predictor.init_state(video_path=video_path)

        points_np = np.array(points_state, dtype=np.float32)
        labels_np = np.array(labels_state, dtype=np.int32)
        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=frame_idx, obj_id=1,
            points=points_np, labels=labels_np)
        log(f"âœ… SAM æç¤ºå·²æ·»åŠ ")

        progress(0.1, desc="SAM ä¼ æ’­ä¸­ ...")
        video_segments = {}
        for out_fi, out_ids, out_logits in predictor.propagate_in_video(inference_state):
            video_segments[out_fi] = {
                oid: (out_logits[i] > 0.0).cpu().numpy()
                for i, oid in enumerate(out_ids)
            }
        predictor.reset_state(inference_state)
        n_with_mask = sum(1 for segs in video_segments.values()
                         for m in segs.values() if np.any(np.squeeze(m) > 0))
        log(f"âœ… SAM ä¼ æ’­å®Œæˆ: {n_with_mask}/{len(frames)} å¸§æœ‰ mask")

        # â”€â”€ 3. ç¡®å®šåº•éƒ¨ç«¯ç‚¹ + åˆå§‹åŒ–è¿½è¸ªçŠ¶æ€ â”€â”€
        # ä»Step1å¯¹é½ç¡®å®šæ¨¡æ¿å“ªä¸ªç«¯ç‚¹åœ¨å¸§ä¸­æ˜¯åº•éƒ¨(Yæœ€å¤§)
        bottom_tpl_key = find_bottom_tpl_key(
            tpl_endpoints, tpl_center, pos_x, pos_y, scale, angle_deg)
        top_tpl_key = "p2" if bottom_tpl_key == "p1" else "p1"

        # ç”¨Step1ä»¿å°„å˜æ¢è®¡ç®—åˆå§‹ç«¯ç‚¹ä½ç½® â†’ ä½œä¸ºç¬¬ä¸€å¸§è¿½è¸ªèµ·ç‚¹
        if tpl_endpoints is not None and use_endpoint_matching:
            M_ref = make_affine(tpl_center, pos_x, pos_y, scale, angle_deg)
            p_bottom = M_ref @ np.array([
                tpl_endpoints[bottom_tpl_key][0],
                tpl_endpoints[bottom_tpl_key][1], 1.0])
            p_top = M_ref @ np.array([
                tpl_endpoints[top_tpl_key][0],
                tpl_endpoints[top_tpl_key][1], 1.0])
            prev_bottom_pos = (float(p_bottom[0]), float(p_bottom[1]))
            prev_top_pos = (float(p_top[0]), float(p_top[1]))
            log(f"ğŸ”½ åº•éƒ¨ç«¯ç‚¹(æ¨¡æ¿{bottom_tpl_key}): "
                f"åˆå§‹ä½ç½®=({prev_bottom_pos[0]:.0f},{prev_bottom_pos[1]:.0f})")
            log(f"ğŸ”¼ é¡¶éƒ¨ç«¯ç‚¹(æ¨¡æ¿{top_tpl_key}): "
                f"åˆå§‹ä½ç½®=({prev_top_pos[0]:.0f},{prev_top_pos[1]:.0f})")
        else:
            prev_bottom_pos = None
            prev_top_pos = None

        last_good_angle = float(angle_deg)

        # â”€â”€ 4. é€å¸§æ¨¡æ¿è¡¥å…¨ (åº•éƒ¨ç«¯ç‚¹é”šå®š) â”€â”€
        progress(0.3, desc="æ¨¡æ¿è¡¥å…¨ä¸­ ...")
        completed_masks = {}
        warped_templates = {}
        pose_log = []
        method_counts = {}

        for fi in range(len(frames)):
            prog = 0.3 + 0.5 * fi / len(frames)
            progress(prog, desc=f"è¡¥å…¨å¸§ {fi+1}/{len(frames)} ...")

            sam_mask_2d = np.zeros((fh, fw), dtype=np.uint8)
            has_sam = False
            if fi in video_segments:
                for oid, mask in video_segments[fi].items():
                    m2d = np.squeeze(mask).astype(np.uint8)
                    if m2d.shape[:2] != (fh, fw):
                        m2d = cv2.resize(m2d, (fw, fh), interpolation=cv2.INTER_NEAREST)
                    sam_mask_2d = (m2d > 0).astype(np.uint8) * 255
                    if np.any(sam_mask_2d > 0):
                        has_sam = True
                    break

            if has_sam:
                comp, warped, tracking = complete_single_mask(
                    sam_mask_2d, tpl_binary, tpl_center,
                    scale, angle_deg,
                    tpl_endpoints=tpl_endpoints,
                    use_endpoint_matching=use_endpoint_matching,
                    bottom_tpl_key=bottom_tpl_key,
                    prev_bottom_pos=prev_bottom_pos,
                    prev_top_pos=prev_top_pos,
                    last_good_angle=last_good_angle)
                completed_masks[fi] = comp
                warped_templates[fi] = warped
                if tracking:
                    method = tracking["method"]
                    method_counts[method] = method_counts.get(method, 0) + 1
                    # æ›´æ–°è¿½è¸ªçŠ¶æ€
                    if tracking["bottom_pos"] is not None:
                        prev_bottom_pos = tracking["bottom_pos"]
                    if tracking["top_pos"] is not None:
                        prev_top_pos = tracking["top_pos"]
                    # åªåœ¨ä¸¤ç«¯éƒ½æ­£ç¡®æ£€æµ‹æ—¶æ›´æ–°è§’åº¦
                    if track_angle and "dual_endpoint" in method:
                        last_good_angle = tracking["angle_deg"]
                    pose_log.append(
                        f"  å¸§{fi}: pos=({tracking['pos'][0]:.0f},{tracking['pos'][1]:.0f}), "
                        f"angle={tracking['angle_deg']:.1f}Â°, "
                        f"bottom=({tracking['bottom_pos'][0]:.0f},{tracking['bottom_pos'][1]:.0f})"
                        if tracking['bottom_pos'] else
                        f"  å¸§{fi}: æ–¹æ³•={method}(æ— åº•éƒ¨ç«¯ç‚¹)")
            else:
                completed_masks[fi] = sam_mask_2d
                warped_templates[fi] = np.zeros((fh, fw), dtype=np.uint8)

        # ç»Ÿè®¡
        n_completed = sum(1 for m in completed_masks.values() if np.any(m > 0))
        log(f"\nâœ… æ¨¡æ¿è¡¥å…¨å®Œæˆ: {n_completed}/{len(frames)} å¸§æœ‰å®Œæ•´ mask")
        log(f"   å¯¹é½æ–¹æ³•ç»Ÿè®¡:")
        for method, cnt in sorted(method_counts.items()):
            log(f"     {method}: {cnt} å¸§")
        if pose_log:
            log(f"\nå‰å‡ å¸§ pose (å…± {len(pose_log)}):")
            for line in pose_log[:8]:
                log(line)
            if len(pose_log) > 8:
                log(f"  ... ç­‰ {len(pose_log)} å¸§")

        # â”€â”€ 5. å¯¼å‡º YOLO â”€â”€
        progress(0.8, desc="å¯¼å‡º YOLO æ•°æ®é›† ...")
        output_dir = os.path.join(SCRIPT_DIR, 'yolo_dataset', video_dir_name)
        images_dir = os.path.join(output_dir, 'images')
        labels_dir = os.path.join(output_dir, 'labels')
        vis_dir = os.path.join(output_dir, 'images_vis')
        for d in [images_dir, labels_dir, vis_dir]:
            os.makedirs(d, exist_ok=True)

        saved = 0
        skipped = 0
        for fi in range(len(frames)):
            prog = 0.8 + 0.18 * fi / len(frames)
            progress(prog, desc=f"å¯¼å‡ºå¸§ {fi+1}/{len(frames)} ...")

            img_path = os.path.join(video_path, frames[fi])
            img = Image.open(img_path).convert('RGB')
            img_w, img_h = img.size
            img_name = f'{fi:05d}.jpg'
            shutil.copy(img_path, os.path.join(images_dir, img_name))

            mask = completed_masks.get(fi, np.zeros((fh, fw), dtype=np.uint8))
            label_lines = []
            if np.any(mask > 0):
                polygon = mask_to_yolo_seg(mask, img_w, img_h)
                if polygon and len(polygon) >= 6:
                    poly_arr = np.array(polygon)
                    if not (np.any(poly_arr < 0) or np.any(poly_arr > 1)):
                        poly_str = ' '.join(f'{c:.6f}' for c in polygon)
                        label_lines.append(f"{class_id_int} {poly_str}\n")

            with open(os.path.join(labels_dir, f'{fi:05d}.txt'), 'w') as f:
                f.writelines(label_lines)
            if not label_lines:
                skipped += 1

            # å¯è§†åŒ–
            frame_bgr = cv2.imread(img_path)
            vis_rgb = build_completion_vis(
                frame_bgr,
                video_segments.get(fi, {}).get(1, np.zeros((fh, fw), dtype=np.uint8)),
                mask,
                warped_templates.get(fi, np.zeros((fh, fw), dtype=np.uint8)))
            cv2.imwrite(os.path.join(vis_dir, img_name),
                        cv2.cvtColor(vis_rgb, cv2.COLOR_RGB2BGR))
            saved += 1

        progress(1.0, desc="å®Œæˆ!")
        log(f"\n{'='*50}")
        log(f"   å¯¼å‡ºå®Œæˆ!")
        log(f"   æ€»å¸§æ•°: {len(frames)}")
        log(f"   æœ‰ mask çš„å¸§: {saved - skipped}")
        log(f"   æ—  mask çš„å¸§: {skipped}")
        log(f"   è¾“å‡ºç›®å½•: {output_dir}")
        log(f"     â”œâ”€â”€ images/     (åŸå›¾)")
        log(f"     â”œâ”€â”€ labels/     (YOLO æ ‡æ³¨)")
        log(f"     â””â”€â”€ images_vis/ (å¯è§†åŒ–: è“=SAM, é»„=æ¨¡æ¿è¡¥å…¨, ç»¿=æœ€ç»ˆè½®å»“)")

        # ä¿å­˜ç»“æœä¾›æµè§ˆ
        _propagation_results["video_dir"] = video_dir_name
        _propagation_results["completed_masks"] = completed_masks
        _propagation_results["warped_templates"] = warped_templates
        _propagation_results["video_segments"] = video_segments
        _propagation_results["frames"] = frames

        # è¿”å›ç¬¬ä¸€å¸§å¯è§†åŒ–
        first_vis_path = os.path.join(vis_dir, '00000.jpg')
        first_vis = None
        if os.path.exists(first_vis_path):
            first_vis = cv2.cvtColor(cv2.imread(first_vis_path), cv2.COLOR_BGR2RGB)

        return "\n".join(log_lines), first_vis

    except Exception as e:
        traceback.print_exc()
        return f"âŒ å¤±è´¥: {e}\n{traceback.format_exc()}", None


def on_s2_browse(video_dir_name, frame_idx):
    """æµè§ˆè¡¥å…¨ç»“æœ"""
    if not _propagation_results or _propagation_results.get("video_dir") != video_dir_name:
        return None, "è¯·å…ˆè¿è¡Œ SAMä¼ æ’­+æ¨¡æ¿è¡¥å…¨"
    try:
        frames = _propagation_results["frames"]
        idx = max(0, min(int(frame_idx), len(frames) - 1))
        completed = _propagation_results["completed_masks"]
        warped_tpls = _propagation_results["warped_templates"]
        video_segments = _propagation_results["video_segments"]

        frame_bgr = cv2.imread(os.path.join(BASE_DIR, video_dir_name, frames[idx]))
        fh, fw = frame_bgr.shape[:2]

        sam_mask = np.zeros((fh, fw), dtype=np.uint8)
        if idx in video_segments:
            for oid, m in video_segments[idx].items():
                m2d = np.squeeze(m).astype(np.uint8)
                if m2d.shape[:2] != (fh, fw):
                    m2d = cv2.resize(m2d, (fw, fh), interpolation=cv2.INTER_NEAREST)
                sam_mask = (m2d > 0).astype(np.uint8) * 255
                break

        comp = completed.get(idx, np.zeros((fh, fw), dtype=np.uint8))
        wt = warped_tpls.get(idx, np.zeros((fh, fw), dtype=np.uint8))

        vis = build_completion_vis(frame_bgr, sam_mask, comp, wt)

        sam_area = int(np.sum(sam_mask > 0))
        comp_area = int(np.sum(comp > 0))
        added = comp_area - sam_area
        info = (f"å¸§ {idx}/{len(frames)-1} | "
                f"SAM: {sam_area}pxÂ² â†’ è¡¥å…¨å: {comp_area}pxÂ² "
                f"(+{added}pxÂ², +{added/max(sam_area,1)*100:.0f}%)")
        return vis, info
    except Exception as e:
        return None, f"é”™è¯¯: {e}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   Gradio ç•Œé¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_app():
    video_dirs = list_video_dirs()

    with gr.Blocks(title="æ¨¡æ¿å¯¹é½è¡¥å…¨å·¥å…·") as app:
        gr.Markdown("# ğŸ”§ 2Dæ¨¡æ¿å¯¹é½ + SAMè¡¥å…¨ ä¸€ç«™å¼å·¥å…·")
        gr.Markdown(
            "**Step 1**: æ‰‹åŠ¨å¯¹é½æ¨¡æ¿åˆ°ç¬¬ä¸€å¸§ (ç‚¹å‡»å®šä½ + ç¼©æ”¾/æ—‹è½¬)\n\n"
            "**Step 2**: SAM ä¼ æ’­ + æ¯å¸§æ ¹æ®SAM maskåŠ¨æ€è¿½è¸ªä½å§¿ â†’ æ¨¡æ¿è¡¥å…¨ â†’ å¯¼å‡º YOLO"
        )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: æ¨¡æ¿å¯¹é½
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        with gr.Accordion("Step 1: æ¨¡æ¿å¯¹é½ (æ‰‹åŠ¨)", open=True):
            with gr.Row():
                video_dir_dd = gr.Dropdown(
                    choices=video_dirs, label="é€‰æ‹©è§†é¢‘å¸§ç›®å½•",
                    info="video_to_img/ ä¸‹çš„å­æ–‡ä»¶å¤¹")
                template_path_input = gr.Textbox(
                    value=DEFAULT_TEMPLATE, label="æ¨¡æ¿å›¾ç‰‡è·¯å¾„")
                load_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")

            pos_x_state = gr.State(None)
            pos_y_state = gr.State(None)

            with gr.Row():
                with gr.Column(scale=1):
                    tpl_img = gr.Image(label="æ¨¡æ¿è½®å»“", interactive=False, height=220)
                    gr.Markdown("ğŸ“ **ç‚¹å‡»å³å›¾å®šä½** | ğŸš æ»‘æ¡è°ƒç¼©æ”¾/æ—‹è½¬")
                    scale_slider = gr.Slider(0.01, 1.0, 0.15, step=0.005, label="ç¼©æ”¾")
                    angle_slider = gr.Slider(-180, 180, 0, step=0.5, label="æ—‹è½¬ (åº¦)")
                    alpha_slider = gr.Slider(10, 80, 40, step=5, label="é€æ˜åº¦ %")
                    with gr.Row():
                        update_btn = gr.Button("åˆ·æ–°", variant="secondary")
                        save_btn = gr.Button("ğŸ’¾ ä¿å­˜å¯¹é½", variant="primary")
                    info_box = gr.Textbox(label="çŠ¶æ€", lines=3, interactive=False)
                with gr.Column(scale=2):
                    preview_img = gr.Image(
                        label="ç‚¹å‡»è®¾ç½®ä½ç½® | ç»¿çº¿=è½®å»“ | é’è‰²=åŒºåŸŸ | çº¢åå­—=ä¸­å¿ƒ",
                        interactive=False)
                    preview_info = gr.Textbox(label="é¢„è§ˆ", lines=1, interactive=False)
            save_result = gr.Textbox(label="ä¿å­˜ç»“æœ", lines=3, interactive=False)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: SAMä¼ æ’­ + æ¨¡æ¿è¡¥å…¨
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        with gr.Accordion("Step 2: SAMä¼ æ’­ + æ¨¡æ¿è¡¥å…¨ + YOLOå¯¼å‡º", open=True):
            gr.Markdown(
                "åœ¨ä¸‹æ–¹æ ‡æ³¨ç›®æ ‡ â†’ è¿è¡Œä¼ æ’­+è¡¥å…¨\n\n"
                "ğŸ”µ è“è‰² = SAMåŸå§‹mask | ğŸŸ¡ é»„è‰² = æ¨¡æ¿è¡¥å…¨éƒ¨åˆ† | ğŸŸ¢ ç»¿è‰²è½®å»“ = æœ€ç»ˆå®Œæ•´mask"
            )
            s2_pts_state = gr.State([])
            s2_labels_state = gr.State([])

            with gr.Row():
                with gr.Column(scale=1):
                    s2_frame_slider = gr.Slider(0, 1000, 0, step=1, label="æ ‡æ³¨å¸§ç´¢å¼•")
                    s2_point_type = gr.Radio(
                        ["æ­£æ ·æœ¬ (å‰æ™¯)", "è´Ÿæ ·æœ¬ (èƒŒæ™¯)"],
                        value="æ­£æ ·æœ¬ (å‰æ™¯)", label="ç‚¹å‡»ç±»å‹")
                    s2_class_id = gr.Number(value=0, label="CLASS_ID", precision=0)
                    s2_track_angle = gr.Checkbox(value=True, label="è§’åº¦è¿½è¸ª",
                                                 info="ä»SAM maskè¿½è¸ªæ—‹è½¬è§’åº¦å˜åŒ–")
                    s2_use_endpoints = gr.Checkbox(value=True, label="ğŸ”µ ç«¯ç‚¹åœ†åŒ¹é…",
                                                   info="ç”¨ä¸¤ç«¯åœ†å½¢ç‰¹å¾å®šä½(æ¨è),å¦åˆ™ç”¨è´¨å¿ƒ")
                    with gr.Row():
                        s2_clear_btn = gr.Button("æ¸…é™¤æ ‡æ³¨", variant="secondary")
                        s2_preview_btn = gr.Button("ğŸ‘ é¢„è§ˆ SAM Mask",
                                                   variant="secondary")
                    s2_run_btn = gr.Button("ğŸš€ ä¼ æ’­ + è¡¥å…¨ + å¯¼å‡º YOLO",
                                           variant="primary", size="lg")
                    s2_pts_info = gr.Textbox(label="æ ‡æ³¨", lines=2, interactive=False)

                with gr.Column(scale=2):
                    s2_annotate_img = gr.Image(
                        label="ç‚¹å‡»æ ‡æ³¨ç›®æ ‡ (ç»¿=å‰æ™¯, çº¢=èƒŒæ™¯)",
                        interactive=False)

            gr.Markdown("#### ğŸ‘ SAM Mask é¢„è§ˆ (å¯¹æ¯”æ¨¡æ¿è½®å»“)")
            gr.Markdown(
                "ğŸ”´ çº¢è‰²è½®å»“/è“è‰²å¡«å…… = SAMåˆ†å‰²ç»“æœ | ğŸŸ¢ ç»¿è‰²è½®å»“ = æ¨¡æ¿ | "
                "è“åå­— = SAMè´¨å¿ƒ | çº¢åå­— = æ¨¡æ¿ä¸­å¿ƒ"
            )
            with gr.Row():
                s2_preview_img = gr.Image(label="SAM mask vs æ¨¡æ¿å¯¹æ¯”", interactive=False)
                s2_preview_info = gr.Textbox(label="SAM vs æ¨¡æ¿ å¯¹æ¯”ä¿¡æ¯", lines=12,
                                             interactive=False)

            s2_log = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=15, interactive=False)

            gr.Markdown("#### æµè§ˆè¡¥å…¨ç»“æœ")
            with gr.Row():
                s2_browse_slider = gr.Slider(0, 1000, 0, step=1, label="å¸§ç´¢å¼•")
                s2_browse_btn = gr.Button("æŸ¥çœ‹", variant="secondary")
            s2_browse_img = gr.Image(label="è¡¥å…¨å¯è§†åŒ–", interactive=False)
            s2_browse_info = gr.Textbox(label="å¸§ä¿¡æ¯", lines=1, interactive=False)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        #   äº‹ä»¶ç»‘å®š
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # â”€â”€ Step 1 â”€â”€
        load_btn.click(
            fn=on_load_data,
            inputs=[video_dir_dd, template_path_input],
            outputs=[tpl_img, preview_img, pos_x_state, pos_y_state,
                     scale_slider, info_box])

        preview_img.select(
            fn=on_click_frame,
            inputs=[pos_x_state, pos_y_state, video_dir_dd, template_path_input,
                    scale_slider, angle_slider, alpha_slider],
            outputs=[preview_img, pos_x_state, pos_y_state, preview_info])

        _s_in = [pos_x_state, pos_y_state, video_dir_dd, template_path_input,
                 scale_slider, angle_slider, alpha_slider]
        _s_out = [preview_img, preview_info]
        for ctrl in [scale_slider, angle_slider, alpha_slider]:
            ctrl.release(fn=on_slider_change, inputs=_s_in, outputs=_s_out)
        update_btn.click(fn=on_slider_change, inputs=_s_in, outputs=_s_out)

        save_btn.click(
            fn=on_save_params,
            inputs=[pos_x_state, pos_y_state, video_dir_dd,
                    template_path_input, scale_slider, angle_slider],
            outputs=[save_result])

        # â”€â”€ Step 2 â”€â”€
        # åŠ è½½æ ‡æ³¨å¸§
        def s2_on_dir_or_frame(vdir, fi):
            img, info = on_s2_load_frame(vdir, fi)
            frames = get_sorted_frames(vdir) if vdir else []
            return (img, info,
                    gr.update(maximum=max(len(frames)-1, 0)),
                    gr.update(maximum=max(len(frames)-1, 0)),
                    [], [])

        video_dir_dd.change(
            fn=s2_on_dir_or_frame,
            inputs=[video_dir_dd, s2_frame_slider],
            outputs=[s2_annotate_img, s2_pts_info,
                     s2_frame_slider, s2_browse_slider,
                     s2_pts_state, s2_labels_state])

        s2_frame_slider.release(
            fn=lambda vd, fi: on_s2_load_frame(vd, fi),
            inputs=[video_dir_dd, s2_frame_slider],
            outputs=[s2_annotate_img, s2_pts_info])

        # ç‚¹å‡»æ ‡æ³¨
        s2_annotate_img.select(
            fn=on_s2_click,
            inputs=[video_dir_dd, s2_frame_slider, s2_point_type,
                    s2_pts_state, s2_labels_state],
            outputs=[s2_annotate_img, s2_pts_state, s2_labels_state, s2_pts_info])

        s2_clear_btn.click(
            fn=on_s2_clear,
            inputs=[video_dir_dd, s2_frame_slider],
            outputs=[s2_annotate_img, s2_pts_state, s2_labels_state, s2_pts_info])

        # é¢„è§ˆ SAM mask (ä¸æ¨¡æ¿å¯¹æ¯”)
        s2_preview_btn.click(
            fn=on_s2_preview_mask,
            inputs=[video_dir_dd, template_path_input, s2_frame_slider,
                    s2_pts_state, s2_labels_state,
                    pos_x_state, pos_y_state, scale_slider, angle_slider],
            outputs=[s2_preview_img, s2_preview_info])

        # è¿è¡Œä¼ æ’­+è¡¥å…¨
        s2_run_btn.click(
            fn=on_s2_propagate_and_complete,
            inputs=[video_dir_dd, template_path_input, s2_frame_slider,
                    s2_pts_state, s2_labels_state, s2_class_id,
                    pos_x_state, pos_y_state, scale_slider, angle_slider,
                    s2_track_angle, s2_use_endpoints],
            outputs=[s2_log, s2_browse_img])

        # æµè§ˆç»“æœ
        s2_browse_btn.click(
            fn=on_s2_browse,
            inputs=[video_dir_dd, s2_browse_slider],
            outputs=[s2_browse_img, s2_browse_info])

    return app


if __name__ == '__main__':
    app = build_app()
    app.launch(
        server_name='0.0.0.0',
        server_port=7862,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
    )
